% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}

@misc{joshi_triviaqa_2017,
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	shorttitle = {{TriviaQA}},
	url = {http://arxiv.org/abs/1705.03551},
	doi = {10.48550/arXiv.1705.03551},
	abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
	urldate = {2025-05-19},
	publisher = {arXiv},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
	month = may,
	year = {2017},
	note = {arXiv:1705.03551 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/TL3QE9XP/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/H5DKCHPV/1705.html:text/html},
}

@inproceedings{ang_socratic_2023,
	address = {Dubrovnik, Croatia},
	title = {Socratic {Question} {Generation}: {A} {Novel} {Dataset}, {Models}, and {Evaluation}},
	shorttitle = {Socratic {Question} {Generation}},
	url = {https://aclanthology.org/2023.eacl-main.12},
	doi = {10.18653/v1/2023.eacl-main.12},
	language = {en},
	urldate = {2025-05-19},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ang, Beng Heng and Gollapalli, Sujatha Das and Ng, See-Kiong},
	year = {2023},
	pages = {147--165},
	file = {Full Text:/Users/ricostadeli/Zotero/storage/FLLATQIY/Ang et al. - 2023 - Socratic Question Generation A Novel Dataset, Models, and Evaluation.pdf:application/pdf},
}

@inproceedings{ye-teufel-2021-end,
    title = "End-to-End Argument Mining as Biaffine Dependency Parsing",
    author = "Ye, Yuxiao  and
      Teufel, Simone",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.55/",
    doi = "10.18653/v1/2021.eacl-main.55",
    pages = "669--678",
    abstract = "Non-neural approaches to argument mining (AM) are often pipelined and require heavy feature-engineering. In this paper, we propose a neural end-to-end approach to AM which is based on dependency parsing, in contrast to the current state-of-the-art which relies on relation extraction. Our biaffine AM dependency parser significantly outperforms the state-of-the-art, performing at F1 = 73.5{\%} for component identification and F1 = 46.4{\%} for relation identification. One of the advantages of treating AM as biaffine dependency parsing is the simple neural architecture that results. The idea of treating AM as dependency parsing is not new, but has previously been abandoned as it was lagging far behind the state-of-the-art. In a thorough analysis, we investigate the factors that contribute to the success of our model: the biaffine model itself, our representation for the dependency structure of arguments, different encoders in the biaffine model, and syntactic information additionally fed to the model. Our work demonstrates that dependency parsing for AM, an overlooked idea from the past, deserves more attention in the future."
}

@misc{yang_hotpotqa_2018,
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {http://arxiv.org/abs/1809.09600},
	doi = {10.48550/arXiv.1809.09600},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	urldate = {2025-05-19},
	publisher = {arXiv},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = sep,
	year = {2018},
	note = {arXiv:1809.09600 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/SDTSV6HZ/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi-hop Question Answering.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/QFLBGS4H/1809.html:text/html},
}

@inproceedings{lahitani2016cosine,
  title={Cosine similarity to determine similarity measure: Study case in online essay assessment},
  author={Lahitani, Alfirna Rizqi and Permanasari, Adhistya Erna and Setiawan, Noor Akhmad},
  booktitle={2016 4th International conference on cyber and IT service management},
  pages={1--6},
  year={2016},
  organization={IEEE},
  doi={10.1109/CITSM.2016.7577578}
}

@inproceedings{koudas2004flexible,
  title={Flexible string matching against large databases in practice},
  author={Koudas, Nick and Marathe, Amit and Srivastava, Divesh},
  booktitle={Proceedings of the Thirtieth international conference on Very large data bases-Volume 30},
  pages={1078--1086},
  year={2004}
}

@misc{rafailov_direct_2024,
	title = {Direct {Preference} {Optimization}: {Your} {Language} {Model} is {Secretly} a {Reward} {Model}},
	shorttitle = {Direct {Preference} {Optimization}},
	url = {http://arxiv.org/abs/2305.18290},
	doi = {10.48550/arXiv.2305.18290},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2025-05-20},
	publisher = {arXiv},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = jul,
	year = {2024},
	note = {arXiv:2305.18290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/2G7CWJ9Z/Rafailov et al. - 2024 - Direct Preference Optimization Your Language Model is Secretly a Reward Model.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/MPUVCX96/2305.html:text/html},
}

@inproceedings{ruiz-dolz_detecting_2023,
	address = {Singapore},
	title = {Detecting {Argumentative} {Fallacies} in the {Wild}: {Problems} and {Limitations} of {Large} {Language} {Models}},
	shorttitle = {Detecting {Argumentative} {Fallacies} in the {Wild}},
	url = {https://aclanthology.org/2023.argmining-1.1},
	doi = {10.18653/v1/2023.argmining-1.1},
	language = {en},
	urldate = {2025-05-20},
	booktitle = {Proceedings of the 10th {Workshop} on {Argument} {Mining}},
	publisher = {Association for Computational Linguistics},
	author = {Ruiz-Dolz, Ramon and Lawrence, John},
	year = {2023},
	pages = {1--10},
	file = {Full Text:/Users/ricostadeli/Zotero/storage/93RMBIND/Ruiz-Dolz and Lawrence - 2023 - Detecting Argumentative Fallacies in the Wild Problems and Limitations of Large Language Models.pdf:application/pdf},
}

@inproceedings{calvo_figueras_critical_2024,
	address = {Miami, FL, USA},
	title = {Critical {Questions} {Generation}: {Motivation} and {Challenges}},
	shorttitle = {Critical {Questions} {Generation}},
	url = {https://aclanthology.org/2024.conll-1.9},
	doi = {10.18653/v1/2024.conll-1.9},
	language = {en},
	urldate = {2025-05-20},
	booktitle = {Proceedings of the 28th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Calvo Figueras, Blanca and Agerri, Rodrigo},
	year = {2024},
	pages = {105--116},
}

@article{visser_argumentation_2020,
	title = {Argumentation in the 2016 {US} presidential elections: annotated corpora of television debates and social media reaction},
	volume = {54},
	issn = {1574-020X, 1574-0218},
	shorttitle = {Argumentation in the 2016 {US} presidential elections},
	url = {http://link.springer.com/10.1007/s10579-019-09446-8},
	doi = {10.1007/s10579-019-09446-8},
	language = {en},
	number = {1},
	urldate = {2025-05-28},
	journal = {Language Resources and Evaluation},
	author = {Visser, Jacky and Konat, Barbara and Duthie, Rory and Koszowy, Marcin and Budzynska, Katarzyna and Reed, Chris},
	month = mar,
	year = {2020},
	pages = {123--154},
	file = {Full Text:/Users/ricostadeli/Zotero/storage/CBY8GPUS/Visser et al. - 2020 - Argumentation in the 2016 US presidential elections annotated corpora of television debates and soc.pdf:application/pdf},
}

@inproceedings{lawrence-reed-2015-combining,
    title = "Combining Argument Mining Techniques",
    author = "Lawrence, John  and
      Reed, Chris",
    editor = "Cardie, Claire",
    booktitle = "Proceedings of the 2nd Workshop on Argumentation Mining",
    month = jun,
    year = "2015",
    address = "Denver, CO",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-0516/",
    doi = "10.3115/v1/W15-0516",
    pages = "127--136"
}

@misc{pareja_unveiling_2024,
	title = {Unveiling the {Secret} {Recipe}: {A} {Guide} {For} {Supervised} {Fine}-{Tuning} {Small} {LLMs}},
	shorttitle = {Unveiling the {Secret} {Recipe}},
	url = {http://arxiv.org/abs/2412.13337},
	doi = {10.48550/arXiv.2412.13337},
	abstract = {The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. Key insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, enabling early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observed no significant difference in performance between phased and stacked training strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets and models, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive environment for LLM research.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Pareja, Aldo and Nayak, Nikhil Shivakumar and Wang, Hao and Killamsetty, Krishnateja and Sudalairaj, Shivchander and Zhao, Wenlong and Han, Seungwook and Bhandwaldar, Abhishek and Xu, Guangxuan and Xu, Kai and Han, Ligong and Inglis, Luke and Srivastava, Akash},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13337 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/L3UI3HLI/Pareja et al. - 2024 - Unveiling the Secret Recipe A Guide For Supervised Fine-Tuning Small LLMs.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/R57ASQYR/2412.html:text/html},
}
@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	doi = {10.48550/arXiv.2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/V4QXQWSV/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/EUDXRJQ8/2305.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/4PSP8NF8/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/TDBDNF9Q/2106.html:text/html},
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


@misc{meta_llama_2024,
	title = {Llama 3.1 {\textbar} {Model} {Cards} and {Prompt} formats},
	url = {https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/},
	abstract = {Llama 3.1 - the most capable open model.},
	language = {en},
	urldate = {2025-05-29},
	journal = {Llama 3.1},
	author = {Meta},
	year = {2024},
	file = {Snapshot:/Users/ricostadeli/Zotero/storage/DJLJNAXU/llama3_1.html:text/html},
}

@misc{micikevicius_mixed_2018,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	doi = {10.48550/arXiv.1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = feb,
	year = {2018},
	note = {arXiv:1710.03740 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/ricostadeli/Zotero/storage/QXI47ELU/Micikevicius et al. - 2018 - Mixed Precision Training.pdf:application/pdf;Snapshot:/Users/ricostadeli/Zotero/storage/GH3D46LW/1710.html:text/html},
}
@book{walton_argumentation_2008,
	edition = {1},
	title = {Argumentation {Schemes}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-0-521-89790-7 978-0-521-72374-9 978-0-511-80203-4},
	url = {https://www.cambridge.org/core/product/identifier/9780511802034/type/book},
	abstract = {This book provides a systematic analysis of many common argumentation schemes and a compendium of 96 schemes. The study of these schemes, or forms of argument that capture stereotypical patterns of human reasoning, is at the core of argumentation research. Surveying all aspects of argumentation schemes from the ground up, the book takes the reader from the elementary exposition in the first chapter to the latest state of the art in the research efforts to formalize and classify the schemes, outlined in the last chapter. It provides a systematic and comprehensive account, with notation suitable for computational applications that increasingly make use of argumentation schemes.},
	urldate = {2025-05-30},
	publisher = {Cambridge University Press},
	author = {Walton, Douglas and Reed, Christopher and Macagno, Fabrizio},
	month = aug,
	year = {2008},
	doi = {10.1017/CBO9780511802034},
}
@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

@misc{hong2024orpo,
	title = {{ORPO}: {Monolithic} {Preference} {Optimization} without {Reference} {Model}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{ORPO}},
	url = {https://arxiv.org/abs/2403.07691},
	doi = {10.48550/ARXIV.2403.07691},
	abstract = {While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20\% on \${\textbackslash}text\{AlpacaEval\}\_\{2.0\}\$ (Figure 1), 66.19\% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-\$α\$ (7B) and Mistral-ORPO-\$β\$ (7B).},
	urldate = {2025-06-01},
	publisher = {arXiv},
	author = {Hong, Jiwoo and Lee, Noah and Thorne, James},
	year = {2024},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}