{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/2_Question_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d44ba819438d4c7f"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate Questions\n",
        "With this notebook we generate the questions for the evaluation dataset.\n",
        "- use this notebook with at least 22 GB of GPU RAM, preferable an L4 GPU in Colab\n",
        "- Push only to GitHub if necessary\n",
        "- To run this notebook you need access to the GitHub Repository and have to create an accesstoken which is required to add in Google Colab as a secret with the name **GITHUB**\n",
        "- If you want to generate questions for the baseline model you have to register on Huggingface for the Meta Llama 3.1 model family because this is a gated repository. After that you have to add you HF token to Google Colab as a Secret with the name **HF_TOKEN**"
      ],
      "id": "d44ba819438d4c7f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "6yd9jnEIPiXa"
      },
      "id": "6yd9jnEIPiXa"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install accelerate bitsandbytes\n",
        "!pip install -U peft"
      ],
      "metadata": {
        "id": "HbEouL7eL0AB"
      },
      "id": "HbEouL7eL0AB",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.252191Z",
          "start_time": "2025-04-28T13:09:02.197619Z"
        },
        "id": "56e5bbe87af65278"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "import logging\n",
        "import transformers\n",
        "import os\n",
        "import gc\n",
        "from bitsandbytes import nn as bnb_nn\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "id": "56e5bbe87af65278",
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub repository for data access"
      ],
      "metadata": {
        "id": "KFcQkC9e0UvG"
      },
      "id": "KFcQkC9e0UvG"
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "7tOU4FCdPmop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9119a9-0e5a-4cfa-9bf4-b06152dcc94f"
      },
      "id": "7tOU4FCdPmop",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 1945, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 1945 (delta 194), reused 157 (delta 157), pack-reused 1735 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1945/1945), 52.59 MiB | 23.64 MiB/s, done.\n",
            "Resolving deltas: 100% (1173/1173), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.322602Z",
          "start_time": "2025-04-28T13:09:03.308445Z"
        },
        "id": "c97a513058176f9f"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "models = {\n",
        "    \"baseline\": {\n",
        "        \"id\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"result_file\": \"Meta-Llama-3.1-8B-Instruct_Baseline_schema_prompt\"\n",
        "    },\n",
        "    \"dpo\": {\n",
        "        \"id\": \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_DPO\",\n",
        "        \"result_file\": \"Meta-Llama-3.1-8B-Instruct_DPO_schema_prompt\"\n",
        "    },\n",
        "    \"dpo_sft\": {\n",
        "        \"id\": \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_DPO\",\n",
        "        \"result_file\": \"Meta-Llama-3.1-8B-Instruct_DPO_SFT_schema_prompt\"\n",
        "    },\n",
        "    \"orpo\": {\n",
        "        \"id\": \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_ORPO\",\n",
        "        \"result_file\": \"Meta-Llama-3.1-8B-Instruct_ORPO_schema_prompt\"\n",
        "    },\n",
        "    \"orpo_sft\": {\n",
        "        \"id\": \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_ORPO_SFT\",\n",
        "        \"result_file\": \"Meta-Llama-3.1-8B-Instruct_ORPO_SFT_schema_prompt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Choose model for inference\n",
        "selected_model = \"baseline\"\n",
        "\n",
        "model_id = models[selected_model][\"id\"]\n",
        "result_file_name = models[selected_model][\"result_file\"]\n",
        "\n",
        "test_dataset_path = f\"/content/NLP2025_CQG/Data/Processed/test.csv\"\n",
        "\n",
        "results_path = os.path.join(os.getcwd(), f\"/content/NLP2025_CQG/Evaluation/Results/results_{result_file_name}.json\")\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Logs/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_path = log_base_path + f\"4_cqs_generation_{result_file_name}.log\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "USE_EXAMPLES_IN_PROMPT = True\n",
        "PUSH_TO_GITHUB = False\n"
      ],
      "id": "c97a513058176f9f",
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "# Log the device info\n",
        "logger.info(\"--------  Start with Baseline Generation  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Results Path: {results_path}')\n",
        "logger.info(f'Log Path: {log_path}')\n",
        "logger.info(\"--------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "zbgFJsxgKi40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8cb38f-6832-46a6-c441-4ad00746f2b0"
      },
      "id": "zbgFJsxgKi40",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--------  Start with Baseline Generation  -------------\n",
            "INFO:__main__:Device selected: cuda\n",
            "INFO:__main__:Results Path: /content/NLP2025_CQG/Evaluation/Results/results_Meta-Llama-3.1-8B-Instruct_Baseline_schema_prompt.json\n",
            "INFO:__main__:Log Path: /content/NLP2025_CQG/Logs/4_cqs_generation_Meta-Llama-3.1-8B-Instruct_Baseline_schema_prompt.log\n",
            "INFO:__main__:--------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Answers"
      ],
      "metadata": {
        "id": "z-iPCguiZEoP"
      },
      "id": "z-iPCguiZEoP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess dataset"
      ],
      "metadata": {
        "id": "I5JlZ5K__oHb"
      },
      "id": "I5JlZ5K__oHb"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Define the schema types\n",
        "schemas = {\n",
        "    \"CauseToEffect\": \"\"\"Examples are:\n",
        "    What if the job is one that will cause their illness to manifest and affect their job performance?\n",
        "    If so, do you think the lack of any of the pieces influence the decisions they make about whether to create the digital widget or which digital widget to create?\n",
        "    If miners and the working class are such a small percentage of the population as to not make a difference in a straight up popular vote, then why do we let them have so much influence?\n",
        "    \"\"\",\n",
        "\n",
        "    \"ExpertOpinion\": \"\"\"'Examples are:\n",
        "    Do you have a scientific source that confirm that women only wants one partner to a greater degree than men?\n",
        "    If you are just stating facts, the surely you can point to some studies that show most women are narcissistic sociopaths?\n",
        "    Where in any scientific textbook or journal have you seen evidence of sex being described as a spectrum?\n",
        "    \"\"\",\n",
        "\n",
        "    \"Analogy\": \"\"\"Examples are:\n",
        "    If so, would these groups resemble what are commonly referred to as races?\n",
        "    If whites are guilty of enjoying the benefits of stolen land after being here for two generations, why would that not mean a 2nd generation American POC not have a similar benefit?\n",
        "    But why is the way her hair naturally grows from her head seen as less professional than chemically altering to mimic straight white European hair?\n",
        "    \"\"\",\n",
        "\n",
        "    \"FearAppeal\": \"\"\"'Examples are:\n",
        "    If he did such a great job making peace, why do we need sanctions to pressure North Korea into stopping its violent threats?\n",
        "    And if we should strive to stop all killings then why would the implement of this murder be spared if it causes a very large number of deaths but was intended to cause none?\n",
        "    If you say you can, then are you implying that you see Islam as being no worse?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Prepare the JSON structure\n",
        "json_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    context = row['input'].strip()\n",
        "    original_id = row['id']\n",
        "\n",
        "    for schema_key, schema_value in schemas.items():\n",
        "        schema_text = f\"\\n\\nSchema definition:\\n{schema_value}\" if USE_EXAMPLES_IN_PROMPT else \"\"\n",
        "        entry = {\n",
        "            \"prompt\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.\"\n",
        "                },\n",
        "                 {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Generate one critical question addressing the provided context. Ensure it matches the schema:{schema_key}{schema_text}.\\n\\nContext: {context} Respond only with the question and nothing else.\"\n",
        "                }\n",
        "\n",
        "            ],\n",
        "            \"id\": f\"{original_id}\",\n",
        "            \"schema\" : schema_key,\n",
        "            \"input\" : context\n",
        "\n",
        "        }\n",
        "        json_data.append(entry)\n",
        "\n",
        "\n",
        "# Save the JSON data to a file\n",
        "temp_output_file = 'processed_dataset.json'\n",
        "with open(temp_output_file, 'w') as f:\n",
        "    json.dump(json_data, f, indent=4)"
      ],
      "metadata": {
        "id": "RYW1D3-5_kq5"
      },
      "id": "RYW1D3-5_kq5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation with transformers"
      ],
      "metadata": {
        "id": "iusfZQx8eQEc"
      },
      "id": "iusfZQx8eQEc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is only needed if you have to clear the GPU cache."
      ],
      "metadata": {
        "id": "bFUhN2nz1ee5"
      },
      "id": "bFUhN2nz1ee5"
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8kQxcTvzijM1"
      },
      "id": "8kQxcTvzijM1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pipeline with quantized model for less GPU usage\n"
      ],
      "metadata": {
        "id": "ui1nT_se4xaS"
      },
      "id": "ui1nT_se4xaS"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\n",
        "            \"load_in_4bit\": True,\n",
        "            \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "        },\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "qOpGGiwQlDj4"
      },
      "id": "qOpGGiwQlDj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single generation\n",
        "This cell is just for debuging purpose to quickly get an overview of the generated questions from the evaluated model."
      ],
      "metadata": {
        "id": "nbHgSdKr4j2Q"
      },
      "id": "nbHgSdKr4j2Q"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(query, max_tokens=128, temperature=0.7, top_p=0.9):\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        query, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    response = outputs[0][\"generated_text\"][len(prompt):]\n",
        "    print(response)\n",
        "    return response\n",
        "\n",
        "with open(temp_output_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "combined_output = {}\n",
        "i = 0\n",
        "for item in data:\n",
        "    print(f\"Number: {i}\")\n",
        "    item_id = item[\"id\"]\n",
        "    schema = item[\"schema\"]\n",
        "    message = item[\"prompt\"]\n",
        "    input = item[\"input\"]\n",
        "    question = get_response(message).strip()\n",
        "\n",
        "    if item_id not in combined_output:\n",
        "        combined_output[item_id] = {\"input\": input, \"cqs\": []}\n",
        "\n",
        "    combined_output[item_id][\"cqs\"].append({\"schema\": schema, \"cq\": question})\n",
        "    i = i + 1\n",
        "    if i == 10:\n",
        "      break\n",
        "\n",
        "\n",
        "# Save the combined questions\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(combined_output, f, indent=4)\n",
        "print(f\"Combined questions saved to {results_path}\")"
      ],
      "metadata": {
        "id": "3UeKeadQj2_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9a6919-67e2-4711-b9c8-652d8ef8dab6"
      },
      "id": "3UeKeadQj2_F",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does increased cooperation with the Muslim community in America actually lead to a more effective counter-terrorism strategy?\n",
            "Number: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you provide evidence from any scientific study that suggests Muslims are more likely to provide valuable intelligence to counter-terrorism efforts than other demographic groups?\n",
            "Number: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If we rely heavily on intelligence from Europe and the Middle East to combat terrorism, wouldn't it be wise to prioritize building trust with our Muslim allies and communities, rather than dismissing or alienating them?\n",
            "Number: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If we're to believe that our Muslim allies are essential for combating terrorism, why have Trump's divisive rhetoric towards Muslims potentially undermined our ability to gather crucial intelligence from them?\n",
            "Number: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does her plan to make the wealthy pay their fair share and close corporate loopholes guarantee that the profits will be equitably distributed among all employees?\n",
            "Number: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you provide evidence from peer-reviewed studies that supports your claim that paid family leave, earned sick days, affordable child care, and debt-free college are essential for supporting people who are struggling to balance family and work?\n",
            "Number: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If we are to provide paid family leave, earned sick days, and affordable child care, why should corporate executives, who have been responsible for creating profits, not be required to share some of those profits with their employees?\n",
            "Number: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If we are truly committed to helping those struggling to balance family and work, why not implement these policies immediately rather than making them a bargaining chip for the wealthy to pay their fair share?\n",
            "Number: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will pursuing a policy of clean energy lead to a stronger economy in the long run?\n",
            "Number: 9\n",
            "Do you have an independent, unbiased scientific source that supports the claim that climate change is a hoax perpetrated by the Chinese?\n",
            "Combined questions saved to /content/NLP2025_CQG/Evaluation/Results/results_Meta-Llama-3.1-8B-Instruct_Baseline_schema_prompt.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch processing\n",
        "This cell generated the questions for the evaluation dataset as a batch processing."
      ],
      "metadata": {
        "id": "CdE1gOeR1493"
      },
      "id": "CdE1gOeR1493"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_responses(queries, max_tokens=512, temperature=0.6, top_p=0.9):\n",
        "    # Prepare the prompts\n",
        "    prompts = [pipeline.tokenizer.apply_chat_template(q, tokenize=False, add_generation_prompt=True) for q in queries]\n",
        "\n",
        "    # Run generation in batches to avoid OOM\n",
        "    all_responses = []\n",
        "    batch_size = 50\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "        # Run the pipeline in batch mode\n",
        "        outputs = pipeline(\n",
        "            batch_prompts,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "\n",
        "        # Extract responses\n",
        "        for prompt, output_list in zip(batch_prompts, outputs):\n",
        "            # Each output_list is a list with a single dictionary\n",
        "            generated_text = output_list[0][\"generated_text\"]\n",
        "            response = generated_text[len(prompt):].strip()\n",
        "            all_responses.append(response)\n",
        "            print(response)\n",
        "\n",
        "    return all_responses\n",
        "\n",
        "# Load the data\n",
        "with open(temp_output_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "queries = [item[\"prompt\"] for item in data]\n",
        "responses = get_responses(queries)\n",
        "\n",
        "# Combine the outputs\n",
        "combined_output = {}\n",
        "for item, response in zip(data, responses):\n",
        "    item_id = item[\"id\"]\n",
        "    schema = item[\"schema\"]\n",
        "    input = item[\"input\"]\n",
        "\n",
        "    if item_id not in combined_output:\n",
        "        combined_output[item_id] = {\"input\": input, \"cqs\": []}\n",
        "\n",
        "    combined_output[item_id][\"cqs\"].append({\"schema\": schema, \"cq\": response})\n",
        "\n",
        "# Save the combined questions\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(combined_output, f, indent=4)\n",
        "\n",
        "print(f\"Combined questions saved to {results_path}\")"
      ],
      "metadata": {
        "id": "P7QFLvBir-sG"
      },
      "id": "P7QFLvBir-sG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.remove(temp_output_file)\n",
        "    print(f\"File '{temp_output_file}' deleted successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{temp_output_file}' not found.\")\n",
        "except PermissionError:\n",
        "    print(f\"Permission denied to delete '{temp_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting file: {e}\")"
      ],
      "metadata": {
        "id": "64jpcwQ2P0QD"
      },
      "id": "64jpcwQ2P0QD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commit & Push"
      ],
      "metadata": {
        "id": "Pqf89mWeUFAl"
      },
      "id": "Pqf89mWeUFAl"
    },
    {
      "cell_type": "code",
      "source": [
        "if PUSH_TO_GITHUB:\n",
        "  os.chdir(\"NLP2025_CQG\")\n",
        "  !ls\n",
        "\n",
        "  !git config --global user.name \"Rico Städeli\"\n",
        "  !git config --global user.email \"rico@yabrriga.ch\"\n",
        "\n",
        "\n",
        "  commit_message = f\"Finetuned generation for {result_file_name}\"\n",
        "  !git add .\n",
        "  !git commit -m \"{commit_message}\"\n",
        "  !git push"
      ],
      "metadata": {
        "id": "yDYJfL1_UEg-"
      },
      "id": "yDYJfL1_UEg-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "name": "2_Question_Generation.ipynb",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}