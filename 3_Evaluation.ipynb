{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMl0sF4eSW2vVYeHs8LUZso",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0780815befad479e8cf6d72c3a6a2851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68f045b3c9834f67835bc51c1d7fede9",
              "IPY_MODEL_1e763041b17f45ea9b58ff6df01f2773",
              "IPY_MODEL_cc0d2310db42452795b6bdef7f7abfb8"
            ],
            "layout": "IPY_MODEL_f5156e7a7bc1471e9606185e144cf4d4"
          }
        },
        "68f045b3c9834f67835bc51c1d7fede9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a50d0fb623e46fba4343c749114118b",
            "placeholder": "​",
            "style": "IPY_MODEL_f74e449098d14eab91d89552ab1af2a4",
            "value": "config.json: 100%"
          }
        },
        "1e763041b17f45ea9b58ff6df01f2773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0afcc0baaaab4c018aaee8dbd1b6be63",
            "max": 1154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2274605856b74dd1b512a474983cdf4b",
            "value": 1154
          }
        },
        "cc0d2310db42452795b6bdef7f7abfb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52bafb43ce914a88a3fd5b64c779a187",
            "placeholder": "​",
            "style": "IPY_MODEL_e6210f42df2045d082d04f5a5a998a63",
            "value": " 1.15k/1.15k [00:00&lt;00:00, 32.3kB/s]"
          }
        },
        "f5156e7a7bc1471e9606185e144cf4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a50d0fb623e46fba4343c749114118b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74e449098d14eab91d89552ab1af2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0afcc0baaaab4c018aaee8dbd1b6be63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2274605856b74dd1b512a474983cdf4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52bafb43ce914a88a3fd5b64c779a187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6210f42df2045d082d04f5a5a998a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f6a5ae9026d4be4bee6646199e1b5ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cc8894819464d28831bc8c4a213fee2",
              "IPY_MODEL_4c5e6d4e67484dfbb23d8d9a1f2d0017",
              "IPY_MODEL_4f014729bc624b259d095eed48f75fe9"
            ],
            "layout": "IPY_MODEL_a4eee85b02ff4df28393cc2493750dc9"
          }
        },
        "8cc8894819464d28831bc8c4a213fee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19060f7350794969b275e63ee1c7cf6a",
            "placeholder": "​",
            "style": "IPY_MODEL_18948916878d46349bd81f199c3999be",
            "value": "model.safetensors:  30%"
          }
        },
        "4c5e6d4e67484dfbb23d8d9a1f2d0017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46022acf3da54a9ca9b51b41ebbb32c7",
            "max": 1629437147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbafe0a173504fca8b906401b63fa4de",
            "value": 492830720
          }
        },
        "4f014729bc624b259d095eed48f75fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_504268049f974503930ee35b2d2825fe",
            "placeholder": "​",
            "style": "IPY_MODEL_730152c61d3d40e4bcb6ae10763c861f",
            "value": " 493M/1.63G [00:07&lt;00:15, 72.9MB/s]"
          }
        },
        "a4eee85b02ff4df28393cc2493750dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19060f7350794969b275e63ee1c7cf6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18948916878d46349bd81f199c3999be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46022acf3da54a9ca9b51b41ebbb32c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbafe0a173504fca8b906401b63fa4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "504268049f974503930ee35b2d2825fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730152c61d3d40e4bcb6ae10763c861f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/3_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "\n",
        "nltk.download('framenet_v17')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "hdCiUk29B-N1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298f2f8d-3c8a-4612-d7a1-c23f6925d188"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/framenet_v17.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "qJX3DjvDEIAJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "AsM1FL3hEJSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355f4d9f-284c-43f9-95bd-d8e3441801cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 996, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 996 (delta 72), reused 54 (delta 54), pack-reused 913 (from 1)\u001b[K\n",
            "Receiving objects: 100% (996/996), 46.08 MiB | 15.96 MiB/s, done.\n",
            "Resolving deltas: 100% (548/548), done.\n",
            "Updating files: 100% (95/95), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "-SxxGXv6EKeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbf22cc-c0ae-4766-bd52-1f7f07ea5166"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_Information_preprocessing.md\t      Doc\n",
            "1_Preprocessing.ipynb\t\t      Evaluation\n",
            "2_Baseline_Generation.ipynb\t      INFORMATION.md\n",
            "2_Information_Baseline_Generation.md  LICENSE\n",
            "3_Evaluation.ipynb\t\t      Logs\n",
            "4_Finetuned_Generation.ipynb\t      README.md\n",
            "5_Evaluation_Analytics.ipynb\t      requirements.txt\n",
            "Data\t\t\t\t      Training\n",
            "Development\t\t\t      Utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_file = \"results_Meta-Llama-3.1-1B-Instruct_SFT_1\""
      ],
      "metadata": {
        "id": "fGwv3TlOIPvd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(os.getcwd(), f\"Evaluation/Results/{result_file}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)"
      ],
      "metadata": {
        "id": "VUPZBmWMEQP2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema"
      ],
      "metadata": {
        "id": "E9RNGP-XBvOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import framenet as fn\n",
        "\n",
        "def get_causal_verbs_from_framenet():\n",
        "    causal_frame_names = [\n",
        "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
        "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\",\n",
        "        \"Causation_scenario\", \"Cause_to_end\", \"Cause_to_resume\",\n",
        "        \"Cause_to_continue\", \"Cause_change_of_consistency\",\"Cause_expansion\",\"Cause_impact\"\n",
        "    ]\n",
        "\n",
        "    causal_verbs = set()\n",
        "    for frame_name in causal_frame_names:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:  # Only verbs\n",
        "                    causal_verbs.add(lu['name'].split('.')[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading frame '{frame_name}': {e}\")\n",
        "\n",
        "    return causal_verbs\n",
        "\n",
        "\n",
        "causal_meta_terms = {\"generalisation\", \"implies\", \"entail\", \"necessitate\", \"follow from\", \"inference\"}\n",
        "alternative_factor_terms = {\"factor\", \"interfere\", \"influence\", \"affect\", \"contribute\", \"complicate\"}\n",
        "\n",
        "\n",
        "def detect_cause_to_effect(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    explanations = []\n",
        "    score = 0\n",
        "\n",
        "    causal_verbs = get_causal_verbs_from_framenet()\n",
        "\n",
        "    has_condition = any(tok.dep_ == \"mark\" and tok.text.lower() in {\"if\", \"when\"} for tok in doc)\n",
        "    if has_condition:\n",
        "        explanations.append(\"✓ Conditional clause detected (e.g., 'if', 'when')\")\n",
        "        score += 3\n",
        "\n",
        "    has_advcl = any(tok.dep_ == \"advcl\" for tok in doc)\n",
        "    if has_advcl:\n",
        "        explanations.append(\"✓ Adverbial clause (likely effect clause) detected\")\n",
        "        score += 2\n",
        "\n",
        "    has_causal_verb_structure = False\n",
        "    for tok in doc:\n",
        "        if tok.lemma_ in causal_verbs and tok.pos_ == \"VERB\":\n",
        "            subj = any(child.dep_ == \"nsubj\" for child in tok.children)\n",
        "            obj = any(child.dep_ == \"dobj\" for child in tok.children)\n",
        "            prep = any(child.dep_ == \"prep\" for child in tok.children)\n",
        "            if subj or obj or prep:\n",
        "                has_causal_verb_structure = True\n",
        "                explanations.append(\n",
        "                    f\"✓ Verb '{tok.lemma_}' is listed in FrameNet under causal frames with subject/object/prep\"\n",
        "                )\n",
        "                score += 3\n",
        "                if subj: score += 0.5\n",
        "                if obj: score += 0.5\n",
        "                if prep: score += 0.5\n",
        "                break\n",
        "\n",
        "    if any(tok.lemma_ in causal_meta_terms for tok in doc):\n",
        "      explanations.append(\"✓ Causal generalisation or implication term detected (e.g., 'implies', 'generalisation')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.lemma_ in alternative_factor_terms for tok in doc):\n",
        "      explanations.append(\"✓ Terms indicating alternative causes or interfering factors detected\")\n",
        "      score += 1\n",
        "\n",
        "    is_causal = has_condition and has_advcl or has_causal_verb_structure\n",
        "    if not is_causal:\n",
        "        causal_phrases = [\"result in\", \"lead to\", \"may cause\", \"because of\", \"due to\",\"given rise to\",\"resulting from\", \"stemming from\", \"driven by\", \"caused by\", \"attributed to\", \"stems from\", \"reason\", \"result of\", \"consequence of\", \"owning to\", \"thus\", \"so\", \"therefore\", \"hence\"  \"thereby\"]\n",
        "        if any(phrase in sentence.lower() for phrase in causal_phrases):\n",
        "            explanations.append(\"✓ Phrase pattern matches known cause-to-effect trigger\")\n",
        "            score += 2\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong CauseToEffect\" if score >= 7 else \"Weak/Partial CauseToEffect\" if score >= 4 else \"Not CauseToEffect\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "mtlx4oR2QYdU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lexical_units_from_frames(frames):\n",
        "    terms = set()\n",
        "    for frame_name in frames:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:\n",
        "                    terms.add(lu['name'].split('.')[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
        "    return terms\n",
        "\n",
        "\n",
        "expert_frames = [\n",
        "    \"Expertise\", \"Judgment_communication\", \"Opinion\",\n",
        "    \"Authority\", \"Statement\", \"Certainty\"\n",
        "]\n",
        "quote_frames = [\"Statement\", \"Judgment_communication\"]\n",
        "clarity_frames = [\"Reasoning\"]\n",
        "evidence_frames = [\"Evidence\", \"Certainty\", \"Causation\"]\n",
        "\n",
        "\n",
        "expert_verbs = get_lexical_units_from_frames(expert_frames)\n",
        "quote_verbs = get_lexical_units_from_frames(quote_frames)\n",
        "clarity_terms = get_lexical_units_from_frames(clarity_frames)\n",
        "evidence_terms = get_lexical_units_from_frames(evidence_frames)\n",
        "\n",
        "def detect_expert_opinion(question):\n",
        "\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "\n",
        "    expert_titles = {\"expert\", \"researcher\", \"scientist\", \"doctor\", \"analyst\", \"professor\", \"Dr.\"}\n",
        "\n",
        "    implicit_expert_terms = {\"study\", \"research\", \"evidence\", \"report\", \"findings\", \"scientific\", \"government\", \"official\", \"paper\", \"survey\", \"data\"}\n",
        "    comparison_cues = {\"consistent\", \"align\", \"similar\", \"agree\", \"disagree\", \"corroborate\", \"conflict\"}\n",
        "    technical_request_verbs = {\"define\", \"explain\", \"describe\", \"elaborate\", \"clarify\"}\n",
        "    assertion_verbs = {\"assert\", \"affirm\", \"pronounce\", \"declare\", \"maintain\", \"claim\", \"state\"}\n",
        "    reference_terms = {\"quote\", \"reference\", \"cite\", \"check\", \"verify\", \"source\"}\n",
        "    domain_terms = {\"science\", \"scientific\", \"domain\", \"field\", \"discipline\", \"area\", \"sector\"}\n",
        "\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
        "            if any(title in ent.text.lower() for title in expert_titles):\n",
        "                explanations.append(f\"✓ Expert entity detected: '{ent.text}'\")\n",
        "                score += 3\n",
        "                break\n",
        "\n",
        "    if any(tok.lemma_ in expert_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Detected expert-related verb from FrameNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_ in quote_verbs for tok in doc):\n",
        "        explanations.append(\"✓ Quotation or claim verb found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in clarity_terms for tok in doc):\n",
        "        explanations.append(\"✓ Clarity/definition markers found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in evidence_terms for tok in doc):\n",
        "        explanations.append(\"✓ Evidence or support-related terms found\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in implicit_expert_terms for tok in doc):\n",
        "      explanations.append(\"✓ Implicit expert-related term detected (e.g., 'study', 'government')\")\n",
        "      score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in comparison_cues for tok in doc):\n",
        "      explanations.append(\"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\")\n",
        "      score += 0.5\n",
        "\n",
        "    if any(tok.lemma_.lower() in technical_request_verbs for tok in doc):\n",
        "      explanations.append(\"✓ Technical explanation request detected (e.g., 'define', 'explain')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.dep_ == \"attr\" and tok.lemma_ == \"expert\" for tok in doc):\n",
        "      explanations.append(\"✓ Predicate nominative indicating expertise detected (e.g., 'X is an expert')\")\n",
        "      score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in assertion_verbs for tok in doc):\n",
        "      explanations.append(\"✓ Assertion or claim verb detected (e.g., 'assert', 'affirm')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.lemma_.lower() in reference_terms for tok in doc):\n",
        "      explanations.append(\"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.lemma_.lower() in domain_terms for tok in doc):\n",
        "      explanations.append(\"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\")\n",
        "      score += 1\n",
        "\n",
        "    label = \"Strong Expert Opinion\" if score >= 7 else \"Weak/Partial Expert Opinion\" if score >= 4 else \"Not Expert Opinion\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "3n5B9kQlQkyA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "analogy_synsets = [wn.synset('similar.a.01'), wn.synset('analogy.n.01'), wn.synset('compare.v.01')]\n",
        "\n",
        "comparison_frames = [\"Similarity\"]\n",
        "contrast_frames = [\"Categorization\"]\n",
        "evidence_frames = [\"Evidence\", \"Judgment_communication\"]\n",
        "\n",
        "comparison_verbs = get_lexical_units_from_frames(comparison_frames)\n",
        "contrast_verbs = get_lexical_units_from_frames(contrast_frames)\n",
        "evidence_verbs = get_lexical_units_from_frames(evidence_frames)\n",
        "\n",
        "def is_semantically_analogical(token):\n",
        "    token_synsets = wn.synsets(token.lemma_)\n",
        "    for s in token_synsets:\n",
        "        for analogy_syn in analogy_synsets:\n",
        "            if s.path_similarity(analogy_syn) and s.path_similarity(analogy_syn) > 0.3:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "analogy_context_cues = {\"respect\", \"in which\", \"such that\", \"with regard to\", \"in terms of\"}\n",
        "\n",
        "analogy_force_cues = {\"undermine\", \"weaken\", \"strengthen\", \"force of similarity\", \"degree of analogy\"}\n",
        "\n",
        "analogy_nouns = {\"analogy\", \"comparison\", \"parallel\", \"similarity\", \"analogue\"}\n",
        "\n",
        "def detect_analogy_question(question):\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "    noun_chunks = list(doc.noun_chunks)\n",
        "\n",
        "    if any(tok.lemma_ in comparison_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Comparison verb detected from FrameNet\")\n",
        "        score += 2.5\n",
        "\n",
        "    entity_tokens = [tok for tok in doc if tok.pos_ in {\"PROPN\", \"NOUN\"}]\n",
        "    if len(set(tok.lemma_ for tok in entity_tokens)) >= 2:\n",
        "        explanations.append(\"✓ Contains at least two distinct concepts/entities\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in contrast_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Contrast or difference verb detected from FrameNet\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in evidence_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Evidence or justification verb found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.tag_ == \"MD\" for tok in doc):\n",
        "        score += 0.5\n",
        "\n",
        "    if len(noun_chunks) >= 2 and any(tok.lemma_ in {\"similar\", \"like\", \"as\"} for tok in doc):\n",
        "        explanations.append(\"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\")\n",
        "        score += 3\n",
        "\n",
        "    if any(tok.text.lower() == \"if\" for tok in doc):\n",
        "        explanations.append(\"✓ Conditional structure suggesting hypothetical reasoning\")\n",
        "        score += 1\n",
        "\n",
        "    if any(is_semantically_analogical(tok) for tok in doc if tok.pos_ in {\"ADJ\", \"NOUN\", \"VERB\"}):\n",
        "        explanations.append(\"✓ Semantic similarity to analogy-related terms detected via WordNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.dep_ in {\"prep\", \"relcl\"} and tok.lemma_ in {\"compare\", \"similar\"} for tok in doc):\n",
        "        explanations.append(\"✓ Syntactic cue of analogy (e.g., 'compared with', 'similar to')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(phrase in question.lower() for phrase in analogy_context_cues):\n",
        "      explanations.append(\"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\")\n",
        "      score += 0.5\n",
        "\n",
        "    if any(tok.lemma_ in analogy_force_cues for tok in doc):\n",
        "      explanations.append(\"✓ Analogy evaluation term detected (e.g., 'undermine', 'strengthen')\")\n",
        "      score += 0.5\n",
        "\n",
        "    if any(tok.lemma_ in analogy_nouns for tok in doc if tok.pos_ == \"NOUN\"):\n",
        "      explanations.append(\"✓ Explicit analogy noun detected (e.g., 'analogy', 'comparison')\")\n",
        "      score += 2\n",
        "\n",
        "    if any(tok.dep_ == \"neg\" for tok in doc):\n",
        "      if any(tok.lemma_ in {\"similar\", \"compare\", \"alike\", \"match\"} for tok in doc):\n",
        "          explanations.append(\"✓ Negated comparison detected (suggesting analogy breakdown)\")\n",
        "          score += 1\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong Analogy Question\" if score >= 7 else \"Weak/Partial Analogy Question\" if score >= 4 else \"Not Analogy Question\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "PYEMsIgKQt11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8acb890-bfe6-41df-84b0-60031774587b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_fear_related(token):\n",
        "    syns = wn.synsets(token.lemma_)\n",
        "    for s in syns:\n",
        "        if any(s.path_similarity(wn.synset('danger.n.01')) or s.path_similarity(wn.synset('problem.n.01')) or\n",
        "               s.path_similarity(wn.synset('fear.n.01')) or s.path_similarity(wn.synset('harm.n.01')) or\n",
        "               s.path_similarity(wn.synset('threat.n.01')) for s in syns):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ---- FrameNet Utility ----\n",
        "def get_lexical_units_from_frames(frames):\n",
        "    terms = set()\n",
        "    for frame_name in frames:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:\n",
        "                    terms.add(lu['name'].split('.')[0])\n",
        "        except:\n",
        "            continue\n",
        "    return terms\n",
        "\n",
        "# ---- Relevant Lexical Resources ----\n",
        "causal_frames = [\"Causation\", \"Cause_to_start\", \"Preventing\", \"Risk\", \"Threaten\", \"Danger\"]\n",
        "causal_verbs = get_lexical_units_from_frames(causal_frames)\n",
        "\n",
        "fear_keywords = {\"danger\", \"threat\", \"risky\", \"harm\", \"catastrophe\", \"crisis\", \"ruin\", \"fear\", \"worse\", \"bad\", \"fatal\", \"negative\", \"die\", \"death\"}\n",
        "preventive_keywords = {\"prevent\", \"avoid\", \"stop\", \"ban\", \"rescue\", \"save\", \"protect\"}\n",
        "\n",
        "urgency_keywords = {\"immediately\", \"soon\", \"before it's too late\", \"critical\", \"urgent\", \"suddenly\", \"unexpectedly\"}\n",
        "\n",
        "possibility_terms = {\"possible\", \"possibility\", \"likely\", \"likelihood\", \"chance\", \"probability\", \"conceivable\", \"potential\", \"can\", \"could\", \"might\", \"may\", \"able\"}\n",
        "\n",
        "\n",
        "def detect_fear_appeal_question(question):\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "\n",
        "    if any(tok.lemma_.lower() in fear_keywords for tok in doc):\n",
        "        explanations.append(\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\")\n",
        "        score += 3\n",
        "\n",
        "    if any(tok.lemma_.lower() in preventive_keywords for tok in doc):\n",
        "        explanations.append(\"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_ in causal_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Causal/preventive verb from FrameNet detected\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.text.lower() in {\"if\", \"unless\"} for tok in doc):\n",
        "        explanations.append(\"✓ Conditional clause found (e.g., 'if', 'unless')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(is_fear_related(tok) for tok in doc if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"}):\n",
        "        explanations.append(\"✓ Semantic fear-related concept detected via WordNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(phrase in question.lower() for phrase in urgency_keywords):\n",
        "        explanations.append(\"✓ Urgency marker detected (e.g., 'immediately', 'before it's too late')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in possibility_terms for tok in doc):\n",
        "        explanations.append(\"✓ Possibility-related term detected (e.g., 'possible', 'feasible', 'chance')\")\n",
        "        score += 1\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong Fear Appeal\" if score >= 7 else \"Weak/Partial Fear Appeal\" if score >= 4 else \"Not Fear Appeal\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "2NcmmmqfBmVb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"How strong is the generalisation that if the world exists then we exist?\",\n",
        "    \"Are there other factors in this particular case that could have interfered with the event of existance of the world?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_cause_to_effect(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "DBBJEXBVJUyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7faa601-b018-4ab9-b334-3f8fb22d1917"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How strong is the generalisation that if the world exists then we exist?\n",
            "Result: ('Weak/Partial CauseToEffect', 6, [\"✓ Conditional clause detected (e.g., 'if', 'when')\", '✓ Adverbial clause (likely effect clause) detected', \"✓ Causal generalisation or implication term detected (e.g., 'implies', 'generalisation')\"])\n",
            "\n",
            "Question: Are there other factors in this particular case that could have interfered with the event of existance of the world?\n",
            "Result: ('Not CauseToEffect', 1, ['✓ Terms indicating alternative causes or interfering factors detected'])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Is Peter a genuine expert in science?\",\n",
        "    \"Did Peter really assert that the world exists?\",\n",
        "    \"Is Peter’s pronouncement directly quoted? If not, is a reference to the original source given? Can it be checked?\",\n",
        "    \"If Peter’s advice is not quoted, does it look like important information or qualifications may have been left out?\",\n",
        "    \"Is what Peter said clear? Are there technical terms used that are not explained clearly?\",\n",
        "    \"Is existance of the world relevant to domain science?\",\n",
        "    \"Is existance of the world consistent with what other experts in <domainD> say?\",\n",
        "    \"Is existance of the world consistent with known evidence in <domainD>?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_expert_opinion(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "uPYflc0RINmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7c8f43-982e-4c47-de78-e82114a49a45"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Is Peter a genuine expert in science?\n",
            "Result: ('Not Expert Opinion', 3, [\"✓ Predicate nominative indicating expertise detected (e.g., 'X is an expert')\", \"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\"])\n",
            "\n",
            "Question: Did Peter really assert that the world exists?\n",
            "Result: ('Weak/Partial Expert Opinion', 4, ['✓ Detected expert-related verb from FrameNet', '✓ Quotation or claim verb found', \"✓ Assertion or claim verb detected (e.g., 'assert', 'affirm')\"])\n",
            "\n",
            "Question: Is Peter’s pronouncement directly quoted? If not, is a reference to the original source given? Can it be checked?\n",
            "Result: ('Not Expert Opinion', 1, [\"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\"])\n",
            "\n",
            "Question: If Peter’s advice is not quoted, does it look like important information or qualifications may have been left out?\n",
            "Result: ('Not Expert Opinion', 3, ['✓ Evidence or support-related terms found', \"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\"])\n",
            "\n",
            "Question: Is what Peter said clear? Are there technical terms used that are not explained clearly?\n",
            "Result: ('Weak/Partial Expert Opinion', 4, ['✓ Detected expert-related verb from FrameNet', '✓ Quotation or claim verb found', \"✓ Technical explanation request detected (e.g., 'define', 'explain')\"])\n",
            "\n",
            "Question: Is existance of the world relevant to domain science?\n",
            "Result: ('Not Expert Opinion', 1, [\"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\"])\n",
            "\n",
            "Question: Is existance of the world consistent with what other experts in <domainD> say?\n",
            "Result: ('Not Expert Opinion', 3.5, ['✓ Detected expert-related verb from FrameNet', '✓ Quotation or claim verb found', \"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\"])\n",
            "\n",
            "Question: Is existance of the world consistent with known evidence in <domainD>?\n",
            "Result: ('Weak/Partial Expert Opinion', 6.5, ['✓ Detected expert-related verb from FrameNet', '✓ Evidence or support-related terms found', \"✓ Implicit expert-related term detected (e.g., 'study', 'government')\", \"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\"])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Are frogs and horses similar in the respect cited?\",\n",
        "    \"Is the existance of the world true in horses?\",\n",
        "    \"Are there differences between horses and frogs that would tend to undermine the force of the similarity cited?\",\n",
        "    \"Is there some other case that is also similar to horses, but in which frog is false?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_analogy_question(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "hYsWnbjWFwhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b170a1-b218-4f68-f377-3450ac8f0c35"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Are frogs and horses similar in the respect cited?\n",
            "Result: ('Strong Analogy Question', 7.5, ['✓ Contains at least two distinct concepts/entities', '✓ Evidence or justification verb found', \"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\", '✓ Semantic similarity to analogy-related terms detected via WordNet', \"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\"])\n",
            "\n",
            "Question: Is the existance of the world true in horses?\n",
            "Result: ('Not Analogy Question', 3, ['✓ Contains at least two distinct concepts/entities', '✓ Semantic similarity to analogy-related terms detected via WordNet'])\n",
            "\n",
            "Question: Are there differences between horses and frogs that would tend to undermine the force of the similarity cited?\n",
            "Result: ('Weak/Partial Analogy Question', 5.0, ['✓ Contains at least two distinct concepts/entities', '✓ Evidence or justification verb found', \"✓ Analogy evaluation term detected (e.g., 'undermine', 'strengthen')\", \"✓ Explicit analogy noun detected (e.g., 'analogy', 'comparison')\"])\n",
            "\n",
            "Question: Is there some other case that is also similar to horses, but in which frog is false?\n",
            "Result: ('Weak/Partial Analogy Question', 6.5, ['✓ Contains at least two distinct concepts/entities', \"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\", '✓ Semantic similarity to analogy-related terms detected via WordNet', \"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\"])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Is the world existing bad? Why and to whom is it bad?\",\n",
        "    \"Is the world existing away to prevent people from dying?\",\n",
        "    \"Is it practically possible for world existing to happen?\",\n",
        "    \"Are there other consequences from the world existing?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_fear_appeal_question(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "rKfapoMPD90G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb14aed-ba69-4e70-a6a7-727f198ade66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Is the world existing bad? Why and to whom is it bad?\n",
            "Result: ('Weak/Partial Fear Appeal', 5, [\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\", '✓ Semantic fear-related concept detected via WordNet'])\n",
            "\n",
            "Question: Is the world existing away to prevent people from dying?\n",
            "Result: ('Strong Fear Appeal', 7, [\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\", \"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\", '✓ Semantic fear-related concept detected via WordNet'])\n",
            "\n",
            "Question: Is it practically possible for world existing to happen?\n",
            "Result: ('Not Fear Appeal', 3, ['✓ Semantic fear-related concept detected via WordNet', \"✓ Possibility-related term detected (e.g., 'possible', 'feasible', 'chance')\"])\n",
            "\n",
            "Question: Are there other consequences from the world existing?\n",
            "Result: ('Not Fear Appeal', 2, ['✓ Semantic fear-related concept detected via WordNet'])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Cu1EtwNuB6Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule evaluation"
      ],
      "metadata": {
        "id": "W54yvjfKBZoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_max_length(question, max_chars=120, max_words=20):\n",
        "    return len(question) <= max_chars and len(question.split()) <= max_words"
      ],
      "metadata": {
        "id": "Tc1qRyhdBY-2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg97rXCjFr2V",
        "outputId": "9fcc7dae-f4df-45b5-9798-e28c4686567a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.9.3-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2025.4.26)\n",
            "Downloading language_tool_python-2.9.3-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install openjdk-17-jdk\n",
        "!java -version"
      ],
      "metadata": {
        "id": "x2nFfvaqICzE",
        "outputId": "cc5a85ef-9a7f-4577-ddae-3bfd6670c0a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connecting to cloud.\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 129 kB/129 kB 100%] [Connecting to cloud.\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\u001b[33m\r0% [3 InRelease 28.7 kB/128 kB 22%] [Connected to cloud.r-project.org (65.9.86.\u001b[0m\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (65.9.86.109)] [Conn\u001b[0m\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\u001b[33m\r0% [4 InRelease 15.6 kB/127 kB 12%] [Connected to cloud.r-project.org (65.9.86.\u001b[0m\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,363 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,934 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,546 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,245 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,517 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [77.3 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,725 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,944 kB]\n",
            "Fetched 30.6 MB in 7s (4,158 kB/s)\n",
            "Reading package lists... Done\n",
            "^C\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1\n",
            "  openjdk-17-jdk-headless openjdk-17-jre openjdk-17-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-17-demo openjdk-17-source visualvm libnss-mdns\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-17-jdk\n",
            "  openjdk-17-jdk-headless openjdk-17-jre openjdk-17-jre-headless x11-utils\n",
            "0 upgraded, 12 newly installed, 0 to remove and 89 not upgraded.\n",
            "Need to get 126 MB of archives.\n",
            "After this operation, 288 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre-headless amd64 17.0.15+6~us1-0ubuntu1~22.04 [48.3 MB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre amd64 17.0.15+6~us1-0ubuntu1~22.04 [232 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk-headless amd64 17.0.15+6~us1-0ubuntu1~22.04 [71.3 MB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk amd64 17.0.15+6~us1-0ubuntu1~22.04 [2,403 kB]\n",
            "Fetched 126 MB in 4s (34.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 12.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../02-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../03-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../04-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../05-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../06-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../07-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-17-jre-headless:amd64.\n",
            "Preparing to unpack .../08-openjdk-17-jre-headless_17.0.15+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jre-headless:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-17-jre:amd64.\n",
            "Preparing to unpack .../09-openjdk-17-jre_17.0.15+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jre:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-17-jdk-headless:amd64.\n",
            "Preparing to unpack .../10-openjdk-17-jdk-headless_17.0.15+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk-headless:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-17-jdk:amd64.\n",
            "Preparing to unpack .../11-openjdk-17-jdk_17.0.15+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up openjdk-17-jre-headless:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up openjdk-17-jre:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-17-jdk-headless:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "Setting up openjdk-17-jdk:amd64 (17.0.15+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "openjdk version \"17.0.15\" 2025-04-15\n",
            "OpenJDK Runtime Environment (build 17.0.15+6-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.15+6-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "import re\n",
        "\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "def check_grammar_tool(question):\n",
        "    matches = tool.check(question)\n",
        "    return len(matches) == 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HgIhU3GE9_f",
        "outputId": "16675b37-61b8-47fe-8d7e-284f1d1a49c0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool latest: 100%|██████████| 252M/252M [00:12<00:00, 19.7MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmppukjce6h.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://internal1.languagetool.org/snapshots/LanguageTool-latest-snapshot.zip to /root/.cache/language_tool_python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_no_newlines(question):\n",
        "    return '\\n' not in question"
      ],
      "metadata": {
        "id": "jJhyM26lFE7J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_single_question(question):\n",
        "    return question.count('?') == 1 and not re.search(r\"\\?\\s*(and|or)\\s\", question.lower())"
      ],
      "metadata": {
        "id": "P-fopGMmFG3x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "def is_grammatical_question(text, threshold=0.7):\n",
        "    candidate_labels = [\"grammatically correct question\", \"not a question\", \"grammatically incorrect question\"]\n",
        "    result = classifier(text, candidate_labels)\n",
        "\n",
        "    top_label = result['labels'][0]\n",
        "    top_score = result['scores'][0]\n",
        "\n",
        "    return top_label == \"grammatically correct question\" and top_score >= threshold"
      ],
      "metadata": {
        "id": "nCbkK5VRLd0Z",
        "outputId": "78709b91-29e6-431d-9287-61dc3046f14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "referenced_widgets": [
            "0780815befad479e8cf6d72c3a6a2851",
            "68f045b3c9834f67835bc51c1d7fede9",
            "1e763041b17f45ea9b58ff6df01f2773",
            "cc0d2310db42452795b6bdef7f7abfb8",
            "f5156e7a7bc1471e9606185e144cf4d4",
            "6a50d0fb623e46fba4343c749114118b",
            "f74e449098d14eab91d89552ab1af2a4",
            "0afcc0baaaab4c018aaee8dbd1b6be63",
            "2274605856b74dd1b512a474983cdf4b",
            "52bafb43ce914a88a3fd5b64c779a187",
            "e6210f42df2045d082d04f5a5a998a63",
            "4f6a5ae9026d4be4bee6646199e1b5ae",
            "8cc8894819464d28831bc8c4a213fee2",
            "4c5e6d4e67484dfbb23d8d9a1f2d0017",
            "4f014729bc624b259d095eed48f75fe9",
            "a4eee85b02ff4df28393cc2493750dc9",
            "19060f7350794969b275e63ee1c7cf6a",
            "18948916878d46349bd81f199c3999be",
            "46022acf3da54a9ca9b51b41ebbb32c7",
            "bbafe0a173504fca8b906401b63fa4de",
            "504268049f974503930ee35b2d2825fe",
            "730152c61d3d40e4bcb6ae10763c861f"
          ]
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0780815befad479e8cf6d72c3a6a2851"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f6a5ae9026d4be4bee6646199e1b5ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c0884dba6f96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zero-shot-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-mnli\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_grammatical_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0madapter_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0madapter_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4258\u001b[0m             )\n\u001b[1;32m   4259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4260\u001b[0;31m         checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n\u001b[0m\u001b[1;32m   4261\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4262\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_resolved_checkpoint_files\u001b[0;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                 }\n\u001b[0;32m--> 997\u001b[0;31m                 \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_filenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         )\n\u001b[1;32m   1007\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1721\u001b[0m                 )\n\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1724\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                     \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;31m# Some data has been downloaded from the server so we reset the number of retries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_grammar_check(question):\n",
        "    try:\n",
        "        lt_ok = check_grammar_tool(question)\n",
        "    except Exception as e:\n",
        "        print(\"LanguageTool not available or error:\", e)\n",
        "        lt_ok = True\n",
        "\n",
        "    spacy_ok = check_basic_grammar_syntax(question)\n",
        "\n",
        "    zero_shot_ok = is_grammatical_question(question, threshold=0.7)\n",
        "\n",
        "    checks = [lt_ok, spacy_ok, zero_shot_ok]\n",
        "    return sum(checks) >= 2"
      ],
      "metadata": {
        "id": "g-VMMgPaL0-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_no_consecutive_repeated_words(question):\n",
        "    words = question.lower().split()\n",
        "    for i in range(len(words) - 1):\n",
        "        if words[i] == words[i + 1]:\n",
        "            print(f\"Consecutive repetition detected: '{words[i]}'\")\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "UipQFQgCJC6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_question(question):\n",
        "    return all([\n",
        "        check_max_length(question),\n",
        "        check_grammar_tool(question),\n",
        "        check_no_newlines(question),\n",
        "        check_single_question(question),\n",
        "        check_no_consecutive_repeated_words(question),\n",
        "        combined_grammar_check(question)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "Uif42CT5FOzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"Why sky blue is?\"\n",
        "print(validate_question(q))"
      ],
      "metadata": {
        "id": "z7H6gUYTHyOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### syntax"
      ],
      "metadata": {
        "id": "q86nwut_q8CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_schema(question):\n",
        "    _, cte_score, _ = detect_cause_to_effect(question)\n",
        "\n",
        "    _, expert_score, _ = detect_expert_opinion(question)\n",
        "\n",
        "    _, analogy_score, _ = detect_analogy_question(question)\n",
        "\n",
        "    _, fear_score, _ = detect_fear_appeal_question(question)\n",
        "\n",
        "    return cte_score, expert_score, analogy_score, fear_score"
      ],
      "metadata": {
        "id": "w4-DTU6tWL6u"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### semantic"
      ],
      "metadata": {
        "id": "JMJVXUjyrDDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "id": "EbXkQXCGt3Zi",
        "outputId": "dc4e5fd7-8ca7-446e-ac06-3b1c0f25c3fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=86b3b433197618194d75c4c32c28aeee46613d0f84c1f0a7a45f5b1ec1df0d8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kxnD-k2BrmTV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\")\n",
        "\n",
        "def token_overlap(input_text, question):\n",
        "    doc1 = nlp(input_text)\n",
        "    doc2 = nlp(question)\n",
        "    tokens1 = set([token.lemma_.lower() for token in doc1 if not token.is_stop and token.is_alpha])\n",
        "    tokens2 = set([token.lemma_.lower() for token in doc2 if not token.is_stop and token.is_alpha])\n",
        "    intersection = tokens1.intersection(tokens2)\n",
        "    union = tokens1.union(tokens2)\n",
        "    jaccard = len(intersection) / len(union) if union else 0\n",
        "    return jaccard\n",
        "\n",
        "def cosine_similarity(input_text, question):\n",
        "    embeddings = sbert.encode([input_text, question], convert_to_tensor=True)\n",
        "    score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "    return score\n",
        "\n",
        "def entailment_score(input_text, question):\n",
        "    inputs = tokenizer.encode_plus(input_text, question, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    probs = torch.softmax(logits, dim=1).squeeze()\n",
        "    entailment_prob = probs[2].item()\n",
        "    return entailment_prob\n",
        "\n",
        "def evaluate_context_closeness(input_text, question):\n",
        "    scores = {\n",
        "        \"token_overlap\": token_overlap(input_text, question),\n",
        "        \"cosine_similarity\": cosine_similarity(input_text, question),\n",
        "        \"entailment_score\": entailment_score(input_text, question),\n",
        "    }\n",
        "    scores[\"average_score\"] = np.mean(list(scores.values()))\n",
        "\n",
        "    verdict = True if (\n",
        "        scores[\"token_overlap\"] > 0.3 and\n",
        "        scores[\"cosine_similarity\"] > 0.6 and\n",
        "        scores[\"entailment_score\"] > 0.7\n",
        "    ) else False\n",
        "\n",
        "    return scores, verdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HtdwfjUrk4v",
        "outputId": "e33931cc-4c6e-4d81-8174-3cdb59642333"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Context Closeness Evaluation ---\n",
            "token_overlap: 0.3333\n",
            "cosine_similarity: 0.7829\n",
            "entailment_score: 0.7297\n",
            "average_score: 0.6153\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "def rouge_score(context, question):\n",
        "    scores = scorer.score(context, question)\n",
        "    return scores['rougeL'].fmeasure\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
        "\n",
        "def word_overlap_score(context, question):\n",
        "    ctx_tokens = set(clean_and_tokenize(context))\n",
        "    q_tokens = set(clean_and_tokenize(question))\n",
        "    if not q_tokens:\n",
        "        return 0.0\n",
        "    return len(ctx_tokens & q_tokens) / len(q_tokens)\n",
        "\n",
        "def semantic_score(context, question):\n",
        "\n",
        "    embeddings = embedder.encode([context, question], convert_to_tensor=True)\n",
        "    cos_sim_score = float(util.pytorch_cos_sim(embeddings[0], embeddings[1]).item())\n",
        "\n",
        "    w_score = word_overlap_score(context, question)\n",
        "    r_score = rouge_score(context, question)\n",
        "\n",
        "    hybrid_score = float(0.6 * cos_sim_score + 0.2 * w_score + 0.2 * r_score)\n",
        "    verdict = True if hybrid_score >= threshold else False\n",
        "\n",
        "    return cos_sim_score, w_score, r_score, hybrid_score, verdict"
      ],
      "metadata": {
        "id": "sFYXWCYpV8rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### total evaluation"
      ],
      "metadata": {
        "id": "Mpdh5jdprTuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_schema_and_semantics(context, question):\n",
        "    cte_score, expert_score, analogy_score, fear_score = classify_schema(question)\n",
        "    return {\n",
        "        \"CauseToEffect\": cte_score,\n",
        "        \"ExpertOpinion\": expert_score,\n",
        "        \"Analogy\": analogy_score,\n",
        "        \"FearAppeal\": fear_score,\n",
        "    }"
      ],
      "metadata": {
        "id": "Lpo41tG-WJTM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, obj in results.items():\n",
        "    context_text = obj[\"input\"]\n",
        "    for cq_entry in obj[\"cqs\"]:\n",
        "        schema = cq_entry.get(\"schema\")\n",
        "        question = cq_entry[\"cq\"]\n",
        "\n",
        "        scores = classify_schema_and_semantics(context_text, question)\n",
        "\n",
        "        cq_entry[\"CauseToEffect\"] = scores[\"CauseToEffect\"]\n",
        "        cq_entry[\"ExpertOpinion\"] = scores[\"ExpertOpinion\"]\n",
        "        cq_entry[\"Analogy\"] = scores[\"Analogy\"]\n",
        "        cq_entry[\"FearAppeal\"] = scores[\"FearAppeal\"]\n",
        "\n",
        "        is_critical = False\n",
        "        if schema and cq_entry.get(schema, 0) >= 7:\n",
        "            is_critical = True\n",
        "        cq_entry[\"is_critical\"] = is_critical\n"
      ],
      "metadata": {
        "id": "ryDIUlthBsWV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Git"
      ],
      "metadata": {
        "id": "R5qFh12Tq00T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(os.getcwd(), f\"Evaluation/Scored/{result_file}_eval.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(results, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "_TwYIMicHwUF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Showcas\"\n",
        "!git config --global user.email \"cedric.bohni@gmx.de\"\n",
        "\n",
        "\n",
        "commit_message = f\"evaluate CQs for {result_file}\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "jZCHLXO2HtH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff11c62c-98f8-4285-ee20-3cf7ccbb6143"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main f195a5c] evaluate CQs for results_Meta-Llama-3.1-1B-Instruct_SFT_1\n",
            " 1 file changed, 3720 deletions(-)\n",
            "To https://github.com/RicoStaedeli/NLP2025_CQG.git\n",
            " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/RicoStaedeli/NLP2025_CQG.git'\n",
            "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
            "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
            "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
            "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
            "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
          ]
        }
      ]
    }
  ]
}