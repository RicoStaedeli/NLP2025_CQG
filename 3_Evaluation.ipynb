{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNO1qgza1XKimIQ5rvQlgYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/3_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "\n",
        "nltk.download('framenet_v17')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "hdCiUk29B-N1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8919ba-050f-47e8-e437-72ab9e19d7de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/framenet_v17.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "qJX3DjvDEIAJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "AsM1FL3hEJSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89babcc3-e4ea-4180-c328-48fc29a3329c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 797, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 797 (delta 48), reused 35 (delta 35), pack-reused 737 (from 2)\u001b[K\n",
            "Receiving objects: 100% (797/797), 25.27 MiB | 24.44 MiB/s, done.\n",
            "Resolving deltas: 100% (400/400), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "-SxxGXv6EKeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5424fc32-c9e1-40ee-de4f-dec8bbb4fe7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_Information_preprocessing.md\t      Development\n",
            "1_Preprocessing.ipynb\t\t      Doc\n",
            "2_Baseline_Generation.ipynb\t      Evaluation\n",
            "2_Information_Baseline_Generation.md  INFORMATION.md\n",
            "3a_Finetuned_CQS_generation.ipynb     LICENSE\n",
            "3b_Finetune_Evaluation.ipynb\t      Logs\n",
            "3_Evaluation.ipynb\t\t      README.md\n",
            "4_Finetuned_Generation.ipynb\t      requirements.txt\n",
            "5_Evaluation_Analytics.ipynb\t      Training\n",
            "Data\t\t\t\t      Utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_file = \"results_schema_Baseline_Meta-Llama-3.1-8B-Instruct-bnb-4bit\""
      ],
      "metadata": {
        "id": "fGwv3TlOIPvd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(os.getcwd(), f\"Evaluation/Results/{result_file}.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)"
      ],
      "metadata": {
        "id": "VUPZBmWMEQP2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema"
      ],
      "metadata": {
        "id": "E9RNGP-XBvOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import framenet as fn\n",
        "\n",
        "def get_causal_verbs_from_framenet():\n",
        "    causal_frame_names = [\n",
        "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
        "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\",\n",
        "        \"Causation_scenario\", \"Cause_to_end\", \"Cause_to_resume\",\n",
        "        \"Cause_to_continue\", \"Cause_change_of_consistency\",\"Cause_expansion\",\"Cause_impact\"\n",
        "    ]\n",
        "\n",
        "    causal_verbs = set()\n",
        "    for frame_name in causal_frame_names:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:  # Only verbs\n",
        "                    causal_verbs.add(lu['name'].split('.')[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading frame '{frame_name}': {e}\")\n",
        "\n",
        "    return causal_verbs\n",
        "\n",
        "\n",
        "causal_meta_terms = {\"generalisation\", \"implies\", \"entail\", \"necessitate\", \"follow from\", \"inference\"}\n",
        "alternative_factor_terms = {\"factor\", \"interfere\", \"influence\", \"affect\", \"contribute\", \"complicate\"}\n",
        "\n",
        "\n",
        "def detect_cause_to_effect(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    explanations = []\n",
        "    score = 0\n",
        "\n",
        "    causal_verbs = get_causal_verbs_from_framenet()\n",
        "\n",
        "    has_condition = any(tok.dep_ == \"mark\" and tok.text.lower() in {\"if\", \"when\"} for tok in doc)\n",
        "    if has_condition:\n",
        "        explanations.append(\"✓ Conditional clause detected (e.g., 'if', 'when')\")\n",
        "        score += 3\n",
        "\n",
        "    has_advcl = any(tok.dep_ == \"advcl\" for tok in doc)\n",
        "    if has_advcl:\n",
        "        explanations.append(\"✓ Adverbial clause (likely effect clause) detected\")\n",
        "        score += 2\n",
        "\n",
        "    has_causal_verb_structure = False\n",
        "    for tok in doc:\n",
        "        if tok.lemma_ in causal_verbs and tok.pos_ == \"VERB\":\n",
        "            subj = any(child.dep_ == \"nsubj\" for child in tok.children)\n",
        "            obj = any(child.dep_ == \"dobj\" for child in tok.children)\n",
        "            prep = any(child.dep_ == \"prep\" for child in tok.children)\n",
        "            if subj or obj or prep:\n",
        "                has_causal_verb_structure = True\n",
        "                explanations.append(\n",
        "                    f\"✓ Verb '{tok.lemma_}' is listed in FrameNet under causal frames with subject/object/prep\"\n",
        "                )\n",
        "                score += 3\n",
        "                if subj: score += 0.5\n",
        "                if obj: score += 0.5\n",
        "                if prep: score += 0.5\n",
        "                break\n",
        "\n",
        "    if any(tok.lemma_ in causal_meta_terms for tok in doc):\n",
        "      explanations.append(\"✓ Causal generalisation or implication term detected (e.g., 'implies', 'generalisation')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.lemma_ in alternative_factor_terms for tok in doc):\n",
        "      explanations.append(\"✓ Terms indicating alternative causes or interfering factors detected\")\n",
        "      score += 1\n",
        "\n",
        "    is_causal = has_condition and has_advcl or has_causal_verb_structure\n",
        "    if not is_causal:\n",
        "        causal_phrases = [\"result in\", \"lead to\", \"may cause\", \"because of\", \"due to\",\"given rise to\",\"resulting from\", \"stemming from\", \"driven by\", \"caused by\", \"attributed to\", \"stems from\", \"reason\", \"result of\", \"consequence of\", \"owning to\", \"thus\", \"so\", \"therefore\", \"hence\"  \"thereby\"]\n",
        "        if any(phrase in sentence.lower() for phrase in causal_phrases):\n",
        "            explanations.append(\"✓ Phrase pattern matches known cause-to-effect trigger\")\n",
        "            score += 2\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong CauseToEffect\" if score >= 7 else \"Weak/Partial CauseToEffect\" if score >= 4 else \"Not CauseToEffect\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "mtlx4oR2QYdU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lexical_units_from_frames(frames):\n",
        "    terms = set()\n",
        "    for frame_name in frames:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:\n",
        "                    terms.add(lu['name'].split('.')[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
        "    return terms\n",
        "\n",
        "\n",
        "expert_frames = [\n",
        "    \"Expertise\", \"Judgment_communication\", \"Opinion\",\n",
        "    \"Authority\", \"Statement\", \"Certainty\"\n",
        "]\n",
        "quote_frames = [\"Statement\", \"Judgment_communication\"]\n",
        "clarity_frames = [\"Reasoning\"]\n",
        "evidence_frames = [\"Evidence\", \"Certainty\", \"Causation\"]\n",
        "\n",
        "\n",
        "expert_verbs = get_lexical_units_from_frames(expert_frames)\n",
        "quote_verbs = get_lexical_units_from_frames(quote_frames)\n",
        "clarity_terms = get_lexical_units_from_frames(clarity_frames)\n",
        "evidence_terms = get_lexical_units_from_frames(evidence_frames)\n",
        "\n",
        "def detect_expert_opinion(question):\n",
        "\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "\n",
        "    expert_titles = {\"expert\", \"researcher\", \"scientist\", \"doctor\", \"analyst\", \"professor\", \"Dr.\"}\n",
        "\n",
        "    implicit_expert_terms = {\"study\", \"research\", \"evidence\", \"report\", \"findings\", \"scientific\", \"government\", \"official\", \"paper\", \"survey\", \"data\"}\n",
        "    comparison_cues = {\"consistent\", \"align\", \"similar\", \"agree\", \"disagree\", \"corroborate\", \"conflict\"}\n",
        "    technical_request_verbs = {\"define\", \"explain\", \"describe\", \"elaborate\", \"clarify\"}\n",
        "    assertion_verbs = {\"assert\", \"affirm\", \"pronounce\", \"declare\", \"maintain\", \"claim\", \"state\"}\n",
        "    reference_terms = {\"quote\", \"reference\", \"cite\", \"check\", \"verify\", \"source\"}\n",
        "    domain_terms = {\"science\", \"scientific\", \"domain\", \"field\", \"discipline\", \"area\", \"sector\"}\n",
        "\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
        "            if any(title in ent.text.lower() for title in expert_titles):\n",
        "                explanations.append(f\"✓ Expert entity detected: '{ent.text}'\")\n",
        "                score += 3\n",
        "                break\n",
        "\n",
        "    if any(tok.lemma_ in expert_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Detected expert-related verb from FrameNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_ in quote_verbs for tok in doc):\n",
        "        explanations.append(\"✓ Quotation or claim verb found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in clarity_terms for tok in doc):\n",
        "        explanations.append(\"✓ Clarity/definition markers found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in evidence_terms for tok in doc):\n",
        "        explanations.append(\"✓ Evidence or support-related terms found\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in implicit_expert_terms for tok in doc):\n",
        "      explanations.append(\"✓ Implicit expert-related term detected (e.g., 'study', 'government')\")\n",
        "      score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in comparison_cues for tok in doc):\n",
        "      explanations.append(\"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\")\n",
        "      score += 0.5\n",
        "\n",
        "    if any(tok.lemma_.lower() in technical_request_verbs for tok in doc):\n",
        "      explanations.append(\"✓ Technical explanation request detected (e.g., 'define', 'explain')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.dep_ == \"attr\" and tok.lemma_ == \"expert\" for tok in doc):\n",
        "      explanations.append(\"✓ Predicate nominative indicating expertise detected (e.g., 'X is an expert')\")\n",
        "      score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in assertion_verbs for tok in doc):\n",
        "      explanations.append(\"✓ Assertion or claim verb detected (e.g., 'assert', 'affirm')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.lemma_.lower() in reference_terms for tok in doc):\n",
        "      explanations.append(\"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\")\n",
        "      score += 1\n",
        "\n",
        "    if any(tok.lemma_.lower() in domain_terms for tok in doc):\n",
        "      explanations.append(\"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\")\n",
        "      score += 1\n",
        "\n",
        "    label = \"Strong Expert Opinion\" if score >= 7 else \"Weak/Partial Expert Opinion\" if score >= 4 else \"Not Expert Opinion\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "3n5B9kQlQkyA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "analogy_synsets = [wn.synset('similar.a.01'), wn.synset('analogy.n.01'), wn.synset('compare.v.01')]\n",
        "\n",
        "comparison_frames = [\"Similarity\"]\n",
        "contrast_frames = [\"Categorization\"]\n",
        "evidence_frames = [\"Evidence\", \"Judgment_communication\"]\n",
        "\n",
        "comparison_verbs = get_lexical_units_from_frames(comparison_frames)\n",
        "contrast_verbs = get_lexical_units_from_frames(contrast_frames)\n",
        "evidence_verbs = get_lexical_units_from_frames(evidence_frames)\n",
        "\n",
        "def is_semantically_analogical(token):\n",
        "    token_synsets = wn.synsets(token.lemma_)\n",
        "    for s in token_synsets:\n",
        "        for analogy_syn in analogy_synsets:\n",
        "            if s.path_similarity(analogy_syn) and s.path_similarity(analogy_syn) > 0.3:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "analogy_context_cues = {\"respect\", \"in which\", \"such that\", \"with regard to\", \"in terms of\"}\n",
        "\n",
        "analogy_force_cues = {\"undermine\", \"weaken\", \"strengthen\", \"force of similarity\", \"degree of analogy\"}\n",
        "\n",
        "analogy_nouns = {\"analogy\", \"comparison\", \"parallel\", \"similarity\", \"analogue\"}\n",
        "\n",
        "def detect_analogy_question(question):\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "    noun_chunks = list(doc.noun_chunks)\n",
        "\n",
        "    if any(tok.lemma_ in comparison_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Comparison verb detected from FrameNet\")\n",
        "        score += 2.5\n",
        "\n",
        "    entity_tokens = [tok for tok in doc if tok.pos_ in {\"PROPN\", \"NOUN\"}]\n",
        "    if len(set(tok.lemma_ for tok in entity_tokens)) >= 2:\n",
        "        explanations.append(\"✓ Contains at least two distinct concepts/entities\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in contrast_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Contrast or difference verb detected from FrameNet\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in evidence_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Evidence or justification verb found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.tag_ == \"MD\" for tok in doc):\n",
        "        score += 0.5\n",
        "\n",
        "    if len(noun_chunks) >= 2 and any(tok.lemma_ in {\"similar\", \"like\", \"as\"} for tok in doc):\n",
        "        explanations.append(\"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\")\n",
        "        score += 3\n",
        "\n",
        "    if any(tok.text.lower() == \"if\" for tok in doc):\n",
        "        explanations.append(\"✓ Conditional structure suggesting hypothetical reasoning\")\n",
        "        score += 1\n",
        "\n",
        "    if any(is_semantically_analogical(tok) for tok in doc if tok.pos_ in {\"ADJ\", \"NOUN\", \"VERB\"}):\n",
        "        explanations.append(\"✓ Semantic similarity to analogy-related terms detected via WordNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.dep_ in {\"prep\", \"relcl\"} and tok.lemma_ in {\"compare\", \"similar\"} for tok in doc):\n",
        "        explanations.append(\"✓ Syntactic cue of analogy (e.g., 'compared with', 'similar to')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(phrase in question.lower() for phrase in analogy_context_cues):\n",
        "      explanations.append(\"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\")\n",
        "      score += 0.5\n",
        "\n",
        "    if any(tok.lemma_ in analogy_force_cues for tok in doc):\n",
        "      explanations.append(\"✓ Analogy evaluation term detected (e.g., 'undermine', 'strengthen')\")\n",
        "      score += 0.5\n",
        "\n",
        "    if any(tok.lemma_ in analogy_nouns for tok in doc if tok.pos_ == \"NOUN\"):\n",
        "      explanations.append(\"✓ Explicit analogy noun detected (e.g., 'analogy', 'comparison')\")\n",
        "      score += 2\n",
        "\n",
        "    if any(tok.dep_ == \"neg\" for tok in doc):\n",
        "      if any(tok.lemma_ in {\"similar\", \"compare\", \"alike\", \"match\"} for tok in doc):\n",
        "          explanations.append(\"✓ Negated comparison detected (suggesting analogy breakdown)\")\n",
        "          score += 1\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong Analogy Question\" if score >= 7 else \"Weak/Partial Analogy Question\" if score >= 4 else \"Not Analogy Question\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "PYEMsIgKQt11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8db6be-b78e-453a-c2bc-8a0432c0c6ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_fear_related(token):\n",
        "    syns = wn.synsets(token.lemma_)\n",
        "    for s in syns:\n",
        "        if any(s.path_similarity(wn.synset('danger.n.01')) or s.path_similarity(wn.synset('problem.n.01')) or\n",
        "               s.path_similarity(wn.synset('fear.n.01')) or s.path_similarity(wn.synset('harm.n.01')) or\n",
        "               s.path_similarity(wn.synset('threat.n.01')) for s in syns):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ---- FrameNet Utility ----\n",
        "def get_lexical_units_from_frames(frames):\n",
        "    terms = set()\n",
        "    for frame_name in frames:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:\n",
        "                    terms.add(lu['name'].split('.')[0])\n",
        "        except:\n",
        "            continue\n",
        "    return terms\n",
        "\n",
        "# ---- Relevant Lexical Resources ----\n",
        "causal_frames = [\"Causation\", \"Cause_to_start\", \"Preventing\", \"Risk\", \"Threaten\", \"Danger\"]\n",
        "causal_verbs = get_lexical_units_from_frames(causal_frames)\n",
        "\n",
        "fear_keywords = {\"danger\", \"threat\", \"risky\", \"harm\", \"catastrophe\", \"crisis\", \"ruin\", \"fear\", \"worse\", \"bad\", \"fatal\", \"negative\", \"die\", \"death\"}\n",
        "preventive_keywords = {\"prevent\", \"avoid\", \"stop\", \"ban\", \"rescue\", \"save\", \"protect\"}\n",
        "\n",
        "urgency_keywords = {\"immediately\", \"soon\", \"before it's too late\", \"critical\", \"urgent\", \"suddenly\", \"unexpectedly\"}\n",
        "\n",
        "possibility_terms = {\"possible\", \"possibility\", \"likely\", \"likelihood\", \"chance\", \"probability\", \"conceivable\", \"potential\", \"can\", \"could\", \"might\", \"may\", \"able\"}\n",
        "\n",
        "\n",
        "def detect_fear_appeal_question(question):\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "\n",
        "    if any(tok.lemma_.lower() in fear_keywords for tok in doc):\n",
        "        explanations.append(\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\")\n",
        "        score += 3\n",
        "\n",
        "    if any(tok.lemma_.lower() in preventive_keywords for tok in doc):\n",
        "        explanations.append(\"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_ in causal_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Causal/preventive verb from FrameNet detected\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.text.lower() in {\"if\", \"unless\"} for tok in doc):\n",
        "        explanations.append(\"✓ Conditional clause found (e.g., 'if', 'unless')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(is_fear_related(tok) for tok in doc if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"}):\n",
        "        explanations.append(\"✓ Semantic fear-related concept detected via WordNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(phrase in question.lower() for phrase in urgency_keywords):\n",
        "        explanations.append(\"✓ Urgency marker detected (e.g., 'immediately', 'before it's too late')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in possibility_terms for tok in doc):\n",
        "        explanations.append(\"✓ Possibility-related term detected (e.g., 'possible', 'feasible', 'chance')\")\n",
        "        score += 1\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong Fear Appeal\" if score >= 7 else \"Weak/Partial Fear Appeal\" if score >= 4 else \"Not Fear Appeal\"\n",
        "    return label, score, explanations"
      ],
      "metadata": {
        "id": "2NcmmmqfBmVb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"How strong is the generalisation that if the world exists then we exist?\",\n",
        "    \"Are there other factors in this particular case that could have interfered with the event of existance of the world?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_cause_to_effect(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "DBBJEXBVJUyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6520a392-1bfd-49b8-a2cb-419282f39f82"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How strong is the generalisation that if the world exists then we exist?\n",
            "Result: ('Weak/Partial CauseToEffect', 6, [\"✓ Conditional clause detected (e.g., 'if', 'when')\", '✓ Adverbial clause (likely effect clause) detected', \"✓ Causal generalisation or implication term detected (e.g., 'implies', 'generalisation')\"])\n",
            "\n",
            "Question: Are there other factors in this particular case that could have interfered with the event of existance of the world?\n",
            "Result: ('Not CauseToEffect', 1, ['✓ Terms indicating alternative causes or interfering factors detected'])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Is Peter a genuine expert in science?\",\n",
        "    \"Did Peter really assert that the world exists?\",\n",
        "    \"Is Peter’s pronouncement directly quoted? If not, is a reference to the original source given? Can it be checked?\",\n",
        "    \"If Peter’s advice is not quoted, does it look like important information or qualifications may have been left out?\",\n",
        "    \"Is what Peter said clear? Are there technical terms used that are not explained clearly?\",\n",
        "    \"Is existance of the world relevant to domain science?\",\n",
        "    \"Is existance of the world consistent with what other experts in <domainD> say?\",\n",
        "    \"Is existance of the world consistent with known evidence in <domainD>?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_expert_opinion(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "uPYflc0RINmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8da333-5fe0-4df7-f253-b8964dfa6999"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Is Peter a genuine expert in science?\n",
            "Result: ('Not Expert Opinion', 3, [\"✓ Predicate nominative indicating expertise detected (e.g., 'X is an expert')\", \"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\"])\n",
            "\n",
            "Question: Did Peter really assert that the world exists?\n",
            "Result: ('Weak/Partial Expert Opinion', 4, ['✓ Detected expert-related verb from FrameNet', '✓ Quotation or claim verb found', \"✓ Assertion or claim verb detected (e.g., 'assert', 'affirm')\"])\n",
            "\n",
            "Question: Is Peter’s pronouncement directly quoted? If not, is a reference to the original source given? Can it be checked?\n",
            "Result: ('Not Expert Opinion', 1, [\"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\"])\n",
            "\n",
            "Question: If Peter’s advice is not quoted, does it look like important information or qualifications may have been left out?\n",
            "Result: ('Not Expert Opinion', 3, ['✓ Evidence or support-related terms found', \"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\"])\n",
            "\n",
            "Question: Is what Peter said clear? Are there technical terms used that are not explained clearly?\n",
            "Result: ('Weak/Partial Expert Opinion', 4, ['✓ Detected expert-related verb from FrameNet', '✓ Quotation or claim verb found', \"✓ Technical explanation request detected (e.g., 'define', 'explain')\"])\n",
            "\n",
            "Question: Is existance of the world relevant to domain science?\n",
            "Result: ('Not Expert Opinion', 1, [\"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\"])\n",
            "\n",
            "Question: Is existance of the world consistent with what other experts in <domainD> say?\n",
            "Result: ('Not Expert Opinion', 3.5, ['✓ Detected expert-related verb from FrameNet', '✓ Quotation or claim verb found', \"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\"])\n",
            "\n",
            "Question: Is existance of the world consistent with known evidence in <domainD>?\n",
            "Result: ('Weak/Partial Expert Opinion', 6.5, ['✓ Detected expert-related verb from FrameNet', '✓ Evidence or support-related terms found', \"✓ Implicit expert-related term detected (e.g., 'study', 'government')\", \"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\"])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Are frogs and horses similar in the respect cited?\",\n",
        "    \"Is the existance of the world true in horses?\",\n",
        "    \"Are there differences between horses and frogs that would tend to undermine the force of the similarity cited?\",\n",
        "    \"Is there some other case that is also similar to horses, but in which frog is false?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_analogy_question(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "hYsWnbjWFwhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6a994d-21bb-4b09-b966-77bd62ae142c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Are frogs and horses similar in the respect cited?\n",
            "Result: ('Strong Analogy Question', 7.5, ['✓ Contains at least two distinct concepts/entities', '✓ Evidence or justification verb found', \"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\", '✓ Semantic similarity to analogy-related terms detected via WordNet', \"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\"])\n",
            "\n",
            "Question: Is the existance of the world true in horses?\n",
            "Result: ('Not Analogy Question', 3, ['✓ Contains at least two distinct concepts/entities', '✓ Semantic similarity to analogy-related terms detected via WordNet'])\n",
            "\n",
            "Question: Are there differences between horses and frogs that would tend to undermine the force of the similarity cited?\n",
            "Result: ('Weak/Partial Analogy Question', 5.0, ['✓ Contains at least two distinct concepts/entities', '✓ Evidence or justification verb found', \"✓ Analogy evaluation term detected (e.g., 'undermine', 'strengthen')\", \"✓ Explicit analogy noun detected (e.g., 'analogy', 'comparison')\"])\n",
            "\n",
            "Question: Is there some other case that is also similar to horses, but in which frog is false?\n",
            "Result: ('Weak/Partial Analogy Question', 6.5, ['✓ Contains at least two distinct concepts/entities', \"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\", '✓ Semantic similarity to analogy-related terms detected via WordNet', \"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\"])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Is the world existing bad? Why and to whom is it bad?\",\n",
        "    \"Is the world existing away to prevent people from dying?\",\n",
        "    \"Is it practically possible for world existing to happen?\",\n",
        "    \"Are there other consequences from the world existing?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = detect_fear_appeal_question(question)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Result: {result}\\n\")"
      ],
      "metadata": {
        "id": "rKfapoMPD90G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69c2d92-935c-497b-ab65-fb15dd4550a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Is the world existing bad? Why and to whom is it bad?\n",
            "Result: ('Weak/Partial Fear Appeal', 5, [\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\", '✓ Semantic fear-related concept detected via WordNet'])\n",
            "\n",
            "Question: Is the world existing away to prevent people from dying?\n",
            "Result: ('Strong Fear Appeal', 7, [\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\", \"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\", '✓ Semantic fear-related concept detected via WordNet'])\n",
            "\n",
            "Question: Is it practically possible for world existing to happen?\n",
            "Result: ('Not Fear Appeal', 3, ['✓ Semantic fear-related concept detected via WordNet', \"✓ Possibility-related term detected (e.g., 'possible', 'feasible', 'chance')\"])\n",
            "\n",
            "Question: Are there other consequences from the world existing?\n",
            "Result: ('Not Fear Appeal', 2, ['✓ Semantic fear-related concept detected via WordNet'])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Cu1EtwNuB6Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_schema(question):\n",
        "    _, cte_score, _ = detect_cause_to_effect(question)\n",
        "\n",
        "    _, expert_score, _ = detect_expert_opinion(question)\n",
        "\n",
        "    _, analogy_score, _ = detect_analogy_question(question)\n",
        "\n",
        "    _, fear_score, _ = detect_fear_appeal_question(question)\n",
        "\n",
        "    return cte_score, expert_score, analogy_score, fear_score\n",
        "\n",
        "for key, obj in results.items():\n",
        "    for index, cq_entry in enumerate(obj[\"cqs\"]):\n",
        "        schema = cq_entry.get(\"schema\")\n",
        "        cte_score, expert_score, analogy_score, fear_score = classify_schema(cq_entry[\"cq\"])\n",
        "\n",
        "        cq_entry[\"CauseToEffect\"] = cte_score\n",
        "        cq_entry[\"ExpertOpinion\"] = expert_score\n",
        "        cq_entry[\"Analogy\"] = analogy_score\n",
        "        cq_entry[\"FearAppeal\"] = fear_score\n",
        "\n",
        "        is_critical = False\n",
        "        if cq_entry[schema] >= 7:\n",
        "          is_critical = True\n",
        "\n",
        "        cq_entry[\"is_critical\"] = is_critical\n"
      ],
      "metadata": {
        "id": "ryDIUlthBsWV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(os.getcwd(), f\"Evaluation/Scored/{result_file}_eval.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(results, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "_TwYIMicHwUF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Showcas\"\n",
        "!git config --global user.email \"cedric.bohni@gmx.de\"\n",
        "\n",
        "\n",
        "commit_message = f\"evaluate CQs for Baseline model {result_file}\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "jZCHLXO2HtH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe6f315-4618-4aa0-eed2-2db76fba6cc3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main c9b6de3] evaluate CQs for Baseline model results_schema_Baseline_Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
            " 1 file changed, 2976 insertions(+), 744 deletions(-)\n",
            "Enumerating objects: 9, done.\n",
            "Counting objects: 100% (9/9), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (5/5), 10.36 KiB | 1.48 MiB/s, done.\n",
            "Total 5 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/RicoStaedeli/NLP2025_CQG.git\n",
            "   f6a2e12..c9b6de3  main -> main\n"
          ]
        }
      ]
    }
  ]
}