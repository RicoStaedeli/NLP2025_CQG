{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "a2ee1b19d3321307"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Baseline Predictions\n",
    "In this file we generate the baseline predictions"
   ],
   "id": "d44ba819438d4c7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T08:56:19.465528Z",
     "start_time": "2025-04-27T08:56:17.975999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import tqdm\n",
    "import re\n",
    "import torch"
   ],
   "id": "56e5bbe87af65278",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T08:56:25.495739Z",
     "start_time": "2025-04-27T08:56:25.492875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "#######################   PATH VARIABLES        ################################\n",
    "################################################################################\n",
    "\n",
    "sample_path = \"Data/Raw/sharedTask/sample.json\"\n",
    "model_path_llama = \"Models/Meta-Llama-3-8B-Instruct\"\n",
    "model_path_qwen = \"Models/Meta-Qwen-3-8B-Instruct\"\n",
    "\n",
    "################################################################################\n",
    "#######################   STATIC VARIABLES      ################################\n",
    "################################################################################\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "id": "c97a513058176f9f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Llama 3 8B Instruct",
   "id": "4952cbcef0d28c5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T08:56:27.960590Z",
     "start_time": "2025-04-27T08:56:27.955966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def structure_output(whole_text):\n",
    "    cqs_list = whole_text.split('\\n')\n",
    "    final = []\n",
    "    valid = []\n",
    "    not_valid = []\n",
    "    for cq in cqs_list:\n",
    "        if re.match('.*\\?(\\\")?( )?(\\([a-zA-Z0-9\\.\\'\\-,\\? ]*\\))?([a-zA-Z \\.,\\\"\\']*)?(\\\")?$', cq):\n",
    "            valid.append(cq)\n",
    "        else:\n",
    "            not_valid.append(cq)\n",
    "\n",
    "    still_not_valid = []\n",
    "    for text in not_valid:\n",
    "        new_cqs = re.split(\"\\?\\\"\", text+'end')\n",
    "        if len(new_cqs) > 1:\n",
    "            for cq in new_cqs[:-1]:\n",
    "                valid.append(cq+'?\\\"')\n",
    "        else:\n",
    "            still_not_valid.append(text)\n",
    "\n",
    "    for i, cq in enumerate(valid):\n",
    "        occurrence = re.search(r'[A-Z]', cq)\n",
    "        if occurrence:\n",
    "            final.append(cq[occurrence.start():])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    output = []\n",
    "    if len(final) >= 3:\n",
    "        for i in [0, 1, 2]:\n",
    "            output.append({'id':i, 'cq':final[i]})\n",
    "        return output\n",
    "    else:\n",
    "        logger.warning('Missing CQs')\n",
    "        return 'Missing CQs'\n",
    "\n",
    "def generate_answer(pipe, terminator, input_text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant to rephrase given text. But you always keep in mind that the meaning must not change.\"},\n",
    "        {\"role\": \"user\", \"content\": input_text},\n",
    "    ]\n",
    "\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminator,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ],
   "id": "a104cb3afede0d30",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\?'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\?'\n",
      "/var/folders/d3/4qjhzn1n1zb9shbj0sqx9g4h0000gn/T/ipykernel_29527/3004912799.py:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  if re.match('.*\\?(\\\")?( )?(\\([a-zA-Z0-9\\.\\'\\-,\\? ]*\\))?([a-zA-Z \\.,\\\"\\']*)?(\\\")?$', cq):\n",
      "/var/folders/d3/4qjhzn1n1zb9shbj0sqx9g4h0000gn/T/ipykernel_29527/3004912799.py:14: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  new_cqs = re.split(\"\\?\\\"\", text+'end')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:01:28.010500Z",
     "start_time": "2025-04-27T09:01:21.660366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefixes = [\"\"\"Suggest 3 critical questions that should be raised before accepting the arguments in this text:\n",
    "\n",
    "                \"{intervention}\"\n",
    "\n",
    "                Give one question per line. Make the questions simple, and do not give any explanation regarding why the question is relevant.\"\"\"]\n",
    "\n",
    "model_path = model_path_llama\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_path,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ],
   "id": "8809244a7524ac5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6f167b2203f4489a89e7c785b1bf984"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T09:02:39.485830Z",
     "start_time": "2025-04-27T09:02:16.332237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(sample_path) as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "for key,line in tqdm.tqdm(data.items()):\n",
    "    for prefix in prefixes:\n",
    "        intervention = line['intervention']\n",
    "        instruction = prefix.format(**{'intervention':intervention})\n",
    "        cqs = generate_answer(pipeline, terminators, instruction)\n",
    "        print(\"Input text: \", instruction)\n",
    "        print(\"Critical Questions: \", cqs)\n",
    "        print(\"-----------\")\n"
   ],
   "id": "de42c76424ef6568",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      " 20%|██        | 1/5 [00:08<00:32,  8.19s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  Suggest 3 critical questions that should be raised before accepting the arguments in this text:\n",
      "\n",
      "                \"CLINTON: \"The central question in this election is really what kind of country we want to be and what kind of future we 'll build together\n",
      "Today is my granddaughter 's second birthday\n",
      "I think about this a lot\n",
      "we have to build an economy that works for everyone , not just those at the top\n",
      "we need new jobs , good jobs , with rising incomes\n",
      "I want us to invest in you\n",
      "I want us to invest in your future\n",
      "jobs in infrastructure , in advanced manufacturing , innovation and technology , clean , renewable energy , and small business\n",
      "most of the new jobs will come from small business\n",
      "We also have to make the economy fairer\n",
      "That starts with raising the national minimum wage and also guarantee , finally , equal pay for women 's work\n",
      "I also want to see more companies do profit-sharing\"\"\n",
      "\n",
      "                Give one question per line. Make the questions simple, and do not give any explanation regarding why the question is relevant.\n",
      "Critical Questions:  Here are three critical questions that can be raised before accepting the arguments in the text:\n",
      "\n",
      "1. What specific measures will be taken to ensure the economy works for everyone, not just those at the top?\n",
      "2. How will the proposed investments in jobs, infrastructure, and innovation be funded and financed?\n",
      "3. What concrete steps will be taken to guarantee equal pay for women's work, and how will it be enforced?\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.04s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:  Suggest 3 critical questions that should be raised before accepting the arguments in this text:\n",
      "\n",
      "                \"Javier: \"I have no problem requiring the airlines to give notification of any \"known\" delays within 30 minutes of when they become aware of it.\n",
      "BUT, being a frequent traveler, I see all sorts of problems which are unavoidable and for which the airlines will be blamed by giving such notice with the intend that some flyers may be able to delay thier trip to the airport or even the departure gate.\n",
      "I know that many times the airlines can't also know exactly when a weather hold or a maintenance issue will be rectified and that the flight is then ready to go.\n",
      "Many time it can be surprisingly faster than expected.\n",
      "The problem is that some flyers may then  wait before going to the airport,\n",
      "but then find that the problem was rectified sooner than expected and the flight departed.\n",
      "Of course the flyer and the flyers rights organization will then crucify the airlines for such poor planning.\n",
      "Flyers need to grow up and quit pouting and blaming everthing on the airlines.\n",
      "They can't have it both ways.\"\"\n",
      "\n",
      "                Give one question per line. Make the questions simple, and do not give any explanation regarding why the question is relevant.\n",
      "Critical Questions:  Here are three critical questions that can be raised before accepting the arguments in the text:\n",
      "\n",
      "1. What is the definition of \"known\" delays in this context?\n",
      "2. How would the proposed 30-minute notification rule account for unexpected changes in flight status?\n",
      "3. Is it fair to assume that flyers should be expected to constantly monitor flight status and adjust their plans accordingly?\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:22<00:34, 11.47s/it]\n",
      "/var/folders/d3/4qjhzn1n1zb9shbj0sqx9g4h0000gn/T/ipykernel_29527/3004912799.py:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  if re.match('.*\\?(\\\")?( )?(\\([a-zA-Z0-9\\.\\'\\-,\\? ]*\\))?([a-zA-Z \\.,\\\"\\']*)?(\\\")?$', cq):\n",
      "/var/folders/d3/4qjhzn1n1zb9shbj0sqx9g4h0000gn/T/ipykernel_29527/3004912799.py:14: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  new_cqs = re.split(\"\\?\\\"\", text+'end')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m intervention = line[\u001B[33m'\u001B[39m\u001B[33mintervention\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      7\u001B[39m instruction = prefix.format(**{\u001B[33m'\u001B[39m\u001B[33mintervention\u001B[39m\u001B[33m'\u001B[39m:intervention})\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m cqs = \u001B[43mgenerate_answer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mterminators\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minstruction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mInput text: \u001B[39m\u001B[33m\"\u001B[39m, instruction)\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mCritical Questions: \u001B[39m\u001B[33m\"\u001B[39m, cqs)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mgenerate_answer\u001B[39m\u001B[34m(pipe, terminator, input_text)\u001B[39m\n\u001B[32m     38\u001B[39m messages = [\n\u001B[32m     39\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mYou are a helpful assistant to rephrase given text. But you always keep in mind that the meaning must not change.\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m     40\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: input_text},\n\u001B[32m     41\u001B[39m ]\n\u001B[32m     43\u001B[39m prompt = pipe.tokenizer.apply_chat_template(\n\u001B[32m     44\u001B[39m         messages,\n\u001B[32m     45\u001B[39m         tokenize=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     46\u001B[39m         add_generation_prompt=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     47\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m outputs = \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     50\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.6\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.9\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mgenerated_text\u001B[39m\u001B[33m\"\u001B[39m][\u001B[38;5;28mlen\u001B[39m(prompt):]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:287\u001B[39m, in \u001B[36mTextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, text_inputs, **kwargs)\u001B[39m\n\u001B[32m    285\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    286\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1379\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1371\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1372\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1373\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1376\u001B[39m         )\n\u001B[32m   1377\u001B[39m     )\n\u001B[32m   1378\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1379\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1386\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1385\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1386\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1387\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1388\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1286\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1284\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1285\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1286\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1287\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1288\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:385\u001B[39m, in \u001B[36mTextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    382\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    383\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m385\u001B[39m output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, ModelOutput):\n\u001B[32m    388\u001B[39m     generated_sequence = output.sequences\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001B[39m\n\u001B[32m   2457\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2458\u001B[39m         input_ids=input_ids,\n\u001B[32m   2459\u001B[39m         expand_size=generation_config.num_return_sequences,\n\u001B[32m   2460\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2461\u001B[39m         **model_kwargs,\n\u001B[32m   2462\u001B[39m     )\n\u001B[32m   2464\u001B[39m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2465\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2466\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2467\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2468\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2469\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2470\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2471\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2472\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2473\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2475\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2476\u001B[39m     \u001B[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001B[39;00m\n\u001B[32m   2477\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2478\u001B[39m         input_ids=input_ids,\n\u001B[32m   2479\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2480\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2481\u001B[39m         **model_kwargs,\n\u001B[32m   2482\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HSG/NLP/Project/CQ/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3476\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   3474\u001B[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001B[32m1\u001B[39m)\n\u001B[32m   3475\u001B[39m     \u001B[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3476\u001B[39m     next_tokens = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmultinomial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m.squeeze(\u001B[32m1\u001B[39m)\n\u001B[32m   3477\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3478\u001B[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001B[32m1\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "new_entries = []\n",
    "\n",
    "for _, row in df_filtered.iterrows():\n",
    "    input_text = system_prompt + row['modified_premises']\n",
    "    rephrased_premise = generate_answer(pipeline, terminators, input_text)\n",
    "\n",
    "    new_entry = {\n",
    "        \"modified_premises\": rephrased_premise,\n",
    "        \"read_premises\" : row['read_premises'],\n",
    "        \"scheme\": row['scheme'],\n",
    "        \"modified_cqs\": row['modified_cqs']\n",
    "    }\n",
    "\n",
    "    new_entries.append(new_entry)\n",
    "\n",
    "# Combine old and new entries\n",
    "full_data = df_filtered.to_dict(orient='records') + new_entries\n",
    "\n",
    "# Save to new file\n",
    "with open(augmented_data_path, \"w\") as f:\n",
    "    json.dump(full_data, f, indent=2)\n",
    "\n",
    "print(f\"Extended dataset saved to {augmented_data_path}\")'''"
   ],
   "id": "4e5a3d8ef41595ed"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
