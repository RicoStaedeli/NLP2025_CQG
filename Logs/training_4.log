2025-05-18 19:19:24,391 - INFO - --------  Start with Training  -------------
2025-05-18 19:19:24,392 - INFO - Device selected: cuda
2025-05-18 19:19:24,393 - INFO - Model: Meta-Llama-3.1-8B-Instruct_SFT_3
2025-05-18 19:19:24,394 - INFO - Training number: 4
2025-05-18 19:19:30,432 - INFO - ------ Unlsoth Parameters ---------------
2025-05-18 19:19:30,433 - INFO - max_seq_length: 2048
2025-05-18 19:19:30,433 - INFO - dtype: None
2025-05-18 19:19:30,434 - INFO - load_in_4bit: True
2025-05-18 19:19:30,435 - INFO - ------ PEFT Parameters ------------------
2025-05-18 19:19:30,436 - INFO - r: 16
2025-05-18 19:19:30,436 - INFO - target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-05-18 19:19:30,437 - INFO - lora_alpha: 16
2025-05-18 19:19:30,438 - INFO - lora_dropout: 0
2025-05-18 19:19:30,438 - INFO - bias: none
2025-05-18 19:19:30,439 - INFO - use_gradient_checkpointing: unsloth
2025-05-18 19:19:30,439 - INFO - random_state: 3407
2025-05-18 19:19:30,440 - INFO - use_rslora: False
2025-05-18 19:19:30,440 - INFO - ------  SFT Trainer Parameters ----------
2025-05-18 19:19:30,441 - INFO - dataset_text_field: text
2025-05-18 19:19:30,442 - INFO - dataset_num_proc: 2
2025-05-18 19:19:30,442 - INFO - packing: False
2025-05-18 19:19:30,443 - INFO - per_device_train_batch_size: 38
2025-05-18 19:19:30,443 - INFO - gradient_accumulation_steps: 4
2025-05-18 19:19:30,444 - INFO - warmup_steps: 5
2025-05-18 19:19:30,445 - INFO - max_steps: 40
2025-05-18 19:19:30,445 - INFO - learning_rate: 0.0002
2025-05-18 19:19:30,446 - INFO - fp16: False
2025-05-18 19:19:30,447 - INFO - bf16: True
2025-05-18 19:19:30,448 - INFO - logging_steps: 1
2025-05-18 19:19:30,448 - INFO - save_strategy: IntervalStrategy.STEPS
2025-05-18 19:19:30,449 - INFO - save_steps: 10
2025-05-18 19:19:30,449 - INFO - save_total_limit: 1
2025-05-18 19:19:30,450 - INFO - optim: adamw_8bit
2025-05-18 19:19:30,451 - INFO - weight_decay: 0.01
2025-05-18 19:19:30,451 - INFO - lr_scheduler_type: linear
2025-05-18 19:19:30,452 - INFO - seed: 3407
2025-05-18 19:19:30,453 - INFO - output_dir: /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_4/Checkpoints/
2025-05-18 19:19:30,453 - INFO - report_to: tensorboard
2025-05-18 19:19:30,454 - INFO - logging_dir: /content/NLP2025_CQG/Training/Logs/Traing_4/Tensorboard/
2025-05-18 19:21:53,127 - INFO - GPU  Information before Training
2025-05-18 19:21:53,128 - INFO - GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.
2025-05-18 19:21:53,129 - INFO - 7.623 GB of memory reserved.
2025-05-18 19:30:44,922 - INFO - 527.7724 seconds used for training.
2025-05-18 19:30:44,923 - INFO - 8.8 minutes used for training.
2025-05-18 19:30:44,924 - INFO - Peak reserved memory = 21.072 GB.
2025-05-18 19:30:44,924 - INFO - Peak reserved memory for training = 13.449 GB.
2025-05-18 19:30:44,925 - INFO - Peak reserved memory % of max memory = 53.27 %.
2025-05-18 19:30:44,926 - INFO - Peak reserved memory for training % of max memory = 33.999 %.
2025-05-18 19:30:46,616 - INFO - Saved LoRA adapters to /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_4/Model/Meta-Llama-3.1-8B-Instruct_SFT_3_lora_adapters/
