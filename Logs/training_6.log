2025-05-19 13:29:46,304 - INFO - --------  Start with Training  -------------
2025-05-19 13:29:46,305 - INFO - Device selected: cuda
2025-05-19 13:29:46,306 - INFO - Model: Meta-Llama-3.1-8B-Instruct_SFT_2_DPO
2025-05-19 13:29:46,307 - INFO - Training number: 6
2025-05-19 13:29:46,317 - INFO - ------ Unlsoth Parameters ---------------
2025-05-19 13:29:46,318 - INFO - max_seq_length: 2048
2025-05-19 13:29:46,318 - INFO - dtype: None
2025-05-19 13:29:46,319 - INFO - load_in_4bit: True
2025-05-19 13:29:46,320 - INFO - ------ PEFT Parameters ------------------
2025-05-19 13:29:46,320 - INFO - r: 16
2025-05-19 13:29:46,321 - INFO - target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-05-19 13:29:46,321 - INFO - lora_alpha: 16
2025-05-19 13:29:46,322 - INFO - lora_dropout: 0
2025-05-19 13:29:46,323 - INFO - bias: none
2025-05-19 13:29:46,323 - INFO - use_gradient_checkpointing: unsloth
2025-05-19 13:29:46,324 - INFO - random_state: 3407
2025-05-19 13:29:46,325 - INFO - use_rslora: False
2025-05-19 13:29:46,325 - INFO - ------  SFT Trainer Parameters ----------
2025-05-19 13:29:46,326 - INFO - dataset_text_field: input_ids
2025-05-19 13:29:46,327 - INFO - dataset_num_proc: 2
2025-05-19 13:29:46,328 - INFO - packing: False
2025-05-19 13:29:46,328 - INFO - per_device_train_batch_size: 2
2025-05-19 13:29:46,329 - INFO - gradient_accumulation_steps: 4
2025-05-19 13:29:46,330 - INFO - warmup_steps: 5
2025-05-19 13:29:46,330 - INFO - max_steps: 10
2025-05-19 13:29:46,331 - INFO - learning_rate: 0.0002
2025-05-19 13:29:46,331 - INFO - fp16: False
2025-05-19 13:29:46,332 - INFO - bf16: True
2025-05-19 13:29:46,333 - INFO - logging_steps: 1
2025-05-19 13:29:46,333 - INFO - save_strategy: IntervalStrategy.STEPS
2025-05-19 13:29:46,334 - INFO - save_steps: 1
2025-05-19 13:29:46,334 - INFO - save_total_limit: 1
2025-05-19 13:29:46,335 - INFO - optim: adamw_8bit
2025-05-19 13:29:46,336 - INFO - weight_decay: 0.01
2025-05-19 13:29:46,336 - INFO - lr_scheduler_type: linear
2025-05-19 13:29:46,337 - INFO - seed: 3407
2025-05-19 13:29:46,337 - INFO - output_dir: /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_6/Checkpoints/
2025-05-19 13:29:46,338 - INFO - report_to: tensorboard
2025-05-19 13:29:46,338 - INFO - logging_dir: /content/NLP2025_CQG/Training/Logs/Traing_6/Tensorboard/
2025-05-19 13:29:46,339 - INFO - evaluation_strategy: steps
2025-05-19 13:29:46,340 - INFO - eval_steps: 1
2025-05-19 13:31:33,192 - INFO - GPU  Information before Training
2025-05-19 13:31:33,193 - INFO - GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.
2025-05-19 13:31:33,194 - INFO - 7.625 GB of memory reserved.
2025-05-19 14:07:00,344 - INFO - 2124.9501 seconds used for training.
2025-05-19 14:07:00,345 - INFO - 35.42 minutes used for training.
2025-05-19 14:07:00,346 - INFO - Peak reserved memory = 38.322 GB.
2025-05-19 14:07:00,347 - INFO - Peak reserved memory for training = 30.697 GB.
2025-05-19 14:07:00,348 - INFO - Peak reserved memory % of max memory = 96.878 %.
2025-05-19 14:07:00,348 - INFO - Peak reserved memory for training % of max memory = 77.602 %.
2025-05-19 14:07:01,202 - INFO - Saved LoRA adapters to /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_6/Model/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_lora_adapters/
