{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/4_Finetuned_Generation_schema.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d44ba819438d4c7f"
      },
      "cell_type": "markdown",
      "source": [
        "# Finetuned Predictions\n",
        "In this file we generate the finetuned predictions"
      ],
      "id": "d44ba819438d4c7f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "6yd9jnEIPiXa"
      },
      "id": "6yd9jnEIPiXa"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth_zoo\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install accelerate bitsandbytes\n",
        "!pip install -U peft"
      ],
      "metadata": {
        "id": "HbEouL7eL0AB",
        "outputId": "16915f00-cd51-4b04-846b-2eb8f239eacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HbEouL7eL0AB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth_zoo\n",
            "  Downloading unsloth_zoo-2025.5.7-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (2.6.0+cu124)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (24.2)\n",
            "Collecting tyro (from unsloth_zoo)\n",
            "  Downloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,==4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (4.51.3)\n",
            "Collecting datasets>=3.4.1 (from unsloth_zoo)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (1.6.0)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth_zoo)\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.15.2)\n",
            "Collecting protobuf<4.0.0 (from unsloth_zoo)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: huggingface_hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.31.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (0.1.9)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (11.2.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo) (2024.11.6)\n",
            "Collecting msgspec (from unsloth_zoo)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth_zoo) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth_zoo) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth_zoo) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth_zoo) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth_zoo) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth_zoo) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth_zoo) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth_zoo) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth_zoo) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth_zoo) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.30.0->unsloth_zoo) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unsloth_zoo)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->unsloth_zoo) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth_zoo) (13.9.4)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth_zoo) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth_zoo)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth_zoo) (4.4.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth_zoo) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth_zoo) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth_zoo) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth_zoo) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth_zoo) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth_zoo) (2.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unsloth_zoo) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth_zoo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth_zoo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth_zoo) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth_zoo) (1.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth_zoo) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth_zoo) (1.17.0)\n",
            "Downloading unsloth_zoo-2025.5.7-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/363.4 MB\u001b[0m \u001b[31m198.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.252191Z",
          "start_time": "2025-04-28T13:09:02.197619Z"
        },
        "id": "56e5bbe87af65278"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "import logging\n",
        "import transformers\n",
        "import os\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from bitsandbytes import nn as bnb_nn\n",
        "from unsloth import FastLanguageModel\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "id": "56e5bbe87af65278",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "7tOU4FCdPmop"
      },
      "id": "7tOU4FCdPmop",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.322602Z",
          "start_time": "2025-04-28T13:09:03.308445Z"
        },
        "id": "c97a513058176f9f"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "model_name = \"Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_schema_prompt\"\n",
        "model_id = \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO\" #\"meta-llama/Llama-3.1-8B-Instruct\" # ricostaedeli/Meta-Llama-3.1-8B-Instruct_DPO\n",
        "\n",
        "test_dataset_path = f\"/content/NLP2025_CQG/Data/Processed/test.csv\"\n",
        "\n",
        "results_path = os.path.join(os.getcwd(), f\"/content/NLP2025_CQG/Evaluation/Results/results_{model_name}.json\")\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Logs/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_path = log_base_path + f\"4_cqs_generation_{model_name}.log\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n"
      ],
      "id": "c97a513058176f9f",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "# Log the device info\n",
        "logger.info(\"--------  Start with Baseline Generation  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Results Path: {results_path}')\n",
        "logger.info(f'Log Path: {log_path}')\n",
        "logger.info(\"--------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "zbgFJsxgKi40"
      },
      "id": "zbgFJsxgKi40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Answers"
      ],
      "metadata": {
        "id": "z-iPCguiZEoP"
      },
      "id": "z-iPCguiZEoP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess dataset"
      ],
      "metadata": {
        "id": "I5JlZ5K__oHb"
      },
      "id": "I5JlZ5K__oHb"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Define the schema types\n",
        "schemas = {\n",
        "    \"CauseToEffect\": \"\"\"Examples are:\n",
        "    What if the job is one that will cause their illness to manifest and affect their job performance?\n",
        "    If so, do you think the lack of any of the pieces influence the decisions they make about whether to create the digital widget or which digital widget to create?\n",
        "    If miners and the working class are such a small percentage of the population as to not make a difference in a straight up popular vote, then why do we let them have so much influence?\n",
        "    \"\"\",\n",
        "\n",
        "    \"ExpertOpinion\": \"\"\"'Examples are:\n",
        "    Do you have a scientific source that confirm that women only wants one partner to a greater degree than men?\n",
        "    If you are just stating facts, the surely you can point to some studies that show most women are narcissistic sociopaths?\n",
        "    Where in any scientific textbook or journal have you seen evidence of sex being described as a spectrum?\n",
        "    \"\"\",\n",
        "\n",
        "    \"Analogy\": \"\"\"Examples are:\n",
        "    If so, would these groups resemble what are commonly referred to as races?\n",
        "    If whites are guilty of enjoying the benefits of stolen land after being here for two generations, why would that not mean a 2nd generation American POC not have a similar benefit?\n",
        "    But why is the way her hair naturally grows from her head seen as less professional than chemically altering to mimic straight white European hair?\n",
        "    \"\"\",\n",
        "\n",
        "    \"FearAppeal\": \"\"\"'Examples are:\n",
        "    If he did such a great job making peace, why do we need sanctions to pressure North Korea into stopping its violent threats?\n",
        "    And if we should strive to stop all killings then why would the implement of this murder be spared if it causes a very large number of deaths but was intended to cause none?\n",
        "    If you say you can, then are you implying that you see Islam as being no worse?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Prepare the JSON structure\n",
        "json_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    context = row['input'].strip()\n",
        "    original_id = row['id']\n",
        "\n",
        "    for schema_key, schema_value in schemas.items():\n",
        "        entry = {\n",
        "            \"prompt\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.\"\n",
        "                },\n",
        "                 {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Generate one critical question addressing the provided context. Ensure it matches the schema:{schema_key}.\\n\\nSchema definition:\\n{schema_value}\\n\\nContext: {context} Respond only with the question and nothing else.\"\n",
        "                },\n",
        "                 {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": \"Question: \"\n",
        "                }\n",
        "\n",
        "            ],\n",
        "            \"id\": f\"{original_id}\",\n",
        "            \"schema\" : schema_key,\n",
        "            \"input\" : context\n",
        "\n",
        "        }\n",
        "        json_data.append(entry)\n",
        "\n",
        "\n",
        "# Save the JSON data to a file\n",
        "temp_output_file = 'processed_dataset.json'\n",
        "with open(temp_output_file, 'w') as f:\n",
        "    json.dump(json_data, f, indent=4)"
      ],
      "metadata": {
        "id": "RYW1D3-5_kq5"
      },
      "id": "RYW1D3-5_kq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation with transformers"
      ],
      "metadata": {
        "id": "iusfZQx8eQEc"
      },
      "id": "iusfZQx8eQEc"
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8kQxcTvzijM1"
      },
      "id": "8kQxcTvzijM1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pipeline with quantized model for less GPU usage\n"
      ],
      "metadata": {
        "id": "ui1nT_se4xaS"
      },
      "id": "ui1nT_se4xaS"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16},\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "NC3wxLXMjpmP"
      },
      "id": "NC3wxLXMjpmP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\n",
        "            \"load_in_4bit\": True,\n",
        "            \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "        },\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "qOpGGiwQlDj4"
      },
      "id": "qOpGGiwQlDj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single generation"
      ],
      "metadata": {
        "id": "nbHgSdKr4j2Q"
      },
      "id": "nbHgSdKr4j2Q"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_response(query, max_tokens=512, temperature=0.6, top_p=0.9):\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        query, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    response = outputs[0][\"generated_text\"][len(prompt):]\n",
        "    return response\n",
        "    '''"
      ],
      "metadata": {
        "id": "3UeKeadQj2_F"
      },
      "id": "3UeKeadQj2_F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with open(temp_output_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "combined_output = {}\n",
        "i = 0\n",
        "for item in data:\n",
        "    print(f\"Number: {i}\")\n",
        "    item_id = item[\"id\"]\n",
        "    schema = item[\"schema\"]\n",
        "    message = item[\"prompt\"]\n",
        "    input = item[\"input\"]\n",
        "    question = get_response(message).strip()\n",
        "\n",
        "    if item_id not in combined_output:\n",
        "        combined_output[item_id] = {\"input\": input, \"cqs\": []}\n",
        "\n",
        "    combined_output[item_id][\"cqs\"].append({\"schema\": schema, \"cq\": question})\n",
        "    i = i + 1\n",
        "    if i == 2:\n",
        "      break\n",
        "\n",
        "\n",
        "# Save the combined questions\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(combined_output, f, indent=4)\n",
        "print(f\"Combined questions saved to {results_path}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "tFszEi2XkHMp"
      },
      "id": "tFszEi2XkHMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch processing"
      ],
      "metadata": {
        "id": "CdE1gOeR1493"
      },
      "id": "CdE1gOeR1493"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_responses(queries, max_tokens=512, temperature=0.6, top_p=0.9):\n",
        "    # Prepare the prompts\n",
        "    prompts = [pipeline.tokenizer.apply_chat_template(q, tokenize=False, add_generation_prompt=True) for q in queries]\n",
        "\n",
        "    # Run generation in batches to avoid OOM\n",
        "    all_responses = []\n",
        "    batch_size = 250\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "        # Run the pipeline in batch mode\n",
        "        outputs = pipeline(\n",
        "            batch_prompts,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "\n",
        "        # Extract responses\n",
        "        for prompt, output_list in zip(batch_prompts, outputs):\n",
        "            # Each output_list is a list with a single dictionary\n",
        "            generated_text = output_list[0][\"generated_text\"]\n",
        "            response = generated_text[len(prompt):].strip()\n",
        "            all_responses.append(response)\n",
        "            print(response)\n",
        "\n",
        "    return all_responses\n",
        "\n",
        "# Load the data\n",
        "with open(temp_output_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "queries = [item[\"prompt\"] for item in data]\n",
        "responses = get_responses(queries)\n",
        "\n",
        "# Combine the outputs\n",
        "combined_output = {}\n",
        "for item, response in zip(data, responses):\n",
        "    item_id = item[\"id\"]\n",
        "    schema = item[\"schema\"]\n",
        "    input = item[\"input\"]\n",
        "\n",
        "    if item_id not in combined_output:\n",
        "        combined_output[item_id] = {\"input\": input, \"cqs\": []}\n",
        "\n",
        "    combined_output[item_id][\"cqs\"].append({\"schema\": schema, \"cq\": response})\n",
        "\n",
        "# Save the combined questions\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(combined_output, f, indent=4)\n",
        "\n",
        "print(f\"Combined questions saved to {results_path}\")"
      ],
      "metadata": {
        "id": "P7QFLvBir-sG"
      },
      "id": "P7QFLvBir-sG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.remove(temp_output_file)\n",
        "    print(f\"File '{temp_output_file}' deleted successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{temp_output_file}' not found.\")\n",
        "except PermissionError:\n",
        "    print(f\"Permission denied to delete '{temp_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting file: {e}\")"
      ],
      "metadata": {
        "id": "64jpcwQ2P0QD"
      },
      "id": "64jpcwQ2P0QD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commit & Push"
      ],
      "metadata": {
        "id": "Pqf89mWeUFAl"
      },
      "id": "Pqf89mWeUFAl"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "M60niewMW7dP"
      },
      "id": "M60niewMW7dP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabrriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Finetuned generation\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "yDYJfL1_UEg-"
      },
      "id": "yDYJfL1_UEg-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "name": "4_Finetuned_Generation_schema.ipynb",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}