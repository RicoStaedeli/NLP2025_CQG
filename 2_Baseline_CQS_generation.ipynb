{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/2_Baseline_CQS_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d44ba819438d4c7f"
      },
      "cell_type": "markdown",
      "source": [
        "# Baseline Predictions\n",
        "In this file we generate the baseline predictions"
      ],
      "id": "d44ba819438d4c7f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "6yd9jnEIPiXa"
      },
      "id": "6yd9jnEIPiXa"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.252191Z",
          "start_time": "2025-04-28T13:09:02.197619Z"
        },
        "id": "56e5bbe87af65278"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "import logging\n",
        "import tqdm\n",
        "import re\n",
        "import torch\n",
        "from getpass import getpass\n",
        "from google.colab import userdata, drive\n",
        "import os"
      ],
      "id": "56e5bbe87af65278",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o7FZqlh6Wewb"
      },
      "id": "o7FZqlh6Wewb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "7tOU4FCdPmop"
      },
      "id": "7tOU4FCdPmop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "9jACHoyYTqGU"
      },
      "id": "9jACHoyYTqGU",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.322602Z",
          "start_time": "2025-04-28T13:09:03.308445Z"
        },
        "id": "c97a513058176f9f"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "test_dataset_path = \"Data/Processed/test.json\"\n",
        "model_path_llama = \"/content/drive/MyDrive/HSG/NLP/Project NLP/Models/Meta-Llama-3.1-8B-Instruct\"\n",
        "model_path_qwen = \"/content/drive/MyDrive/HSG/NLP/Project NLP/Models/Qwen2.5-7B-Instruct\"\n",
        "results_path = \"Evaluation/Results/\"\n",
        "log_path = \"Logs/2_baseline_predictions.log\"\n",
        "\n",
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "# Log the device info\n",
        "logger.info(\"--------  Start with Baseline Predictions  -------------\")\n",
        "logger.info(f'Device selected: {device}')"
      ],
      "id": "c97a513058176f9f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4952cbcef0d28c5e"
      },
      "cell_type": "markdown",
      "source": [
        "## Zero Shot prompting\n",
        "In this section we genererate critical questions with different pretrained vanilla models. We use this generated questions as a baseline to compare it against our results. The following models were used to generate the baseline results:\n",
        "- LLama 3.1 8B Instruct\n",
        "- Qwen 2.5 7B Instruct"
      ],
      "id": "4952cbcef0d28c5e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.329368Z",
          "start_time": "2025-04-28T13:09:03.327931Z"
        },
        "id": "bf7c6195fc049988"
      },
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    {\n",
        "        \"name\": \"llama\",\n",
        "        \"model_id\": model_path_llama,\n",
        "        \"output_file\": results_path + \"results_zeroshot_llama_3.1-8B-instruct.json\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"qwen\",\n",
        "        \"model_id\": model_path_qwen,\n",
        "        \"output_file\": results_path + \"results_zeroshot_qwen2.5-7b-instruction.json\",\n",
        "    },\n",
        "]"
      ],
      "id": "bf7c6195fc049988",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate critical Questions"
      ],
      "metadata": {
        "id": "-4tnhEQlzURL"
      },
      "id": "-4tnhEQlzURL"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8  # You can adjust this based on your GPU memory\n",
        "\n",
        "def structure_output(whole_text):\n",
        "    cqs_list = whole_text.split('\\n')\n",
        "    final = []\n",
        "    valid = []\n",
        "    not_valid = []\n",
        "    for cq in cqs_list:\n",
        "        if re.match(r'.*\\?(\\\")?( )?(\\([a-zA-Z0-9\\.\\'-\\,\\? ]*\\))?([a-zA-Z \\.,\\\"\\']*)?(\\\")?$', cq):\n",
        "            valid.append(cq)\n",
        "        else:\n",
        "            not_valid.append(cq)\n",
        "\n",
        "    still_not_valid = []\n",
        "    for text in not_valid:\n",
        "        new_cqs = re.split(r'\\?\\\"', text + 'end')\n",
        "        if len(new_cqs) > 1:\n",
        "            for cq in new_cqs[:-1]:\n",
        "                valid.append(cq + '?\"')\n",
        "        else:\n",
        "            still_not_valid.append(text)\n",
        "\n",
        "    for i, cq in enumerate(valid):\n",
        "        occurrence = re.search(r'[A-Z]', cq)\n",
        "        if occurrence:\n",
        "            final.append(cq[occurrence.start():])\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    output = []\n",
        "    if len(final) >= 3:\n",
        "        for i in [0, 1, 2]:\n",
        "            output.append({'cq': final[i]})\n",
        "        return output\n",
        "    else:\n",
        "        return 'Missing CQs'"
      ],
      "metadata": {
        "id": "fwAn93tMywwx"
      },
      "id": "fwAn93tMywwx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_critical_questions_batch(model, tokenizer, model_name, batch_data):\n",
        "    prompts = [\n",
        "        f\"\"\"Suggest 3 critical questions that should be raised before accepting the arguments in this text:\\n\\n\\\"{item['intervention']}\\\"\\n\\nGive one question per line. Make the questions simple, and do not give any explanation regarding why the question is relevant.\"\"\"\n",
        "        for item in batch_data\n",
        "    ]\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=512,\n",
        "          do_sample=True,\n",
        "          temperature=0.6,\n",
        "          top_p=0.9\n",
        "      )\n",
        "\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    del inputs, outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return [\n",
        "        structure_output(decoded[len(prompt):].strip())\n",
        "        for decoded, prompt in zip(decoded_outputs, prompts)\n",
        "    ]"
      ],
      "metadata": {
        "id": "P-t8c-ppywp4"
      },
      "id": "P-t8c-ppywp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(test_dataset_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "for model_info in models:\n",
        "    logger.info(f\"Loading model: {model_info['model_id']}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"model_id\"])\n",
        "    if tokenizer.pad_token is None:\n",
        "      tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_info[\"model_id\"],\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    output_data = []\n",
        "    items = list(data.items())\n",
        "\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        batch = items[i:i+batch_size]\n",
        "        batch_ids = [item_id for item_id, _ in batch]\n",
        "        batch_data = [item for _, item in batch]\n",
        "\n",
        "        questions_list = generate_critical_questions_batch(model, tokenizer, model_info[\"name\"], batch_data)\n",
        "\n",
        "        for item_id, questions in zip(batch_ids, questions_list):\n",
        "            output_entry = {\n",
        "                item_id: {\n",
        "                    \"cqs\": questions\n",
        "                }\n",
        "            }\n",
        "            logger.info(f\"Generated {item_id}: {questions}\")\n",
        "            output_data.append(output_entry)\n",
        "\n",
        "    with open(model_info[\"output_file\"], 'w') as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "\n",
        "    logger.info(f\"Output saved to {model_info['output_file']}\")"
      ],
      "metadata": {
        "id": "mFDwTNeZy6wT"
      },
      "id": "mFDwTNeZy6wT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commit & Push"
      ],
      "metadata": {
        "id": "Pqf89mWeUFAl"
      },
      "id": "Pqf89mWeUFAl"
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\""
      ],
      "metadata": {
        "id": "229p1fTlVjw2"
      },
      "id": "229p1fTlVjw2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commit_message = \"your commit message\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "yDYJfL1_UEg-"
      },
      "id": "yDYJfL1_UEg-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}