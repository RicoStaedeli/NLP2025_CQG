{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_4_ORPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tune with ORPO\n",
    "In this notebook we fine-tune the baseline model with ORPO training\n",
    "- **Baseline Model**: meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "**Prerequisits**\n",
    "- GPU A100 for at least 1 hour\n",
    "- Access to out GitHub repository\n",
    "- HF account with registration for Meta Llama 3.1 model family"
   ],
   "metadata": {
    "id": "xjNdEQfFZsie"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4tKGKGGQWBH"
   },
   "outputs": [],
   "source": [
    "!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes wandb --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "import logging"
   ],
   "metadata": {
    "id": "1qsE1AsYbqPB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Colab\n",
    "This part is only relevant when using the notebook in google colab"
   ],
   "metadata": {
    "id": "cdfFOn11bt9X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import userdata, drive"
   ],
   "metadata": {
    "id": "SnzIK-CKbsFJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "drive.mount('/content/drive')\n",
    "token = userdata.get('GITHUB')"
   ],
   "metadata": {
    "id": "XQOcqp-Uby4t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GitHub\n",
    "Clone GitHub Repository to directly push generated files"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
    "\n",
    "!git clone {repo_url}"
   ],
   "metadata": {
    "id": "tagW5Tp9b0eD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Static Variables"
  },
  {
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "#######################   STATIC VARIABLES      ################################\n",
    "################################################################################\n",
    "\n",
    "TRAINING_NUMBER = 4\n",
    "BASE_MODEL_REPO = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_ORPO\"\n",
    "\n",
    "PUSH_TO_GITHUB = False\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "################################################################################\n",
    "#######################   PATH VARIABLES        ################################\n",
    "################################################################################\n",
    "\n",
    "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ DPO Dataset.json\"\n",
    "\n",
    "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
    "os.makedirs(log_base_path, exist_ok=True)\n",
    "\n",
    "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
    "\n",
    "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
    "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#######################   LOGGER                ################################\n",
    "################################################################################\n",
    "\n",
    "# Setup logger manually\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create file handler (only if not already added)\n",
    "if not logger.handlers:\n",
    "    fh = logging.FileHandler(log_file_path)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ],
   "metadata": {
    "id": "TrIfQmfAb2dW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "logger.info(\"--------  Start with ORPO Training  -------------\")\n",
    "logger.info(f'Device selected: {device}')\n",
    "logger.info(f'Model: {MODEL_NAME}')\n",
    "logger.info(f'Training number: {TRAINING_NUMBER}')"
   ],
   "metadata": {
    "id": "HRE-mOIicakV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwNxh0pCQhfI"
   },
   "outputs": [],
   "source": [
    "# Defined in the secrets tab in Google Colab\n",
    "wb_token = userdata.get('wandb')\n",
    "wandb.login(key=wb_token)\n",
    "torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2kGBNStQoUd"
   },
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_REPO,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('json', data_files=train_dataset_path)\n",
    "dataset = dataset.filter(lambda x: x['score_chosen'] - x['score_rejected'] > 4)"
   ],
   "metadata": {
    "id": "tsD_R0CoctKj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "id": "zv6yYQsDdYE3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaTou6EQQnAK"
   },
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    row[\"prompt\"] = tokenizer.apply_chat_template(row[\"prompt\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "id": "rT78c0i3fq5c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "columns_to_remove = ['id', 'score_chosen', 'score_rejected', 'schema', 'context']\n",
    "\n",
    "dataset = dataset['train'].remove_columns(columns_to_remove)"
   ],
   "metadata": {
    "id": "o4nJmbmuhJU4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = dataset.train_test_split(test_size=0.05)"
   ],
   "metadata": {
    "id": "NxkJl604fqGw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset)"
   ],
   "metadata": {
    "id": "vxep3ltjfxTz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"---------------   Prompt -----------------\")\n",
    "print(dataset['train'][0]['prompt'])\n",
    "print(\"------------------------------------------\")\n",
    "print(\"---------------   Chosen -----------------\")\n",
    "print(\"------------------------------------------\")\n",
    "print(dataset['train'][0]['chosen'])\n",
    "print(\"------------------------------------------\")\n",
    "print(\"---------------   Rejected -----------------\")\n",
    "print(\"------------------------------------------\")\n",
    "print(dataset['train'][0]['rejected'])"
   ],
   "metadata": {
    "id": "uDRHEgWBGyz_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start training"
   ],
   "metadata": {
    "id": "rB7k1fk_Qso6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWDwJe7_Qqgb"
   },
   "outputs": [],
   "source": [
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    beta=0.1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps = 40,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\",\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Flush memory\n",
    "#del trainer, model\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO)\n",
    "\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_REPO,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(fp16_model, MODEL_NAME)\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "id": "n6b3dCS6fL45"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if PUSH_TO_HF:\n",
    "    model.push_to_hub(MODEL_NAME, use_temp_dir=False)\n",
    "    tokenizer.push_to_hub(MODEL_NAME, use_temp_dir=False)"
   ],
   "metadata": {
    "id": "pvWUrbyNgEnX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Push to GitHub"
  },
  {
   "cell_type": "code",
   "source": [
    "if PUSH_TO_GITHUB:\n",
    "    os.chdir(\"NLP2025_CQG\")\n",
    "    !ls\n",
    "\n",
    "    !git config --global user.name \"Rico St√§deli\"\n",
    "    !git config --global user.email \"rico@yabriga.ch\"\n",
    "\n",
    "    commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive.\"\n",
    "    !git add .\n",
    "    !git commit -m \"{commit_message}\"\n",
    "    !git push"
   ],
   "metadata": {
    "id": "JpVu3t9yeWJK"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
