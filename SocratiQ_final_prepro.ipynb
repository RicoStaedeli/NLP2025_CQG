{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/SocratiQ_final_prepro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "41bab268dcf9f851"
      },
      "cell_type": "markdown",
      "source": [
        "# Final preprocessing of full data set for 4 question types (! STRONG threshold here!)"
      ],
      "id": "41bab268dcf9f851"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive\n",
        "import os"
      ],
      "metadata": {
        "id": "saGn6Kn9UBDm"
      },
      "id": "saGn6Kn9UBDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "O7kSVAUQTuao"
      },
      "id": "O7kSVAUQTuao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "wZjqyW3SUDy5"
      },
      "id": "wZjqyW3SUDy5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "80s47wJjUWX9"
      },
      "id": "80s47wJjUWX9",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57507ae3d4b612b4"
      },
      "cell_type": "markdown",
      "source": [
        "### lentgh filtering"
      ],
      "id": "57507ae3d4b612b4"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:21:21.462319Z",
          "start_time": "2025-05-15T16:15:25.899142Z"
        },
        "id": "488634547f88d664"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nltk.download('framenet_v17')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\n",
        "    os.path.join(os.getcwd(), \"Data/Raw/SocraticQ/train_chunk_III.csv\"),\n",
        "    names=[\"category\", \"context\", \"question\"]\n",
        ")\n",
        "\n",
        "df[\"context_token_len\"] = df[\"context\"].apply(lambda text: len(nlp(text)))\n",
        "filtered_df = df[df[\"context_token_len\"] >= 25].copy()\n",
        "\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Rows after filtering: {len(filtered_df)}\")"
      ],
      "id": "488634547f88d664",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d3dbe6be2d9c3d07"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. cause to effect"
      ],
      "id": "d3dbe6be2d9c3d07"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:24:05.012524Z",
          "start_time": "2025-05-15T16:24:05.003862Z"
        },
        "id": "abaa90b8f8570b7a"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import framenet as fn\n",
        "\n",
        "def get_causal_verbs_from_framenet():\n",
        "    causal_frame_names = [\n",
        "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
        "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\"\n",
        "    ]\n",
        "\n",
        "    causal_verbs = set()\n",
        "    for frame_name in causal_frame_names:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:  # Only verbs\n",
        "                    causal_verbs.add(lu['name'].split('.')[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading frame '{frame_name}': {e}\")\n",
        "\n",
        "    return causal_verbs\n",
        "\n",
        "\n",
        "\n",
        "def detect_cause_to_effect(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    explanations = []\n",
        "    score = 0\n",
        "\n",
        "    causal_verbs = get_causal_verbs_from_framenet()\n",
        "\n",
        "    has_condition = any(tok.dep_ == \"mark\" and tok.text.lower() in {\"if\", \"when\"} for tok in doc)\n",
        "    if has_condition:\n",
        "        explanations.append(\"✓ Conditional clause detected (e.g., 'if', 'when')\")\n",
        "        score += 2\n",
        "\n",
        "    has_advcl = any(tok.dep_ == \"advcl\" for tok in doc)\n",
        "    if has_advcl:\n",
        "        explanations.append(\"✓ Adverbial clause (likely effect clause) detected\")\n",
        "        score += 2\n",
        "\n",
        "    has_causal_verb_structure = False\n",
        "    for tok in doc:\n",
        "        if tok.lemma_ in causal_verbs and tok.pos_ == \"VERB\":\n",
        "            subj = any(child.dep_ == \"nsubj\" for child in tok.children)\n",
        "            obj = any(child.dep_ == \"dobj\" for child in tok.children)\n",
        "            prep = any(child.dep_ == \"prep\" for child in tok.children)\n",
        "            if subj or obj or prep:\n",
        "                has_causal_verb_structure = True\n",
        "                explanations.append(\n",
        "                    f\"✓ Verb '{tok.lemma_}' is listed in FrameNet under causal frames with subject/object/prep\"\n",
        "                )\n",
        "                score += 2\n",
        "                if subj: score += 1\n",
        "                if obj: score += 1\n",
        "                if prep: score += 1\n",
        "                break\n",
        "\n",
        "    is_causal = has_condition and has_advcl or has_causal_verb_structure\n",
        "    if not is_causal:\n",
        "        causal_phrases = [\"result in\", \"lead to\", \"cause\", \"because of\", \"due to\"]\n",
        "        if any(phrase in sentence.lower() for phrase in causal_phrases):\n",
        "            explanations.append(\"✓ Phrase pattern matches known cause-to-effect trigger\")\n",
        "            score += 1\n",
        "\n",
        "    score = min(score, 10)\n",
        "    label = \"Strong CauseToEffect\" if score >= 7 else \"Weak/Partial CauseToEffect\" if score >= 4 else \"Not CauseToEffect\"\n",
        "    return label, score, explanations\n"
      ],
      "id": "abaa90b8f8570b7a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "643fcb1c1bf9051d"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. expert opinion"
      ],
      "id": "643fcb1c1bf9051d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:24:07.108589Z",
          "start_time": "2025-05-15T16:24:06.511692Z"
        },
        "id": "4b793c6ec8764e49"
      },
      "cell_type": "code",
      "source": [
        "def get_lexical_units_from_frames(frames):\n",
        "    terms = set()\n",
        "    for frame_name in frames:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:\n",
        "                    terms.add(lu['name'].split('.')[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
        "    return terms\n",
        "\n",
        "\n",
        "expert_frames = [\n",
        "    \"Expertise\", \"Judgment_communication\", \"Opinion\",\n",
        "    \"Authority\", \"Statement\", \"Certainty\"\n",
        "]\n",
        "quote_frames = [\"Statement\", \"Judgment_communication\"]\n",
        "clarity_frames = [\"Reasoning\"]\n",
        "evidence_frames = [\"Evidence\", \"Certainty\", \"Causation\"]\n",
        "\n",
        "\n",
        "expert_verbs = get_lexical_units_from_frames(expert_frames)\n",
        "quote_verbs = get_lexical_units_from_frames(quote_frames)\n",
        "clarity_terms = get_lexical_units_from_frames(clarity_frames)\n",
        "evidence_terms = get_lexical_units_from_frames(evidence_frames)\n",
        "\n",
        "def detect_expert_opinion(question):\n",
        "\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "\n",
        "    expert_titles = {\"expert\", \"researcher\", \"scientist\", \"doctor\", \"analyst\", \"professor\", \"Dr.\"}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
        "            if any(title in ent.text.lower() for title in expert_titles):\n",
        "                explanations.append(f\"✓ Expert entity detected: '{ent.text}'\")\n",
        "                score += 2\n",
        "                break\n",
        "\n",
        "    if any(tok.lemma_ in expert_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Detected expert-related verb from FrameNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_ in quote_verbs for tok in doc):\n",
        "        explanations.append(\"✓ Quotation or claim verb found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in clarity_terms for tok in doc):\n",
        "        explanations.append(\"✓ Clarity/definition markers found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in evidence_terms for tok in doc):\n",
        "        explanations.append(\"✓ Evidence or support-related terms found\")\n",
        "        score += 2\n",
        "\n",
        "    label = \"Strong Expert Opinion\" if score >= 6 else \"Weak/Partial Expert Opinion\" if score >= 3 else \"Not Expert Opinion\"\n",
        "    return label, score, explanations\n"
      ],
      "id": "4b793c6ec8764e49",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "755d0a67225dbe50"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Analogy detection"
      ],
      "id": "755d0a67225dbe50"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:24:09.963923Z",
          "start_time": "2025-05-15T16:24:08.288629Z"
        },
        "id": "42e76e57e97bbaba"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "analogy_synsets = [wn.synset('similar.a.01'), wn.synset('analogy.n.01'), wn.synset('compare.v.01')]\n",
        "\n",
        "comparison_frames = [\"Similarity\"]\n",
        "contrast_frames = [\"Categorization\"]\n",
        "evidence_frames = [\"Evidence\", \"Judgment_communication\"]\n",
        "\n",
        "comparison_verbs = get_lexical_units_from_frames(comparison_frames)\n",
        "contrast_verbs = get_lexical_units_from_frames(contrast_frames)\n",
        "evidence_verbs = get_lexical_units_from_frames(evidence_frames)\n",
        "\n",
        "def is_semantically_analogical(token):\n",
        "    token_synsets = wn.synsets(token.lemma_)\n",
        "    for s in token_synsets:\n",
        "        for analogy_syn in analogy_synsets:\n",
        "            if s.path_similarity(analogy_syn) and s.path_similarity(analogy_syn) > 0.3:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def detect_analogy_question(question):\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "    noun_chunks = list(doc.noun_chunks)\n",
        "\n",
        "    if any(tok.lemma_ in comparison_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Comparison verb detected from FrameNet\")\n",
        "        score += 2\n",
        "\n",
        "    entity_tokens = [tok for tok in doc if tok.pos_ in {\"PROPN\", \"NOUN\"}]\n",
        "    if len(set(tok.lemma_ for tok in entity_tokens)) >= 2:\n",
        "        explanations.append(\"✓ Contains at least two distinct concepts/entities\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in contrast_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Contrast or difference verb detected from FrameNet\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_ in evidence_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Evidence or justification verb found\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.tag_ == \"MD\" for tok in doc):\n",
        "        score += 1\n",
        "\n",
        "    if len(noun_chunks) >= 2 and any(tok.lemma_ in {\"similar\", \"like\", \"as\"} for tok in doc):\n",
        "        explanations.append(\"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.text.lower() == \"if\" for tok in doc):\n",
        "        explanations.append(\"✓ Conditional structure suggesting hypothetical reasoning\")\n",
        "        score += 1\n",
        "\n",
        "    if any(is_semantically_analogical(tok) for tok in doc if tok.pos_ in {\"ADJ\", \"NOUN\", \"VERB\"}):\n",
        "        explanations.append(\"✓ Semantic similarity to analogy-related terms detected via WordNet\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.dep_ in {\"prep\", \"relcl\"} and tok.lemma_ in {\"compare\", \"similar\"} for tok in doc):\n",
        "        explanations.append(\"✓ Syntactic cue of analogy (e.g., 'compared with', 'similar to')\")\n",
        "        score += 1\n",
        "\n",
        "    label = \"Strong Analogy Question\" if score >= 8 else \"Weak/Partial Analogy Question\" if score >= 5 else \"Not Analogy Question\"\n",
        "    return label, score, explanations\n"
      ],
      "id": "42e76e57e97bbaba",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a2cf149cfa5caa03"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Bias"
      ],
      "id": "a2cf149cfa5caa03"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:24:11.581068Z",
          "start_time": "2025-05-15T16:24:11.567985Z"
        },
        "id": "7cf9149aea813762"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def is_fear_related(token):\n",
        "    syns = wn.synsets(token.lemma_)\n",
        "    for s in syns:\n",
        "        if any(s.path_similarity(wn.synset('danger.n.01')) or\n",
        "               s.path_similarity(wn.synset('fear.n.01')) or\n",
        "               s.path_similarity(wn.synset('threat.n.01')) for s in syns):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ---- FrameNet Utility ----\n",
        "def get_lexical_units_from_frames(frames):\n",
        "    terms = set()\n",
        "    for frame_name in frames:\n",
        "        try:\n",
        "            frame = fn.frame_by_name(frame_name)\n",
        "            for lu in frame.lexUnit.values():\n",
        "                if '.v' in lu['name']:\n",
        "                    terms.add(lu['name'].split('.')[0])\n",
        "        except:\n",
        "            continue\n",
        "    return terms\n",
        "\n",
        "# ---- Relevant Lexical Resources ----\n",
        "causal_frames = [\"Causation\", \"Cause_to_start\", \"Preventing\", \"Risk\", \"Threaten\", \"Danger\"]\n",
        "causal_verbs = get_lexical_units_from_frames(causal_frames)\n",
        "\n",
        "fear_keywords = {\"danger\", \"threat\", \"risky\", \"harm\", \"catastrophe\", \"crisis\", \"ruin\", \"fear\", \"worse\", \"bad\", \"fatal\"}\n",
        "preventive_keywords = {\"prevent\", \"avoid\", \"stop\", \"ban\", \"rescue\", \"save\", \"protect\"}\n",
        "modal_keywords = {\"might\", \"could\", \"would\", \"may\", \"should\"}\n",
        "\n",
        "def detect_fear_appeal_question(question):\n",
        "    doc = nlp(question)\n",
        "    score = 0\n",
        "    explanations = []\n",
        "\n",
        "    if any(tok.lemma_ in modal_keywords for tok in doc if tok.tag_ == \"MD\"):\n",
        "        explanations.append(\"✓ Modal verb detected (e.g., 'might', 'would') suggesting hypothetical risk\")\n",
        "        score += 1\n",
        "\n",
        "    if any(tok.lemma_.lower() in fear_keywords for tok in doc):\n",
        "        explanations.append(\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_.lower() in preventive_keywords for tok in doc):\n",
        "        explanations.append(\"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.lemma_ in causal_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
        "        explanations.append(\"✓ Causal/preventive verb from FrameNet detected\")\n",
        "        score += 2\n",
        "\n",
        "    if any(tok.text.lower() in {\"if\", \"unless\"} for tok in doc):\n",
        "        explanations.append(\"✓ Conditional clause found (e.g., 'if', 'unless')\")\n",
        "        score += 1\n",
        "\n",
        "    if any(is_fear_related(tok) for tok in doc if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"}):\n",
        "        explanations.append(\"✓ Semantic fear-related concept detected via WordNet\")\n",
        "        score += 2\n",
        "\n",
        "    label = \"Strong Fear Appeal\" if score >= 6 else \"Weak/Partial Fear Appeal\" if score >= 4 else \"Not Fear Appeal\"\n",
        "    return label, score, explanations\n"
      ],
      "id": "7cf9149aea813762",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7a8603ab85946f5f"
      },
      "cell_type": "markdown",
      "source": [
        "# Final method for preprocessing and filtering for all types"
      ],
      "id": "7a8603ab85946f5f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:29:19.482629Z",
          "start_time": "2025-05-15T16:29:15.075842Z"
        },
        "id": "e2a3daba550e0722"
      },
      "cell_type": "code",
      "source": [
        "def classify_schema(row):\n",
        "    question = row[\"question\"]\n",
        "    results = []\n",
        "\n",
        "\n",
        "    cte_label, cte_score, _ = detect_cause_to_effect(question)\n",
        "    if cte_score >= 7:\n",
        "        results.append((\"CauseToEffect\", cte_score))\n",
        "    elif cte_score >= 4:\n",
        "        results.append((\"CauseToEffect\", cte_score))\n",
        "\n",
        "    expert_label, expert_score, _ = detect_expert_opinion(question)\n",
        "    if expert_score >= 6:\n",
        "        results.append((\"ExpertOpinion\", expert_score))\n",
        "    elif expert_score >= 3:\n",
        "        results.append((\"ExpertOpinion\", expert_score))\n",
        "\n",
        "    analogy_label, analogy_score, _ = detect_analogy_question(question)\n",
        "    if analogy_score >= 8:\n",
        "        results.append((\"Analogy\", analogy_score))\n",
        "    elif analogy_score >= 5:\n",
        "        results.append((\"Analogy\", analogy_score))\n",
        "\n",
        "    fear_label, fear_score, _ = detect_fear_appeal_question(question)\n",
        "    if fear_score >= 6:\n",
        "        results.append((\"FearAppeal\", fear_score))\n",
        "    elif fear_score >= 4:\n",
        "        results.append((\"FearAppeal\", fear_score))\n",
        "\n",
        "    results.sort(key=lambda x: -x[1])\n",
        "\n",
        "\n",
        "    schema_labels = [label for label, _ in results]\n",
        "    while len(schema_labels) < 4:\n",
        "        schema_labels.append(\"\")\n",
        "\n",
        "    return pd.Series({\n",
        "        \"schema_1\": schema_labels[0],\n",
        "        \"schema_2\": schema_labels[1],\n",
        "        \"schema_3\": schema_labels[2],\n",
        "        \"schema_4\": schema_labels[3],\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#filtered_df = filtered_df[:1000]\n",
        "filtered_df[[\"schema_1\", \"schema_2\", \"schema_3\", \"schema_4\"]] = filtered_df.apply(classify_schema, axis=1)\n",
        "filtered_df"
      ],
      "id": "e2a3daba550e0722",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "67c7e6d5f854c8ba"
      },
      "cell_type": "markdown",
      "source": [
        "### final check: how many questions per category ?"
      ],
      "id": "67c7e6d5f854c8ba"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-15T16:30:32.354376Z",
          "start_time": "2025-05-15T16:30:32.347133Z"
        },
        "id": "1d52494ad947f855"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_schemas = filtered_df[\"schema_1\"].tolist() + filtered_df[\"schema_2\"].tolist() + filtered_df[\"schema_3\"].tolist() + filtered_df[\"schema_4\"].tolist()\n",
        "all_schemas = [schema for schema in all_schemas if schema]\n",
        "\n",
        "\n",
        "schema_counts = Counter(all_schemas)\n",
        "\n",
        "schema_summary = pd.DataFrame.from_dict(schema_counts, orient='index', columns=['count'])\n",
        "schema_summary[\"percent\"] = (schema_summary[\"count\"] / len(filtered_df)) * 100\n",
        "schema_summary = schema_summary.sort_values(\"count\", ascending=False)\n",
        "\n",
        "print(\"\\nSchema classification distribution:\")\n",
        "print(schema_summary)\n"
      ],
      "id": "1d52494ad947f855",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c812e73a5186fa37"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "non_empty_mask = filtered_df[[\"schema_1\", \"schema_2\", \"schema_3\", \"schema_4\"]].map(lambda x: pd.notna(x) and str(x).strip() != \"\")\n",
        "multiple_schema_count = (non_empty_mask.sum(axis=1) > 1).sum()\n",
        "schema_1_count = filtered_df[\"schema_1\"].map(lambda x: pd.notna(x) and str(x).strip() != \"\").sum()\n",
        "\n",
        "schema_2_count = filtered_df[\"schema_2\"].map(lambda x: pd.notna(x) and str(x).strip() != \"\").sum()\n",
        "print(schema_1_count)\n",
        "print(schema_2_count)"
      ],
      "id": "c812e73a5186fa37"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.to_csv('/content/drive/MyDrive/HSG/NLP/Project NLP/Data/Final/processed_train_chunk_III.json', index=False)"
      ],
      "metadata": {
        "id": "Qhmiug1WYG4d"
      },
      "id": "Qhmiug1WYG4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Showcas\"\n",
        "!git config --global user.email \"cedric.bohni@gmx.de\"\n",
        "\n",
        "\n",
        "commit_message = f\"Improved Preprocessing file\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Et8Dsm7-UgOB"
      },
      "id": "Et8Dsm7-UgOB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}