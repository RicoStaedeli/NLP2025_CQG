{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMG9e8MLGu8iSiPU0tdd1b5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/5_RAG_System%2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install feedparser\n",
        "!pip install pymupdf\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "cYpunciwO9Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "import feedparser\n",
        "import urllib.parse\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import os"
      ],
      "metadata": {
        "id": "eDLkCWdZSG1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the keyword extractor\n",
        "kw_model = KeyBERT()"
      ],
      "metadata": {
        "id": "PPwAGmOCS3dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topics(text, num_topics=3):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=num_topics)\n",
        "    return [kw[0] for kw in keywords]\n",
        "\n",
        "def get_top_arxiv_papers(topic, max_results=2):\n",
        "    base_url = \"http://export.arxiv.org/api/query?\"\n",
        "    encoded_topic = urllib.parse.quote(topic)\n",
        "    query = f\"search_query=all:{encoded_topic}&start=0&max_results={max_results}&sortBy=relevance&sortOrder=descending\"\n",
        "    feed = feedparser.parse(base_url + query)\n",
        "\n",
        "    papers = []\n",
        "    for entry in feed.entries:\n",
        "        paper = {\n",
        "            \"title\": entry.title,\n",
        "            \"authors\": [author.name for author in entry.authors],\n",
        "            \"summary\": entry.summary,\n",
        "            \"published\": entry.published,\n",
        "            \"link\": entry.link,\n",
        "            \"pdf_url\": entry.id.replace('abs', 'pdf') + '.pdf'\n",
        "        }\n",
        "        papers.append(paper)\n",
        "\n",
        "    return papers\n",
        "\n",
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        for page in doc:\n",
        "            full_text += page.get_text()\n",
        "        return full_text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"[Error extracting text: {e}]\""
      ],
      "metadata": {
        "id": "8-5J551MVN9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download relevnt arxiv papers"
      ],
      "metadata": {
        "id": "kqgz3eDmVg0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmFxL9oGOMEV"
      },
      "outputs": [],
      "source": [
        "input_text = \"\"\"The central question in this election is really what kind of country we want to be and what kind of future we 'll build together\n",
        "              Today is my granddaughter 's second birthday\n",
        "              I think about this a lot\n",
        "              we have to build an economy that works for everyone , not just those at the top\n",
        "              we need new jobs , good jobs , with rising incomes\n",
        "              I want us to invest in you\n",
        "              I want us to invest in your future\n",
        "              jobs in infrastructure , in advanced manufacturing , innovation and technology , clean , renewable energy , and small business\n",
        "              most of the new jobs will come from small business\n",
        "              We also have to make the economy fairer\n",
        "              That starts with raising the national minimum wage and also guarantee , finally , equal pay for women 's work\n",
        "              I also want to see more companies do profit-sharing\"\"\"\n",
        "\n",
        "topics = extract_topics(input_text)\n",
        "print(f\"Detected topics: {topics}\")\n",
        "\n",
        "for topic in topics:\n",
        "    print(f\"\\nTop papers for topic: {topic}\")\n",
        "    papers = get_top_arxiv_papers(topic)\n",
        "\n",
        "    for i, paper in enumerate(papers, 1):\n",
        "        print(f\"\\n--- Paper {i} ---\")\n",
        "        print(f\"Title: {paper['title']}\")\n",
        "        print(f\"Authors: {', '.join(paper['authors'])}\")\n",
        "        print(f\"Published: {paper['published']}\")\n",
        "        print(f\"Link: {paper['link']}\")\n",
        "        print(f\"Summary: {paper['summary'][:300]}...\")\n",
        "\n",
        "        # Download PDF\n",
        "        filename = f\"paper_{i}_{topic.replace(' ', '_')}.pdf\"\n",
        "        if download_pdf(paper['pdf_url'], filename):\n",
        "            print(f\"PDF downloaded: {filename}\")\n",
        "            text = extract_text_from_pdf(filename)\n",
        "            print(f\"\\nExtracted Text Preview:\\n{text[:1000]}...\\n\")\n",
        "            # os.remove(filename)  # Clean up\n",
        "        else:\n",
        "            print(\"⚠️ Failed to download PDF.\")\n"
      ]
    }
  ]
}