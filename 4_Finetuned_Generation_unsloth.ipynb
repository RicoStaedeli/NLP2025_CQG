{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/4_Finetuned_Generation_unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d44ba819438d4c7f"
      },
      "cell_type": "markdown",
      "source": [
        "# Finetuned Predictions\n",
        "In this file we generate the finetuned predictions"
      ],
      "id": "d44ba819438d4c7f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "6yd9jnEIPiXa"
      },
      "id": "6yd9jnEIPiXa"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth_zoo\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install accelerate bitsandbytes\n",
        "!pip install -U peft"
      ],
      "metadata": {
        "id": "HbEouL7eL0AB"
      },
      "id": "HbEouL7eL0AB",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.252191Z",
          "start_time": "2025-04-28T13:09:02.197619Z"
        },
        "id": "56e5bbe87af65278"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "import logging\n",
        "import transformers\n",
        "import os\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from bitsandbytes import nn as bnb_nn\n",
        "from unsloth import FastLanguageModel\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "id": "56e5bbe87af65278",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "7tOU4FCdPmop"
      },
      "id": "7tOU4FCdPmop",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-28T13:09:03.322602Z",
          "start_time": "2025-04-28T13:09:03.308445Z"
        },
        "id": "c97a513058176f9f"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "BASE_MODEL = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
        "model_name = \"Meta-Llama-3.1-8B-Instruct_DPO\"\n",
        "model_id = \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_DPO\" #\"meta-llama/Llama-3.1-8B-Instruct\" # ricostaedeli/Meta-Llama-3.1-8B-Instruct_DPO\n",
        "\n",
        "test_dataset_path = f\"/content/NLP2025_CQG/Data/Processed/test.csv\"\n",
        "\n",
        "results_path = os.path.join(os.getcwd(), f\"/content/NLP2025_CQG/Evaluation/Results/results_{model_name}.json\")\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Logs/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_path = log_base_path + f\"4_cqs_generation_{model_name}.log\"\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n"
      ],
      "id": "c97a513058176f9f",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "# Log the device info\n",
        "logger.info(\"--------  Start with Baseline Generation  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Results Path: {results_path}')\n",
        "logger.info(f'Log Path: {log_path}')\n",
        "logger.info(\"--------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "zbgFJsxgKi40"
      },
      "id": "zbgFJsxgKi40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Answers"
      ],
      "metadata": {
        "id": "z-iPCguiZEoP"
      },
      "id": "z-iPCguiZEoP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess dataset"
      ],
      "metadata": {
        "id": "I5JlZ5K__oHb"
      },
      "id": "I5JlZ5K__oHb"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Define the schema types\n",
        "schemas = [\"CauseToEffect\", \"ExpertOpinion\", \"Analogy\", \"FearAppeal\"]\n",
        "\n",
        "# Prepare the JSON structure\n",
        "json_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    context = row['input'].strip()\n",
        "    original_id = row['id']\n",
        "\n",
        "    for schema in schemas:\n",
        "        entry = {\n",
        "            \"prompt\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.\"\n",
        "                },\n",
        "                 {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Generate one critical question addressing the provided context. Ensure it matches the schema:{schema}\\n\\nContext: {context} Respond only with the question and nothing else.\"\n",
        "                },\n",
        "                 {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": \"Question: \"\n",
        "                }\n",
        "\n",
        "            ],\n",
        "            \"id\": f\"{original_id}\",\n",
        "            \"schema\" : schema,\n",
        "            \"input\" : context\n",
        "\n",
        "        }\n",
        "        json_data.append(entry)\n",
        "\n",
        "\n",
        "# Save the JSON data to a file\n",
        "temp_output_file = 'processed_dataset.json'\n",
        "with open(temp_output_file, 'w') as f:\n",
        "    json.dump(json_data, f, indent=4)"
      ],
      "metadata": {
        "id": "RYW1D3-5_kq5"
      },
      "id": "RYW1D3-5_kq5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation with transformers"
      ],
      "metadata": {
        "id": "iusfZQx8eQEc"
      },
      "id": "iusfZQx8eQEc"
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8kQxcTvzijM1"
      },
      "id": "8kQxcTvzijM1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16},\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "NC3wxLXMjpmP"
      },
      "id": "NC3wxLXMjpmP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pipeline with quantized model for less GPU usage\n"
      ],
      "metadata": {
        "id": "ui1nT_se4xaS"
      },
      "id": "ui1nT_se4xaS"
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\n",
        "        \"torch_dtype\": torch.float16,\n",
        "        \"quantization_config\": {\"load_in_4bit\": True},\n",
        "        \"low_cpu_mem_usage\": True,\n",
        "    },\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "qOpGGiwQlDj4"
      },
      "id": "qOpGGiwQlDj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single generation"
      ],
      "metadata": {
        "id": "nbHgSdKr4j2Q"
      },
      "id": "nbHgSdKr4j2Q"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_response(query, max_tokens=512, temperature=0.6, top_p=0.9):\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        query, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    response = outputs[0][\"generated_text\"][len(prompt):]\n",
        "    return response\n",
        "    '''"
      ],
      "metadata": {
        "id": "3UeKeadQj2_F"
      },
      "id": "3UeKeadQj2_F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with open(temp_output_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "combined_output = {}\n",
        "i = 0\n",
        "for item in data:\n",
        "    print(f\"Number: {i}\")\n",
        "    item_id = item[\"id\"]\n",
        "    schema = item[\"schema\"]\n",
        "    message = item[\"prompt\"]\n",
        "    input = item[\"input\"]\n",
        "    question = get_response(message).strip()\n",
        "\n",
        "    if item_id not in combined_output:\n",
        "        combined_output[item_id] = {\"input\": input, \"cqs\": []}\n",
        "\n",
        "    combined_output[item_id][\"cqs\"].append({\"schema\": schema, \"cq\": question})\n",
        "    i = i + 1\n",
        "    if i == 2:\n",
        "      break\n",
        "\n",
        "\n",
        "# Save the combined questions\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(combined_output, f, indent=4)\n",
        "print(f\"Combined questions saved to {results_path}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "tFszEi2XkHMp"
      },
      "id": "tFszEi2XkHMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch processing"
      ],
      "metadata": {
        "id": "CdE1gOeR1493"
      },
      "id": "CdE1gOeR1493"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_responses(queries, max_tokens=512, temperature=0.6, top_p=0.9):\n",
        "    # Prepare the prompts\n",
        "    prompts = [pipeline.tokenizer.apply_chat_template(q, tokenize=False, add_generation_prompt=True) for q in queries]\n",
        "\n",
        "    # Run generation in batches to avoid OOM\n",
        "    all_responses = []\n",
        "    batch_size = 50\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "        # Run the pipeline in batch mode\n",
        "        outputs = pipeline(\n",
        "            batch_prompts,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "\n",
        "        # Extract responses\n",
        "        for prompt, output_list in zip(batch_prompts, outputs):\n",
        "            # Each output_list is a list with a single dictionary\n",
        "            generated_text = output_list[0][\"generated_text\"]\n",
        "            response = generated_text[len(prompt):].strip()\n",
        "            all_responses.append(response)\n",
        "            print(response)\n",
        "\n",
        "    return all_responses\n",
        "\n",
        "# Load the data\n",
        "with open(temp_output_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "queries = [item[\"prompt\"] for item in data]\n",
        "responses = get_responses(queries)\n",
        "\n",
        "# Combine the outputs\n",
        "combined_output = {}\n",
        "for item, response in zip(data, responses):\n",
        "    item_id = item[\"id\"]\n",
        "    schema = item[\"schema\"]\n",
        "    input = item[\"input\"]\n",
        "\n",
        "    if item_id not in combined_output:\n",
        "        combined_output[item_id] = {\"input\": input, \"cqs\": []}\n",
        "\n",
        "    combined_output[item_id][\"cqs\"].append({\"schema\": schema, \"cq\": response})\n",
        "\n",
        "# Save the combined questions\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(combined_output, f, indent=4)\n",
        "\n",
        "print(f\"Combined questions saved to {results_path}\")"
      ],
      "metadata": {
        "id": "P7QFLvBir-sG"
      },
      "id": "P7QFLvBir-sG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.remove(temp_output_file)\n",
        "    print(f\"File '{temp_output_file}' deleted successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{temp_output_file}' not found.\")\n",
        "except PermissionError:\n",
        "    print(f\"Permission denied to delete '{temp_output_file}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting file: {e}\")"
      ],
      "metadata": {
        "id": "64jpcwQ2P0QD"
      },
      "id": "64jpcwQ2P0QD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commit & Push"
      ],
      "metadata": {
        "id": "Pqf89mWeUFAl"
      },
      "id": "Pqf89mWeUFAl"
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "M60niewMW7dP"
      },
      "id": "M60niewMW7dP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabrriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Finetuned generation\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "yDYJfL1_UEg-"
      },
      "id": "yDYJfL1_UEg-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "name": "2_baseline_cqs_generation_schema.ipynb",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}