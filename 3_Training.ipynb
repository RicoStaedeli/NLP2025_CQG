{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/3_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# Training for critical question"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "%%capture\n",
        "import shutil\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}\n",
        "\n",
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 3\n",
        "BASE_MODEL_REPO = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-1B-Instruct\"\n",
        "NOTEBOOK_PATH = \"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/3_Training.ipynb\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/drive/MyDrive/HSG/NLP/Project NLP/Data/Datasets/SocraticQ/train.csv\"\n",
        "\n",
        "log_base_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Logs/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "save_notebook_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Training_{TRAINING_NUMBER}.ipynb\"\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   SFT Trainer Parameters   #############################\n",
        "################################################################################\n",
        "\n",
        "dataset_text_field = \"input_ids\"\n",
        "dataset_num_proc = 2\n",
        "packing = False\n",
        "per_device_train_batch_size = 2\n",
        "gradient_accumulation_steps = 4\n",
        "warmup_steps = 5\n",
        "max_steps = 10\n",
        "learning_rate = 2e-4\n",
        "fp16 = not is_bfloat16_supported()\n",
        "bf16 = is_bfloat16_supported()\n",
        "logging_steps = 1\n",
        "save_strategy = \"steps\"\n",
        "save_steps = 1\n",
        "save_total_limit = 1\n",
        "optim = \"adamw_8bit\"\n",
        "weight_decay = 0.01\n",
        "lr_scheduler_type = \"linear\"\n",
        "seed = 3407\n",
        "output_dir = checkpoint_dir\n",
        "report_to = \"tensorboard\"\n",
        "logging_dir = log_base_path\n",
        "evaluation_strategy=\"steps\"\n",
        "eval_steps=1\n",
        "\n",
        "################################################################################\n",
        "#######################   Log Parameters            ############################\n",
        "################################################################################\n",
        "\n",
        "logger.info(\"------ Unlsoth Parameters ---------------\")\n",
        "logger.info(f\"max_seq_length: {max_seq_length}\")\n",
        "logger.info(f\"dtype: {dtype}\")\n",
        "logger.info(f\"load_in_4bit: {load_in_4bit}\")\n",
        "\n",
        "logger.info(\"------ PEFT Parameters ------------------\")\n",
        "logger.info(f\"r: {r}\")\n",
        "logger.info(f\"target_modules: {target_modules}\")\n",
        "logger.info(f\"lora_alpha: {lora_alpha}\")\n",
        "logger.info(f\"lora_dropout: {lora_dropout}\")\n",
        "logger.info(f\"bias: {bias}\")\n",
        "logger.info(f\"use_gradient_checkpointing: {use_gradient_checkpointing}\")\n",
        "logger.info(f\"random_state: {random_state}\")\n",
        "logger.info(f\"use_rslora: {use_rslora}\")\n",
        "\n",
        "logger.info(\"------  SFT Trainer Parameters ----------\")\n",
        "logger.info(f\"dataset_text_field: {dataset_text_field}\")\n",
        "logger.info(f\"dataset_num_proc: {dataset_num_proc}\")\n",
        "logger.info(f\"packing: {packing}\")\n",
        "logger.info(f\"per_device_train_batch_size: {per_device_train_batch_size}\")\n",
        "logger.info(f\"gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
        "logger.info(f\"warmup_steps: {warmup_steps}\")\n",
        "logger.info(f\"max_steps: {max_steps}\")\n",
        "logger.info(f\"learning_rate: {learning_rate}\")\n",
        "logger.info(f\"fp16: {fp16}\")\n",
        "logger.info(f\"bf16: {bf16}\")\n",
        "logger.info(f\"logging_steps: {logging_steps}\")\n",
        "logger.info(f\"save_strategy: {save_strategy}\")\n",
        "logger.info(f\"save_steps: {save_steps}\")\n",
        "logger.info(f\"save_total_limit: {save_total_limit}\")\n",
        "logger.info(f\"optim: {optim}\")\n",
        "logger.info(f\"weight_decay: {weight_decay}\")\n",
        "logger.info(f\"lr_scheduler_type: {lr_scheduler_type}\")\n",
        "logger.info(f\"seed: {seed}\")\n",
        "logger.info(f\"output_dir: {output_dir}\")\n",
        "logger.info(f\"report_to: {report_to}\")\n",
        "logger.info(f\"logging_dir: {logging_dir}\")\n",
        "logger.info(f\"evaluation_strategy: {evaluation_strategy}\")\n",
        "logger.info(f\"eval_steps: {eval_steps}\")"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('csv', data_files=train_dataset_path,split=\"train\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "karDAuqnlDPd"
      },
      "source": [
        "# crop the dataset for testing\n",
        "#dataset['train'] = dataset['train'].select(range(500))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLiLKCAPbBvL"
      },
      "source": [
        "# extract the input text and the label\n",
        "def split_input(example):\n",
        "    if ':' in example['input']:\n",
        "        before, after = example['input'].split(':', 1)\n",
        "        return {\n",
        "            'label': before.strip(),\n",
        "            'input_text': after.strip()\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'label': example['input'].strip(),\n",
        "            'input_text': ''\n",
        "        }\n",
        "\n",
        "# Apply the map function\n",
        "dataset = dataset.map(split_input)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to see a sample\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "IdDorVco28h_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVBEly_dR9r"
      },
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You will see a text and you should generate one critical question for this text.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"input_text\"]\n",
        "    outputs      = examples[\"target\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzpgH-4hgOSE"
      },
      "source": [
        "# Check the structure of the dataset\n",
        "print(dataset[0]['text'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "QtoUXIkSEOqM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "    )\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=2,\n",
        "    remove_columns=dataset.column_names, # remove all unnecessary columns\n",
        ")"
      ],
      "metadata": {
        "id": "25CjnKYM8XaS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "lq4lRwcs_O2r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Access train and eval splits\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']"
      ],
      "metadata": {
        "id": "VUtmfc1bX4gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)\n",
        "print(eval_dataset)"
      ],
      "metadata": {
        "id": "vH9fqG6wZaap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = dataset_text_field,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = dataset_num_proc,\n",
        "    packing = packing,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = per_device_train_batch_size,\n",
        "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "        warmup_steps = warmup_steps,\n",
        "        max_steps = max_steps,\n",
        "        learning_rate = learning_rate,\n",
        "        fp16 = fp16,\n",
        "        bf16 = bf16,\n",
        "        logging_steps = logging_steps,\n",
        "        save_strategy = save_strategy,\n",
        "        save_steps = save_steps,\n",
        "        save_total_limit = save_total_limit,\n",
        "        optim = optim,\n",
        "        weight_decay = weight_decay,\n",
        "        lr_scheduler_type = lr_scheduler_type,\n",
        "        seed = seed,\n",
        "        output_dir = output_dir,\n",
        "        report_to = report_to,\n",
        "        logging_dir = logging_dir,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With early stopping"
      ],
      "metadata": {
        "id": "hOHPzD30U-4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    embeddings1 = model_sbert.encode(decoded_preds, convert_to_tensor=True)\n",
        "    embeddings2 = model_sbert.encode(decoded_labels, convert_to_tensor=True)\n",
        "\n",
        "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "    mean_score = cosine_scores.diag().mean().item()\n",
        "\n",
        "    return {\"semantic_similarity\": mean_score}"
      ],
      "metadata": {
        "id": "08gxNLb7YbDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    dataset_text_field = dataset_text_field,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = dataset_num_proc,\n",
        "\n",
        "    packing = packing,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = per_device_train_batch_size,\n",
        "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "        warmup_steps = warmup_steps,\n",
        "        max_steps = max_steps,\n",
        "        learning_rate = learning_rate,\n",
        "        fp16 = fp16,\n",
        "        bf16 = bf16,\n",
        "        logging_steps = logging_steps,\n",
        "        save_strategy = save_strategy,\n",
        "        save_steps = save_steps,\n",
        "        save_total_limit = save_total_limit,\n",
        "        optim = optim,\n",
        "        weight_decay = weight_decay,\n",
        "        lr_scheduler_type = lr_scheduler_type,\n",
        "        seed = seed,\n",
        "        output_dir = output_dir,\n",
        "        report_to = report_to,\n",
        "        logging_dir = logging_dir,\n",
        "        evaluation_strategy=evaluation_strategy,\n",
        "        eval_steps=eval_steps,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"semantic_similarity\",  # or \"perplexity\"\n",
        "        greater_is_better=True,           # because lower loss is better\n",
        "    ),\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "metadata": {
        "id": "HybmWblAU9ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit\n",
        "if True:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_4bit\",)\n",
        "  logger.info(f\"Saved merged model in 4bit to {model_save_path}\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"lora\",)\n",
        "  logger.info(f\"Saved LoRA adapters to {model_save_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "Save model in GGUF format\n",
        "\n",
        "Some supported quant methods (full list on [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "  model.save_pretrained_gguf(model_save_path, tokenizer,)\n",
        "  logger.info(f\"Saved merged model in 8bit Q8_0 to {model_save_path}\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "  model.save_pretrained_gguf(model_save_path, tokenizer, quantization_method = \"f16\")\n",
        "  logger.info(f\"Saved merged model in 16bit GGUF to {model_save_path}\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "  model.save_pretrained_gguf(model_save_path, tokenizer, quantization_method = \"q4_k_m\")\n",
        "  logger.info(f\"Saved merged model in q4_k_m GGUF to {model_save_path}\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy current Notebook to Log Folder\n",
        "shutil.copy(NOTEBOOK_PATH, save_notebook_path)"
      ],
      "metadata": {
        "id": "QpD1RT6hDBEJ"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}