{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-06T18:12:06.223398Z",
     "start_time": "2025-05-06T18:12:06.221532Z"
    }
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:13:17.847748Z",
     "start_time": "2025-05-06T18:13:17.844378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the JSON file\n",
    "file_path = Path(\"../../../Data/Processed/validation.json\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract questions into a DataFrame\n",
    "questions = []\n",
    "for intervention_data in data.values():\n",
    "    for cq in intervention_data[\"cqs\"]:\n",
    "        questions.append({\n",
    "            \"id\": cq[\"id\"],\n",
    "            \"target\": cq[\"cq\"],\n",
    "            \"label\": cq[\"label\"]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(questions)"
   ],
   "id": "6d56ca7d1e737840",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:32:17.117719Z",
     "start_time": "2025-05-06T14:32:17.051067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the uploaded dataset\n",
    "df = pd.read_csv('../../../Data/Processed/SocraticQ/test.csv')"
   ],
   "id": "85c19376b8564319",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T18:13:20.776649Z",
     "start_time": "2025-05-06T18:13:20.770277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classify_blooms(question):\n",
    "    question = question.lower()\n",
    "    blooms_keywords = {\n",
    "        \"Remember\": [\"define\", \"list\", \"name\", \"identify\", \"recall\"],\n",
    "        \"Understand\": [\"summarize\", \"explain\", \"describe\", \"interpret\"],\n",
    "        \"Apply\": [\"apply\", \"use\", \"demonstrate\", \"solve\"],\n",
    "        \"Analyze\": [\"compare\", \"contrast\", \"analyze\", \"differentiate\", \"distinguish\"],\n",
    "        \"Evaluate\": [\"evaluate\", \"justify\", \"critique\", \"argue\", \"assess\"],\n",
    "        \"Create\": [\"create\", \"design\", \"formulate\", \"develop\", \"propose\"]\n",
    "    }\n",
    "    for level, keywords in reversed(blooms_keywords.items()):  # start from top\n",
    "        if any(kw in question for kw in keywords):\n",
    "            return level\n",
    "    return \"Unknown\"\n",
    "\n",
    "def detect_socratic_types(question):\n",
    "    socratic_patterns = {\n",
    "        \"Clarification\": [\"what do you mean\", \"could you explain\", \"can you clarify\"],\n",
    "        \"Assumption\": [\"what are you assuming\", \"underlying assumption\", \"based on assumption\"],\n",
    "        \"Evidence\": [\"what evidence\", \"how do you know\", \"based on what\"],\n",
    "        \"Perspective\": [\"what is another\", \"how might someone else\", \"different viewpoint\"],\n",
    "        \"Implication\": [\"what are the implications\", \"what might happen\", \"what follows if\"],\n",
    "        \"Meta\": [\"why is this question\", \"what is the point of asking\"]\n",
    "    }\n",
    "    matches = []\n",
    "    for typ, patterns in socratic_patterns.items():\n",
    "        if any(re.search(pat, question.lower()) for pat in patterns):\n",
    "            matches.append(typ)\n",
    "    return matches\n",
    "\n",
    "def estimate_dok(question):\n",
    "    question = question.lower()\n",
    "    if any(x in question for x in [\"define\", \"list\", \"name\", \"who\", \"what\", \"when\"]):\n",
    "        return 1\n",
    "    elif any(x in question for x in [\"explain\", \"summarize\", \"compare\", \"describe\"]):\n",
    "        return 2\n",
    "    elif any(x in question for x in [\"analyze\", \"justify\", \"evaluate\", \"why\", \"how\"]):\n",
    "        return 3\n",
    "    elif any(x in question for x in [\"design\", \"construct\", \"formulate\", \"propose\"]):\n",
    "        return 4\n",
    "    return 0\n",
    "\n",
    "def evaluate_paul_elder(question):\n",
    "    question = question.lower()\n",
    "    standards = {\n",
    "        \"clarity\": len(question) > 10 and not question.strip().endswith(\"?\") == False,\n",
    "        \"relevance\": True,  # Assume relevance unless flagged\n",
    "        \"depth\": any(x in question for x in [\"why\", \"how\", \"challenge\", \"complex\"]),\n",
    "        \"breadth\": any(x in question for x in [\"other perspective\", \"different\", \"compare\"]),\n",
    "        \"logic\": not any(x in question for x in [\"nonsense\", \"contradiction\"]),\n",
    "        \"significance\": len(question.split()) > 5,\n",
    "        \"fairness\": not any(x in question for x in [\"always\", \"never\", \"biased\"])\n",
    "    }\n",
    "    return standards"
   ],
   "id": "c74d5aaa81e1b103",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T19:43:47.657838Z",
     "start_time": "2025-05-06T19:43:47.648537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the evaluation functions to the dataset\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    q = row['target']\n",
    "    bloom = classify_blooms(q)\n",
    "    socratic = detect_socratic_types(q)\n",
    "    dok = estimate_dok(q)\n",
    "    paul_elder = evaluate_paul_elder(q)\n",
    "    total_pe_score = sum(paul_elder.values())\n",
    "\n",
    "    # Final composite criticality score (example weights)\n",
    "    bloom_score = {\"Remember\": 1, \"Understand\": 2, \"Apply\": 3, \"Analyze\": 4, \"Evaluate\": 5, \"Create\": 6}.get(bloom, 0)\n",
    "    criticality_score = bloom_score * 1.0 + len(socratic) * 0.7 + dok * 1.0 + total_pe_score * 0.5\n",
    "    label = \"Useful\" if criticality_score >= 10 else \"Unhelpful\" if criticality_score >= 6 else \"Invalid\"\n",
    "\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"bloom_level\": bloom,\n",
    "        \"socratic_types\": socratic,\n",
    "        \"dok_level\": dok,\n",
    "        \"paul_elder_score\": total_pe_score,\n",
    "        \"criticality_score\": round(criticality_score, 2),\n",
    "        \"label\": label,\n",
    "        \"label_shared_task\":row[\"label\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ],
   "id": "384bd5d9f2dd71e4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T19:43:49.322966Z",
     "start_time": "2025-05-06T19:43:49.317743Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.head()",
   "id": "524f0cef0e0a415b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            question bloom_level  \\\n",
       "0  How would you address potential criticisms and...     Unknown   \n",
       "1  Are there other relevant goals that conflict w...     Unknown   \n",
       "2  What is the proposed plan for making the econo...      Create   \n",
       "3  What specific policies would you implement to ...     Unknown   \n",
       "4  Could Clinton investing in you have consequenc...     Unknown   \n",
       "\n",
       "  socratic_types  dok_level  paul_elder_score  criticality_score      label  \\\n",
       "0             []          1                 6                4.0    Invalid   \n",
       "1             []          0                 5                2.5    Invalid   \n",
       "2             []          1                 5                9.5  Unhelpful   \n",
       "3             []          1                 6                4.0    Invalid   \n",
       "4             []          0                 5                2.5    Invalid   \n",
       "\n",
       "  label_shared_task  \n",
       "0           Invalid  \n",
       "1         Unhelpful  \n",
       "2            Useful  \n",
       "3           Invalid  \n",
       "4            Useful  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>bloom_level</th>\n",
       "      <th>socratic_types</th>\n",
       "      <th>dok_level</th>\n",
       "      <th>paul_elder_score</th>\n",
       "      <th>criticality_score</th>\n",
       "      <th>label</th>\n",
       "      <th>label_shared_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How would you address potential criticisms and...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Invalid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are there other relevant goals that conflict w...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Unhelpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the proposed plan for making the econo...</td>\n",
       "      <td>Create</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Unhelpful</td>\n",
       "      <td>Useful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What specific policies would you implement to ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Invalid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Could Clinton investing in you have consequenc...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>Useful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T19:43:50.800707Z",
     "start_time": "2025-05-06T19:43:50.797073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate overall summary statistics\n",
    "summary = results_df['label'].value_counts().to_dict()\n",
    "\n",
    "# Build a readable summary string\n",
    "summary_text = \"\\n\".join([f\"{label}: {count} questions\" for label, count in summary.items()])\n",
    "summary_text = f\"**Critical Question Summary:**\\n\\n{summary_text}\"\n",
    "\n",
    "summary_text"
   ],
   "id": "4b428e765d6fd03f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Critical Question Summary:**\\n\\nInvalid: 91 questions\\nUnhelpful: 34 questions\\nUseful: 8 questions'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T19:43:53.159895Z",
     "start_time": "2025-05-06T19:43:53.154847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add a new column indicating whether the labels match\n",
    "results_df['match'] = results_df['label'] == results_df['label_shared_task']\n",
    "\n",
    "# Summary: count of matches and mismatches\n",
    "match_summary = results_df['match'].value_counts()\n",
    "match_percentage = results_df['match'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Combine both into a single DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Count\": match_summary,\n",
    "    \"Percentage\": match_percentage\n",
    "}).rename(index={True: \"Match\", False: \"Mismatch\"})\n",
    "\n",
    "# Display the result\n",
    "print(summary_df)"
   ],
   "id": "c201f1e25b5aace5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Count  Percentage\n",
      "match                      \n",
      "Mismatch     97   72.932331\n",
      "Match        36   27.067669\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
