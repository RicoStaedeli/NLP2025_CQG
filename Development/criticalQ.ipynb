{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CRITICAL QUESTION DEFINITION & EVALUATION using SPACY",
   "id": "4ec1c96a975d2fc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. schema: Cause to effect CTE",
   "id": "f99159a86754bddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:50:09.689446Z",
     "start_time": "2025-05-12T08:50:06.116721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('framenet_v17')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "def get_causal_verbs_from_framenet():\n",
    "    causal_frame_names = [\n",
    "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
    "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\"\n",
    "    ]\n",
    "\n",
    "    causal_verbs = set()\n",
    "    for frame_name in causal_frame_names:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:  # Only verbs\n",
    "                    causal_verbs.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame '{frame_name}': {e}\")\n",
    "    \n",
    "    return causal_verbs\n",
    "\n",
    "causal_verbs = get_causal_verbs_from_framenet()\n",
    "\n",
    "def detect_cause_to_effect(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    explanations = []\n",
    "    score = 0\n",
    "\n",
    "    has_condition = any(tok.dep_ == \"mark\" and tok.text.lower() in {\"if\", \"when\"} for tok in doc)\n",
    "    if has_condition:\n",
    "        explanations.append(\"✓ Conditional clause detected (e.g., 'if', 'when')\")\n",
    "        score += 2\n",
    "\n",
    "    has_advcl = any(tok.dep_ == \"advcl\" for tok in doc)\n",
    "    if has_advcl:\n",
    "        explanations.append(\"✓ Adverbial clause (likely effect clause) detected\")\n",
    "        score += 2\n",
    "\n",
    "    has_causal_verb_structure = False\n",
    "\n",
    "    for tok in doc:\n",
    "        if tok.lemma_ in causal_verbs and tok.pos_ == \"VERB\":\n",
    "            subj = any(child.dep_ == \"nsubj\" for child in tok.children)\n",
    "            obj = any(child.dep_ == \"dobj\" for child in tok.children)\n",
    "            prep = any(child.dep_ == \"prep\" for child in tok.children)\n",
    "            if subj or obj or prep:\n",
    "                has_causal_verb_structure = True\n",
    "                explanations.append(\n",
    "                    f\"✓ Verb '{tok.lemma_}' is listed in FrameNet under causal frames and appears with subject/object – \"\n",
    "                    f\"possible causality depending on context\"\n",
    "                )\n",
    "                score += 2\n",
    "                if subj: score += 1\n",
    "                if obj: score += 1\n",
    "                if prep: score += 1\n",
    "                break\n",
    "\n",
    "    is_causal = has_condition and has_advcl or has_causal_verb_structure\n",
    "\n",
    "    if not is_causal:\n",
    "        causal_phrases = [\"result in\", \"lead to\", \"cause\", \"because of\", \"due to\"]\n",
    "        if any(phrase in sentence.lower() for phrase in causal_phrases):\n",
    "            explanations.append(\"✓ Phrase pattern matches known cause-to-effect trigger\")\n",
    "            is_causal = True\n",
    "            score += 1\n",
    "\n",
    "    # Clamp max score to 10\n",
    "    score = min(score, 10)\n",
    "    return is_causal, explanations, score\n",
    "\n",
    "\n",
    "def get_causal_verbs_from_framenet():\n",
    "    # TODO: defined FrameNet frames related to causation -> to check!!\n",
    "    causal_frame_names = [\n",
    "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
    "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\"\n",
    "    ]\n",
    "\n",
    "    causal_verbs = set()\n",
    "\n",
    "    for frame_name in causal_frame_names:\n",
    "        frame = fn.frame_by_name(frame_name)\n",
    "        for lu in frame.lexUnit.values():\n",
    "            if '.v' in lu['name']:  # Only verbs\n",
    "                causal_verbs.add(lu['name'].split('.')[0])\n",
    "\n",
    "    return causal_verbs\n",
    "\n",
    "sentences = [\n",
    "    \"What would happen if taxes increased?\",                         # CTE\n",
    "    \"Could rising sea levels result in frequent flooding?\",          # CTE\n",
    "    \"Why do doctors recommend exercise?\",                             # no\n",
    "    \"Did the rain cause the game to be canceled?\",                   #CTE\n",
    "    \"What’s the reason for lowering interest rates?\",                # no\n",
    "    \"Is it sunny today or just bright outside?\",                     # no\n",
    "    \"What happens if students don’t study?\",                         # CTE\n",
    "    \"Why do you prefer chocolate over vanilla?\",                     # no\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    result, explanation, score = detect_cause_to_effect(sentence)\n",
    "    if score >= 7:\n",
    "        label = \"Strong CauseToEffect\"\n",
    "    elif score >= 4:\n",
    "        label = \"Weak/Partial CauseToEffect\"\n",
    "    else:\n",
    "        label = \"Not CauseToEffect\"\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"CTE: {score}/10 → Label: {label}\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "82e287019e0bc6f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     C:\\Users\\cedri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: What would happen if taxes increased?\n",
      "CTE: 7/10 → Label: Strong CauseToEffect\n",
      "   ✓ Conditional clause detected (e.g., 'if', 'when')\n",
      "   ✓ Adverbial clause (likely effect clause) detected\n",
      "   ✓ Verb 'increase' is listed in FrameNet under causal frames and appears with subject/object – possible causality depending on context\n",
      "\n",
      "Sentence: Could rising sea levels result in frequent flooding?\n",
      "CTE: 1/10 → Label: Not CauseToEffect\n",
      "   ✓ Phrase pattern matches known cause-to-effect trigger\n",
      "\n",
      "Sentence: Why do doctors recommend exercise?\n",
      "CTE: 0/10 → Label: Not CauseToEffect\n",
      "\n",
      "Sentence: Did the rain cause the game to be canceled?\n",
      "CTE: 3/10 → Label: Not CauseToEffect\n",
      "   ✓ Verb 'cause' is listed in FrameNet under causal frames and appears with subject/object – possible causality depending on context\n",
      "\n",
      "Sentence: What’s the reason for lowering interest rates?\n",
      "CTE: 3/10 → Label: Not CauseToEffect\n",
      "   ✓ Verb 'lower' is listed in FrameNet under causal frames and appears with subject/object – possible causality depending on context\n",
      "\n",
      "Sentence: Is it sunny today or just bright outside?\n",
      "CTE: 0/10 → Label: Not CauseToEffect\n",
      "\n",
      "Sentence: What happens if students don’t study?\n",
      "CTE: 4/10 → Label: Weak/Partial CauseToEffect\n",
      "   ✓ Conditional clause detected (e.g., 'if', 'when')\n",
      "   ✓ Adverbial clause (likely effect clause) detected\n",
      "\n",
      "Sentence: Why do you prefer chocolate over vanilla?\n",
      "CTE: 0/10 → Label: Not CauseToEffect\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SOCRATIC dataset filtering",
   "id": "8ebe22d6d3aa4b4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:57:31.777200Z",
     "start_time": "2025-05-12T08:52:49.245791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# filter relevant context length \n",
    "df = pd.read_csv(\"../Data/Raw/SocraticQ/train_chunk_I.csv\", names=[\"category\", \"context\", \"question\"])\n",
    "df[\"context_token_len\"] = df[\"context\"].apply(lambda text: len(nlp(text)))\n",
    "filtered_df = df[df[\"context_token_len\"] >= 25].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows after filtering: {len(filtered_df)}\")\n"
   ],
   "id": "3d39555bba99859e",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# filter relevant context length \u001B[39;00m\n\u001B[32m      5\u001B[39m df = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33m../Data/Raw/SocraticQ/train_chunk_I.csv\u001B[39m\u001B[33m\"\u001B[39m, names=[\u001B[33m\"\u001B[39m\u001B[33mcategory\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mcontext_token_len\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcontext\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m filtered_df = df[df[\u001B[33m\"\u001B[39m\u001B[33mcontext_token_len\u001B[39m\u001B[33m\"\u001B[39m] >= \u001B[32m25\u001B[39m].copy()\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTotal rows: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4789\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4790\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4791\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4796\u001B[39m     **kwargs,\n\u001B[32m   4797\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4798\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4799\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4800\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4915\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4916\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4917\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4919\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4920\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4921\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4922\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4924\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1424\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1426\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1427\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1501\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1502\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1503\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1504\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1505\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1506\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1507\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1508\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1509\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1512\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1513\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1514\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    918\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    919\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2972\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 6\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# filter relevant context length \u001B[39;00m\n\u001B[32m      5\u001B[39m df = pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33m../Data/Raw/SocraticQ/train_chunk_I.csv\u001B[39m\u001B[33m\"\u001B[39m, names=[\u001B[33m\"\u001B[39m\u001B[33mcategory\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m df[\u001B[33m\"\u001B[39m\u001B[33mcontext_token_len\u001B[39m\u001B[33m\"\u001B[39m] = df[\u001B[33m\"\u001B[39m\u001B[33mcontext\u001B[39m\u001B[33m\"\u001B[39m].apply(\u001B[38;5;28;01mlambda\u001B[39;00m text: \u001B[38;5;28mlen\u001B[39m(\u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[32m      7\u001B[39m filtered_df = df[df[\u001B[33m\"\u001B[39m\u001B[33mcontext_token_len\u001B[39m\u001B[33m\"\u001B[39m] >= \u001B[32m25\u001B[39m].copy()\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTotal rows: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\spacy\\language.py:1052\u001B[39m, in \u001B[36mLanguage.__call__\u001B[39m\u001B[34m(self, text, disable, component_cfg)\u001B[39m\n\u001B[32m   1050\u001B[39m     error_handler = proc.get_error_handler()\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1052\u001B[39m     doc = \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcomponent_cfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m   1053\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1054\u001B[39m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[32m   1055\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors.E109.format(name=name)) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001B[39m, in \u001B[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001B[39m, in \u001B[36mTok2Vec.predict\u001B[39m\u001B[34m(self, docs)\u001B[39m\n\u001B[32m    124\u001B[39m     width = \u001B[38;5;28mself\u001B[39m.model.get_dim(\u001B[33m\"\u001B[39m\u001B[33mnO\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m.model.ops.alloc((\u001B[32m0\u001B[39m, width)) \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m tokvecs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m tokvecs\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\model.py:334\u001B[39m, in \u001B[36mModel.predict\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) -> OutT:\n\u001B[32m    331\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[32m    332\u001B[39m \u001B[33;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[32m    333\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(model, X, is_train)\u001B[39m\n\u001B[32m     52\u001B[39m callbacks = []\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model.layers:\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     Y, inc_layer_grad = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m     callbacks.append(inc_layer_grad)\n\u001B[32m     56\u001B[39m     X = Y\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, X, is_train)\u001B[39m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) -> Tuple[OutT, Callable]:\n\u001B[32m    308\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(model, X, is_train)\u001B[39m\n\u001B[32m     52\u001B[39m callbacks = []\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model.layers:\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     Y, inc_layer_grad = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m     callbacks.append(inc_layer_grad)\n\u001B[32m     56\u001B[39m     X = Y\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, X, is_train)\u001B[39m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) -> Tuple[OutT, Callable]:\n\u001B[32m    308\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:36\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(model, Xseq, is_train)\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m     33\u001B[39m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[32m     34\u001B[39m ) -> Tuple[SeqT, Callable]:\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(Xseq, Ragged):\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], \u001B[43m_ragged_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(Xseq, Padded):\n\u001B[32m     38\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:91\u001B[39m, in \u001B[36m_ragged_forward\u001B[39m\u001B[34m(model, Xr, is_train)\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_ragged_forward\u001B[39m(\n\u001B[32m     88\u001B[39m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[32m     89\u001B[39m ) -> Tuple[Ragged, Callable]:\n\u001B[32m     90\u001B[39m     layer: Model[ArrayXd, ArrayXd] = model.layers[\u001B[32m0\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     Y, get_dX = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdataXd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mbackprop\u001B[39m(dYr: Ragged) -> Ragged:\n\u001B[32m     94\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m Ragged(get_dX(dYr.dataXd), dYr.lengths)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, X, is_train)\u001B[39m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) -> Tuple[OutT, Callable]:\n\u001B[32m    308\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(model, X, is_train)\u001B[39m\n\u001B[32m     52\u001B[39m callbacks = []\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model.layers:\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     Y, inc_layer_grad = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m     callbacks.append(inc_layer_grad)\n\u001B[32m     56\u001B[39m     X = Y\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, X, is_train)\u001B[39m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) -> Tuple[OutT, Callable]:\n\u001B[32m    308\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(model, X, is_train)\u001B[39m\n\u001B[32m     52\u001B[39m callbacks = []\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model.layers:\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m     Y, inc_layer_grad = \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m     callbacks.append(inc_layer_grad)\n\u001B[32m     56\u001B[39m     X = Y\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, X, is_train)\u001B[39m\n\u001B[32m    307\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) -> Tuple[OutT, Callable]:\n\u001B[32m    308\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\layernorm.py:24\u001B[39m, in \u001B[36mforward\u001B[39m\u001B[34m(model, X, is_train)\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(model: Model[InT, InT], X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) -> Tuple[InT, Callable]:\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m     N, mu, var = \u001B[43m_get_moments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mops\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m     Xhat = (X - mu) * var ** (-\u001B[32m1.0\u001B[39m / \u001B[32m2.0\u001B[39m)\n\u001B[32m     26\u001B[39m     Y, backprop_rescale = _begin_update_scale_shift(model, Xhat)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\thinc\\layers\\layernorm.py:74\u001B[39m, in \u001B[36m_get_moments\u001B[39m\u001B[34m(ops, X)\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_get_moments\u001B[39m(ops: Ops, X: Floats2d) -> Tuple[Floats2d, Floats2d, Floats2d]:\n\u001B[32m     73\u001B[39m     \u001B[38;5;66;03m# TODO: Do mean methods\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m     mu: Floats2d = \u001B[43mX\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     var: Floats2d = X.var(axis=\u001B[32m1\u001B[39m, keepdims=\u001B[38;5;28;01mTrue\u001B[39;00m) + \u001B[32m1e-08\u001B[39m\n\u001B[32m     76\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Floats2d, ops.asarray_f([X.shape[\u001B[32m1\u001B[39m]])), mu, var\n",
      "\u001B[36mFile \u001B[39m\u001B[32mG:\\Projects\\NLP2025_CQG\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:135\u001B[39m, in \u001B[36m_mean\u001B[39m\u001B[34m(a, axis, dtype, out, keepdims, where)\u001B[39m\n\u001B[32m    132\u001B[39m         dtype = mu.dtype(\u001B[33m'\u001B[39m\u001B[33mf4\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    133\u001B[39m         is_float16_result = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m135\u001B[39m ret = \u001B[43mumr_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, mu.ndarray):\n\u001B[32m    137\u001B[39m     ret = um.true_divide(\n\u001B[32m    138\u001B[39m             ret, rcount, out=ret, casting=\u001B[33m'\u001B[39m\u001B[33munsafe\u001B[39m\u001B[33m'\u001B[39m, subok=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df.head()",
   "id": "75c703340cef176a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- test CauseToEffect performance ",
   "id": "7764f21176d074af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:52:05.434010Z",
     "start_time": "2025-05-12T08:52:05.387036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def apply_cte_detection(question):\n",
    "    result, explanations, score = detect_cause_to_effect(question)\n",
    "    if score >= 7:\n",
    "        label = \"Strong CauseToEffect\"\n",
    "    elif score >= 4:\n",
    "        label = \"Weak/Partial CauseToEffect\"\n",
    "    else:\n",
    "        label = \"Not CauseToEffect\"\n",
    "\n",
    "    return pd.Series({\n",
    "        \"cte_score\": score,\n",
    "        \"cte_label\": label,\n",
    "        \"cte_explanations\": \" | \".join(explanations)  \n",
    "    })\n",
    "\n",
    "filtered_df[[\"cte_score\", \"cte_label\", \"cte_explanations\"]] = filtered_df[\"question\"].apply(apply_cte_detection)\n"
   ],
   "id": "737c8906f78d4ee8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m      8\u001B[39m         label = \u001B[33m\"\u001B[39m\u001B[33mNot CauseToEffect\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m pd.Series({\n\u001B[32m     11\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mcte_score\u001B[39m\u001B[33m\"\u001B[39m: score,\n\u001B[32m     12\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mcte_label\u001B[39m\u001B[33m\"\u001B[39m: label,\n\u001B[32m     13\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mcte_explanations\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m | \u001B[39m\u001B[33m\"\u001B[39m.join(explanations)  \n\u001B[32m     14\u001B[39m     })\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m filtered_df[[\u001B[33m\"\u001B[39m\u001B[33mcte_score\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcte_label\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcte_explanations\u001B[39m\u001B[33m\"\u001B[39m]] = \u001B[43mfiltered_df\u001B[49m[\u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m].apply(apply_cte_detection)\n",
      "\u001B[31mNameError\u001B[39m: name 'filtered_df' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df",
   "id": "a6103cb618533da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e8aa0b8fafd24413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df[\"cte_label\"].value_counts()",
   "id": "ab0a395d8fa9db9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_cte_df = filtered_df[filtered_df[\"cte_label\"] == \"Strong CauseToEffect\"].copy()\n",
    "strong_cte_df"
   ],
   "id": "5a5606b8d872251c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_cte_df = filtered_df[filtered_df[\"cte_label\"] == \"Weak/Partial CauseToEffect\"].copy()\n",
    "strong_cte_df"
   ],
   "id": "a6e20d738340a871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- LLM validation of CTE labels only on target rows (with a syntactically indication)",
   "id": "89214ed03ac51b58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_rows = filtered_df[\n",
    "    filtered_df[\"cte_label\"].isin([\"Weak/Partial CauseToEffect\", \"Strong CauseToEffect\"])\n",
    "].copy()\n"
   ],
   "id": "8cff0b7a4937e8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "\n",
    "def prompt_cte_judgment(context, question, score, label):\n",
    "    return f\"\"\"\n",
    "You are an expert in argumentation theory.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "A rule-based system has analyzed this question and determined:\n",
    "- CauseToEffect Score: {score}/10\n",
    "- Heuristic Label: {label}\n",
    "\n",
    "The CauseToEffect score reflects structural indicators (e.g., conditional clauses, causal verbs, prepositions).\n",
    "\n",
    "Does this question genuinely reflect a Cause-to-Effect relationship? For example, does it suggest that one event or condition leads to another?\n",
    "\n",
    "Use the following labels:\n",
    "- Confirmed CauseToEffect → Strong evidence of causal reasoning\n",
    "- Plausible CauseToEffect → Some indicators but less clear\n",
    "- Not CauseToEffect → No signs of causal structure\n",
    "- Needs Human Review → Ambiguous or depends heavily on context\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/falcon-rw-1b\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "\n",
    "def llm_evaluate_cte_open(context, question, score, label):  \n",
    "    prompt = prompt_cte_judgment(context, question, score, label)\n",
    "    response = llm(prompt, max_new_tokens=50, do_sample=False)\n",
    "    reply = response[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "    return reply\n"
   ],
   "id": "12eabe43f3387236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subset_df = target_rows.sample(n=200, random_state=42).copy()\n",
    "subset_df.shape"
   ],
   "id": "6b95c1d44c8ce6d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "subset_df = target_rows.sample(n=200, random_state=42).copy()\n",
    "subset_df[\"llm_cte_label\"] = subset_df.progress_apply(\n",
    "    lambda row: llm_evaluate_cte_open(row[\"context\"], row[\"question\"], row[\"cte_score\"], row[\"cte_label\"]),\n",
    "    axis=1\n",
    ")\n"
   ],
   "id": "39881fbbac40254b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "subset_df.to_excel(\"train_chunk_1_CTE.xlsx\", index=False)"
   ],
   "id": "f28c09c69dcfd0c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "subset_df",
   "id": "a2244ebd03c389d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. schema: Expert opinion",
   "id": "b242e6c66732715e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "nltk.download('framenet_v17')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ----------- Generalized FrameNet Loader -----------\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
    "    return terms\n",
    "\n",
    "# ----------- Extract FrameNet-based Verb Sets -----------\n",
    "\n",
    "expert_frames = [\n",
    "    \"Expertise\", \"Judgment_communication\", \"Opinion\",\n",
    "    \"Authority\", \"Statement\", \"Certainty\"\n",
    "]\n",
    "quote_frames = [\"Statement\", \"Judgment_communication\"]\n",
    "clarity_frames = [\"Reasoning\"]\n",
    "evidence_frames = [\"Evidence\", \"Certainty\", \"Causation\"]\n",
    "\n",
    "\n",
    "expert_verbs = get_lexical_units_from_frames(expert_frames)\n",
    "quote_verbs = get_lexical_units_from_frames(quote_frames)\n",
    "clarity_terms = get_lexical_units_from_frames(clarity_frames)\n",
    "evidence_terms = get_lexical_units_from_frames(evidence_frames)\n",
    "\n",
    "# ----------- Detection Function -----------\n",
    "\n",
    "def detect_expert_opinion(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "\n",
    "    expert_titles = {\"expert\", \"researcher\", \"scientist\", \"doctor\", \"analyst\", \"professor\", \"Dr.\"}\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
    "            if any(title in ent.text.lower() for title in expert_titles):\n",
    "                explanations.append(f\"✓ Expert entity detected: '{ent.text}'\")\n",
    "                score += 2\n",
    "                break\n",
    "\n",
    "    if any(tok.lemma_ in expert_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Detected expert-related verb from FrameNet\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.lemma_ in quote_verbs for tok in doc):\n",
    "        explanations.append(\"✓ Quotation or claim verb found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in clarity_terms for tok in doc):\n",
    "        explanations.append(\"✓ Clarity/definition markers found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in evidence_terms for tok in doc):\n",
    "        explanations.append(\"✓ Evidence or support-related terms found\")\n",
    "        score += 2\n",
    "\n",
    "    if score >= 6:\n",
    "        label = \"Strong Expert Opinion\"\n",
    "    elif score >= 3:\n",
    "        label = \"Weak/Partial Expert Opinion\"\n",
    "    else:\n",
    "        label = \"Not Expert Opinion\"\n",
    "\n",
    "    return label, score, explanations\n",
    "\n",
    "# ----------- Example Usage -----------\n",
    "\n",
    "questions = [\n",
    "    \"Did Professor Lee actually say that the results were inconclusive?\",\n",
    "    \"Is this study consistent with what other experts have found?\",\n",
    "    \"Why do cats sleep so much?\",\n",
    "    \"Are there any recent studies supporting this?\",\n",
    "    \"What did the government state about inflation?\",\n",
    "    \"Is the evidence consistent with past research?\",\n",
    "    \"Can you define the technical term 'quantum entanglement'?\",\n",
    "    \"Who won the World Cup in 2018?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    label, score, explanation = detect_expert_opinion(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Label: {label} | Score: {score}/10\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "af3d5c09b2e0b33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "filtered_df[[\"expert_label\", \"expert_score\", \"expert_explanations\"]] = filtered_df[\"question\"].apply(\n",
    "    lambda q: pd.Series(detect_expert_opinion(q))\n",
    ")\n"
   ],
   "id": "ada4a57bd0716b88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df[\"expert_label\"].value_counts()",
   "id": "a0744391efb940d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_expert_df = filtered_df[filtered_df[\"expert_label\"] == \"Strong Expert Opinion\"].copy()\n",
    "strong_expert_df"
   ],
   "id": "d349dfab6f37a89e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. schema: analogy",
   "id": "68efa2be1108fe5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Get synsets for analogy-related terms\n",
    "analogy_synsets = [wn.synset('similar.a.01'), wn.synset('analogy.n.01'), wn.synset('compare.v.01')]\n",
    "\n",
    "def is_semantically_analogical(token):\n",
    "    token_synsets = wn.synsets(token.lemma_)\n",
    "    for s in token_synsets:\n",
    "        for analogy_syn in analogy_synsets:\n",
    "            if s.path_similarity(analogy_syn) and s.path_similarity(analogy_syn) > 0.3:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
    "    return terms\n",
    "\n",
    "# FrameNet sets\n",
    "comparison_frames = [\"Similarity\"]\n",
    "contrast_frames = [\"Categorization\"]\n",
    "evidence_frames = [\"Evidence\", \"Judgment_communication\"]\n",
    "\n",
    "comparison_verbs = get_lexical_units_from_frames(comparison_frames)\n",
    "contrast_verbs = get_lexical_units_from_frames(contrast_frames)\n",
    "evidence_verbs = get_lexical_units_from_frames(evidence_frames)\n",
    "\n",
    "# Analogy detection function\n",
    "def detect_analogy_question(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "    noun_chunks = list(doc.noun_chunks)\n",
    "\n",
    "    # Comparison markers\n",
    "    if any(tok.lemma_ in comparison_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Comparison verb detected from FrameNet\")\n",
    "        score += 2\n",
    "    \n",
    "    #pairwise comprison\n",
    "    entity_tokens = [tok for tok in doc if tok.pos_ in {\"PROPN\", \"NOUN\"}]\n",
    "    if len(set([tok.lemma_ for tok in entity_tokens])) >= 2:\n",
    "        score += 1\n",
    "        explanations.append(\"✓ Contains at least two distinct concepts/entities\")\n",
    "\n",
    "    # Contrast markers\n",
    "    if any(tok.lemma_ in contrast_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Contrast or difference verb detected from FrameNet\")\n",
    "        score += 1\n",
    "\n",
    "    # Evidence markers\n",
    "    if any(tok.lemma_ in evidence_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Evidence or justification verb found\")\n",
    "        score += 1\n",
    "        \n",
    "    if any(tok.tag_ in {\"MD\"} for tok in doc):  # Modal verbs like would, could\n",
    "        score += 1\n",
    "\n",
    "    if len(noun_chunks) >= 2 and any(tok.lemma_ in {\"similar\", \"like\", \"as\"} for tok in doc):\n",
    "        explanations.append(\"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\")\n",
    "        score += 2\n",
    "\n",
    "\n",
    "    # Conditional marker (e.g., 'if')\n",
    "    if any(tok.text.lower() == \"if\" for tok in doc):\n",
    "        explanations.append(\"✓ Conditional structure suggesting hypothetical reasoning\")\n",
    "        score += 1\n",
    "        \n",
    "    if any(is_semantically_analogical(tok) for tok in doc if tok.pos_ in {\"ADJ\", \"NOUN\", \"VERB\"}):\n",
    "        explanations.append(\"✓ Semantic similarity to analogy-related terms detected via WordNet\")\n",
    "        score += 2\n",
    "        \n",
    "    if any(tok.dep_ in {\"prep\", \"relcl\"} and tok.lemma_ in {\"compare\", \"similar\"} for tok in doc):\n",
    "        score += 1\n",
    "        explanations.append(\"✓ Syntactic cue of analogy (e.g., 'compared with', 'similar to')\")\n",
    "\n",
    "\n",
    "\n",
    "    # Label assignment\n",
    "    if score >= 8:\n",
    "        label = \"Strong Analogy Question\"\n",
    "    elif score >= 5:\n",
    "        label = \"Weak/Partial Analogy Question\"\n",
    "    else:\n",
    "        label = \"Not Analogy Question\"\n",
    "\n",
    "    return label, score, explanations\n"
   ],
   "id": "9da728ccd68b5ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"Are cats and dogs similar in how they form social bonds?\",\n",
    "    \"Is democracy in the U.S. similar to that in ancient Greece?\",\n",
    "    \"If Finland succeeded with this policy, would it work in Germany?\",\n",
    "    \"What did Plato say about justice?\",\n",
    "    \"Are there differences between ancient Rome and the modern EU?\",\n",
    "    \"Who won the match yesterday?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    label, score, explanation = detect_analogy_question(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Label: {label} | Score: {score}/10\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "b39fdf9794c405c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv(\"Data/Raw/SocraticQ/train_chunk_I.csv\", names=[\"category\", \"context\", \"question\"])\n",
    "df[\"context_token_len\"] = df[\"context\"].apply(lambda text: len(nlp(text)))\n",
    "df_filtered = df[df[\"context_token_len\"] >= 25].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")"
   ],
   "id": "b1150d5999f5ba69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_filtered[[\"analogy_label\", \"analogy_score\", \"analogy_explanations\"]] = df_filtered[\"question\"].apply(\n",
    "    lambda q: pd.Series(detect_analogy_question(q))\n",
    ")"
   ],
   "id": "e2e12358bd38777c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered[\"analogy_label\"].value_counts()\n",
   "id": "763f670b615c6321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_analogy_df = df_filtered[df_filtered[\"analogy_label\"] == \"Strong Analogy Question\"].copy()\n",
    "strong_analogy_df"
   ],
   "id": "82116201782eb6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Bias \n",
   "id": "85bd378e21abec3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "# Setup\n",
    "nltk.download('framenet_v17')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_fear_related(token):\n",
    "    syns = wn.synsets(token.lemma_)\n",
    "    for s in syns:\n",
    "        if any(s.path_similarity(wn.synset('danger.n.01')) or\n",
    "               s.path_similarity(wn.synset('fear.n.01')) or\n",
    "               s.path_similarity(wn.synset('threat.n.01')) for s in syns):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ---- FrameNet Utility ----\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except:\n",
    "            continue\n",
    "    return terms\n",
    "\n",
    "# ---- Relevant Lexical Resources ----\n",
    "causal_frames = [\"Causation\", \"Cause_to_start\", \"Preventing\", \"Risk\", \"Threaten\", \"Danger\"]\n",
    "causal_verbs = get_lexical_units_from_frames(causal_frames)\n",
    "\n",
    "fear_keywords = {\"danger\", \"threat\", \"risky\", \"harm\", \"catastrophe\", \"crisis\", \"ruin\", \"fear\", \"worse\", \"bad\", \"fatal\"}\n",
    "preventive_keywords = {\"prevent\", \"avoid\", \"stop\", \"ban\", \"rescue\", \"save\", \"protect\"}\n",
    "modal_keywords = {\"might\", \"could\", \"would\", \"may\", \"should\"}\n",
    "\n",
    "# ---- Detection Function ----\n",
    "def detect_fear_appeal_question(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "\n",
    "    # Modal structure\n",
    "    if any(tok.lemma_ in modal_keywords for tok in doc if tok.tag_ == \"MD\"):\n",
    "        explanations.append(\"✓ Modal verb detected (e.g., 'might', 'would') suggesting hypothetical risk\")\n",
    "        score += 1\n",
    "\n",
    "    # Threat/danger terms\n",
    "    if any(tok.lemma_.lower() in fear_keywords for tok in doc):\n",
    "        explanations.append(\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\")\n",
    "        score += 2\n",
    "\n",
    "    # Prevention-related words\n",
    "    if any(tok.lemma_.lower() in preventive_keywords for tok in doc):\n",
    "        explanations.append(\"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\")\n",
    "        score += 2\n",
    "\n",
    "    # Causal verbs from FrameNet\n",
    "    if any(tok.lemma_ in causal_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Causal/preventive verb from FrameNet detected\")\n",
    "        score += 2\n",
    "\n",
    "    # Hypothetical or conditional reasoning\n",
    "    if any(tok.text.lower() in {\"if\", \"unless\"} for tok in doc):\n",
    "        explanations.append(\"✓ Conditional clause found (e.g., 'if', 'unless')\")\n",
    "        score += 1\n",
    "    \n",
    "    if any(is_fear_related(tok) for tok in doc if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"}):\n",
    "        explanations.append(\"✓ Semantic fear-related concept detected via WordNet\")\n",
    "        score += 2\n",
    "        \n",
    "    # Final label\n",
    "    if score >= 6:\n",
    "        label = \"Strong Fear Appeal\"\n",
    "        \n",
    "    elif score >= 4:\n",
    "        label = \"Weak/Partial Fear Appeal\"\n",
    "    else:\n",
    "        label = \"Not Fear Appeal\"\n",
    "\n",
    "    return label, score, explanations\n"
   ],
   "id": "8184c152fc34a316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"If we don't regulate AI, it might take over critical infrastructure.\",\n",
    "    \"Is banning TikTok the only way to prevent surveillance?\",\n",
    "    \"Could this policy save us from a financial disaster?\",\n",
    "    \"Why is inflation bad for the middle class?\",\n",
    "    \"Should we stop immigration to prevent job loss?\",\n",
    "    \"Who benefits from this healthcare policy?\",\n",
    "    \"What happens if we ignore climate change?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    label, score, explanation = detect_fear_appeal_question(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Label: {label} | Score: {score}/10\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "4bed2eae98d483f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_filtered[[\"fear_label\", \"fear_score\", \"fear_explanations\"]] = df_filtered[\"question\"].apply(\n",
    "    lambda q: pd.Series(detect_fear_appeal_question(q))\n",
    ")\n"
   ],
   "id": "70a23890a9e06c22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered.value_counts()",
   "id": "f1f2b94aa6bdf6c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_expert_df = df_filtered[df_filtered[\"fear_label\"] == \"Strong Fear Appeal\"].copy()\n",
    "strong_expert_df"
   ],
   "id": "6549d84081082231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered[\"fear_label\"].value_counts()",
   "id": "211656534fedde7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "9e03653e0d1061e8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
