{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CRITICAL QUESTION DEFINITION & EVALUATION using SPACY",
   "id": "4ec1c96a975d2fc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. schema: Cause to effect CTE",
   "id": "f99159a86754bddd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.download('framenet_v17')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "def get_causal_verbs_from_framenet():\n",
    "    causal_frame_names = [\n",
    "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
    "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\"\n",
    "    ]\n",
    "\n",
    "    causal_verbs = set()\n",
    "    for frame_name in causal_frame_names:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:  # Only verbs\n",
    "                    causal_verbs.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame '{frame_name}': {e}\")\n",
    "    \n",
    "    return causal_verbs\n",
    "\n",
    "causal_verbs = get_causal_verbs_from_framenet()\n",
    "\n",
    "def detect_cause_to_effect(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    explanations = []\n",
    "    score = 0\n",
    "\n",
    "    has_condition = any(tok.dep_ == \"mark\" and tok.text.lower() in {\"if\", \"when\"} for tok in doc)\n",
    "    if has_condition:\n",
    "        explanations.append(\"✓ Conditional clause detected (e.g., 'if', 'when')\")\n",
    "        score += 2\n",
    "\n",
    "    has_advcl = any(tok.dep_ == \"advcl\" for tok in doc)\n",
    "    if has_advcl:\n",
    "        explanations.append(\"✓ Adverbial clause (likely effect clause) detected\")\n",
    "        score += 2\n",
    "\n",
    "    has_causal_verb_structure = False\n",
    "\n",
    "    for tok in doc:\n",
    "        if tok.lemma_ in causal_verbs and tok.pos_ == \"VERB\":\n",
    "            subj = any(child.dep_ == \"nsubj\" for child in tok.children)\n",
    "            obj = any(child.dep_ == \"dobj\" for child in tok.children)\n",
    "            prep = any(child.dep_ == \"prep\" for child in tok.children)\n",
    "            if subj or obj or prep:\n",
    "                has_causal_verb_structure = True\n",
    "                explanations.append(\n",
    "                    f\"✓ Verb '{tok.lemma_}' is listed in FrameNet under causal frames and appears with subject/object – \"\n",
    "                    f\"possible causality depending on context\"\n",
    "                )\n",
    "                score += 2\n",
    "                if subj: score += 1\n",
    "                if obj: score += 1\n",
    "                if prep: score += 1\n",
    "                break\n",
    "\n",
    "    is_causal = has_condition and has_advcl or has_causal_verb_structure\n",
    "\n",
    "    if not is_causal:\n",
    "        causal_phrases = [\"result in\", \"lead to\", \"cause\", \"because of\", \"due to\"]\n",
    "        if any(phrase in sentence.lower() for phrase in causal_phrases):\n",
    "            explanations.append(\"✓ Phrase pattern matches known cause-to-effect trigger\")\n",
    "            is_causal = True\n",
    "            score += 1\n",
    "\n",
    "    # Clamp max score to 10\n",
    "    score = min(score, 10)\n",
    "    return is_causal, explanations, score\n",
    "\n",
    "\n",
    "def get_causal_verbs_from_framenet():\n",
    "    # TODO: defined FrameNet frames related to causation -> to check!! Muss man evtl noch eingrenzen\n",
    "    causal_frame_names = [\n",
    "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
    "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\"\n",
    "    ]\n",
    "\n",
    "    causal_verbs = set()\n",
    "\n",
    "    for frame_name in causal_frame_names:\n",
    "        frame = fn.frame_by_name(frame_name)\n",
    "        for lu in frame.lexUnit.values():\n",
    "            if '.v' in lu['name']:  # Only verbs\n",
    "                causal_verbs.add(lu['name'].split('.')[0])\n",
    "    \n",
    "    return causal_verbs\n",
    "\n",
    "sentences = [\n",
    "    \"What would happen if taxes increased?\",                         # CTE\n",
    "    \"Could rising sea levels result in frequent flooding?\",          # CTE\n",
    "    \"Why do doctors recommend exercise?\",                             # no\n",
    "    \"Did the rain cause the game to be canceled?\",                   #CTE\n",
    "    \"What’s the reason for lowering interest rates?\",                # no\n",
    "    \"Is it sunny today or just bright outside?\",                     # no\n",
    "    \"What happens if students don’t study?\",                         # CTE\n",
    "    \"Why do you prefer chocolate over vanilla?\",                     # no\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    result, explanation, score = detect_cause_to_effect(sentence)\n",
    "    if score >= 7:\n",
    "        label = \"Strong CauseToEffect\"\n",
    "    elif score >= 4:\n",
    "        label = \"Weak/Partial CauseToEffect\"\n",
    "    else:\n",
    "        label = \"Not CauseToEffect\"\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"CTE: {score}/10 → Label: {label}\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "82e287019e0bc6f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SOCRATIC dataset filtering",
   "id": "8ebe22d6d3aa4b4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# filter relevant context length \n",
    "df = pd.read_csv(\"Data/Raw/SocraticQ/train_chunk_I.csv\", names=[\"category\", \"context\", \"question\"])\n",
    "df[\"context_token_len\"] = df[\"context\"].apply(lambda text: len(nlp(text)))\n",
    "filtered_df = df[df[\"context_token_len\"] >= 25].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows after filtering: {len(filtered_df)}\")\n"
   ],
   "id": "3d39555bba99859e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df.head()",
   "id": "75c703340cef176a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- test CauseToEffect performance ",
   "id": "7764f21176d074af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def apply_cte_detection(question):\n",
    "    result, explanations, score = detect_cause_to_effect(question)\n",
    "    if score >= 7:\n",
    "        label = \"Strong CauseToEffect\"\n",
    "    elif score >= 4:\n",
    "        label = \"Weak/Partial CauseToEffect\"\n",
    "    else:\n",
    "        label = \"Not CauseToEffect\"\n",
    "\n",
    "    return pd.Series({\n",
    "        \"cte_score\": score,\n",
    "        \"cte_label\": label,\n",
    "        \"cte_explanations\": \" | \".join(explanations)  \n",
    "    })\n",
    "\n",
    "filtered_df[[\"cte_score\", \"cte_label\", \"cte_explanations\"]] = filtered_df[\"question\"].apply(apply_cte_detection)\n"
   ],
   "id": "737c8906f78d4ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df",
   "id": "a6103cb618533da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e8aa0b8fafd24413"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df[\"cte_label\"].value_counts()",
   "id": "ab0a395d8fa9db9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_cte_df = filtered_df[filtered_df[\"cte_label\"] == \"Strong CauseToEffect\"].copy()\n",
    "strong_cte_df"
   ],
   "id": "5a5606b8d872251c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_cte_df = filtered_df[filtered_df[\"cte_label\"] == \"Weak/Partial CauseToEffect\"].copy()\n",
    "strong_cte_df"
   ],
   "id": "a6e20d738340a871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- LLM validation of CTE labels only on target rows (with a syntactically indication)",
   "id": "89214ed03ac51b58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_rows = filtered_df[\n",
    "    filtered_df[\"cte_label\"].isin([\"Weak/Partial CauseToEffect\", \"Strong CauseToEffect\"])\n",
    "].copy()\n"
   ],
   "id": "8cff0b7a4937e8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "\n",
    "def prompt_cte_judgment(context, question, score, label):\n",
    "    return f\"\"\"\n",
    "You are an expert in argumentation theory.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "A rule-based system has analyzed this question and determined:\n",
    "- CauseToEffect Score: {score}/10\n",
    "- Heuristic Label: {label}\n",
    "\n",
    "The CauseToEffect score reflects structural indicators (e.g., conditional clauses, causal verbs, prepositions).\n",
    "\n",
    "Does this question genuinely reflect a Cause-to-Effect relationship? For example, does it suggest that one event or condition leads to another?\n",
    "\n",
    "Use the following labels:\n",
    "- Confirmed CauseToEffect → Strong evidence of causal reasoning\n",
    "- Plausible CauseToEffect → Some indicators but less clear\n",
    "- Not CauseToEffect → No signs of causal structure\n",
    "- Needs Human Review → Ambiguous or depends heavily on context\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/falcon-rw-1b\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "\n",
    "def llm_evaluate_cte_open(context, question, score, label):  \n",
    "    prompt = prompt_cte_judgment(context, question, score, label)\n",
    "    response = llm(prompt, max_new_tokens=50, do_sample=False)\n",
    "    reply = response[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "    return reply\n"
   ],
   "id": "12eabe43f3387236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subset_df = target_rows.sample(n=200, random_state=42).copy()\n",
    "subset_df.shape"
   ],
   "id": "6b95c1d44c8ce6d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "subset_df = target_rows.sample(n=200, random_state=42).copy()\n",
    "subset_df[\"llm_cte_label\"] = subset_df.progress_apply(\n",
    "    lambda row: llm_evaluate_cte_open(row[\"context\"], row[\"question\"], row[\"cte_score\"], row[\"cte_label\"]),\n",
    "    axis=1\n",
    ")\n"
   ],
   "id": "39881fbbac40254b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "subset_df.to_excel(\"train_chunk_1_CTE.xlsx\", index=False)"
   ],
   "id": "f28c09c69dcfd0c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "subset_df",
   "id": "a2244ebd03c389d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. schema: Expert opinion",
   "id": "b242e6c66732715e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "nltk.download('framenet_v17')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ----------- Generalized FrameNet Loader -----------\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
    "    return terms\n",
    "\n",
    "# ----------- Extract FrameNet-based Verb Sets -----------\n",
    "\n",
    "expert_frames = [\n",
    "    \"Expertise\", \"Judgment_communication\", \"Opinion\",\n",
    "    \"Authority\", \"Statement\", \"Certainty\"\n",
    "]\n",
    "quote_frames = [\"Statement\", \"Judgment_communication\"]\n",
    "clarity_frames = [\"Reasoning\"]\n",
    "evidence_frames = [\"Evidence\", \"Certainty\", \"Causation\"]\n",
    "\n",
    "\n",
    "expert_verbs = get_lexical_units_from_frames(expert_frames)\n",
    "quote_verbs = get_lexical_units_from_frames(quote_frames)\n",
    "clarity_terms = get_lexical_units_from_frames(clarity_frames)\n",
    "evidence_terms = get_lexical_units_from_frames(evidence_frames)\n",
    "\n",
    "# ----------- Detection Function -----------\n",
    "\n",
    "def detect_expert_opinion(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "\n",
    "    expert_titles = {\"expert\", \"researcher\", \"scientist\", \"doctor\", \"analyst\", \"professor\", \"Dr.\"}\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
    "            if any(title in ent.text.lower() for title in expert_titles):\n",
    "                explanations.append(f\"✓ Expert entity detected: '{ent.text}'\")\n",
    "                score += 2\n",
    "                break\n",
    "\n",
    "    if any(tok.lemma_ in expert_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Detected expert-related verb from FrameNet\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.lemma_ in quote_verbs for tok in doc):\n",
    "        explanations.append(\"✓ Quotation or claim verb found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in clarity_terms for tok in doc):\n",
    "        explanations.append(\"✓ Clarity/definition markers found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in evidence_terms for tok in doc):\n",
    "        explanations.append(\"✓ Evidence or support-related terms found\")\n",
    "        score += 2\n",
    "\n",
    "    if score >= 6:\n",
    "        label = \"Strong Expert Opinion\"\n",
    "    elif score >= 3:\n",
    "        label = \"Weak/Partial Expert Opinion\"\n",
    "    else:\n",
    "        label = \"Not Expert Opinion\"\n",
    "\n",
    "    return label, score, explanations\n",
    "\n",
    "# ----------- Example Usage -----------\n",
    "\n",
    "questions = [\n",
    "    \"Did Professor Lee actually say that the results were inconclusive?\",\n",
    "    \"Is this study consistent with what other experts have found?\",\n",
    "    \"Why do cats sleep so much?\",\n",
    "    \"Are there any recent studies supporting this?\",\n",
    "    \"What did the government state about inflation?\",\n",
    "    \"Is the evidence consistent with past research?\",\n",
    "    \"Can you define the technical term 'quantum entanglement'?\",\n",
    "    \"Who won the World Cup in 2018?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    label, score, explanation = detect_expert_opinion(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Label: {label} | Score: {score}/10\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "af3d5c09b2e0b33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "filtered_df[[\"expert_label\", \"expert_score\", \"expert_explanations\"]] = filtered_df[\"question\"].apply(\n",
    "    lambda q: pd.Series(detect_expert_opinion(q))\n",
    ")\n"
   ],
   "id": "ada4a57bd0716b88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_df[\"expert_label\"].value_counts()",
   "id": "a0744391efb940d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_expert_df = filtered_df[filtered_df[\"expert_label\"] == \"Strong Expert Opinion\"].copy()\n",
    "strong_expert_df"
   ],
   "id": "d349dfab6f37a89e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. schema: analogy",
   "id": "68efa2be1108fe5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Get synsets for analogy-related terms\n",
    "analogy_synsets = [wn.synset('similar.a.01'), wn.synset('analogy.n.01'), wn.synset('compare.v.01')]\n",
    "\n",
    "def is_semantically_analogical(token):\n",
    "    token_synsets = wn.synsets(token.lemma_)\n",
    "    for s in token_synsets:\n",
    "        for analogy_syn in analogy_synsets:\n",
    "            if s.path_similarity(analogy_syn) and s.path_similarity(analogy_syn) > 0.3:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
    "    return terms\n",
    "\n",
    "# FrameNet sets\n",
    "comparison_frames = [\"Similarity\"]\n",
    "contrast_frames = [\"Categorization\"]\n",
    "evidence_frames = [\"Evidence\", \"Judgment_communication\"]\n",
    "\n",
    "comparison_verbs = get_lexical_units_from_frames(comparison_frames)\n",
    "contrast_verbs = get_lexical_units_from_frames(contrast_frames)\n",
    "evidence_verbs = get_lexical_units_from_frames(evidence_frames)\n",
    "\n",
    "# Analogy detection function\n",
    "def detect_analogy_question(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "    noun_chunks = list(doc.noun_chunks)\n",
    "\n",
    "    # Comparison markers\n",
    "    if any(tok.lemma_ in comparison_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Comparison verb detected from FrameNet\")\n",
    "        score += 2\n",
    "    \n",
    "    #pairwise comprison\n",
    "    entity_tokens = [tok for tok in doc if tok.pos_ in {\"PROPN\", \"NOUN\"}]\n",
    "    if len(set([tok.lemma_ for tok in entity_tokens])) >= 2:\n",
    "        score += 1\n",
    "        explanations.append(\"✓ Contains at least two distinct concepts/entities\")\n",
    "\n",
    "    # Contrast markers\n",
    "    if any(tok.lemma_ in contrast_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Contrast or difference verb detected from FrameNet\")\n",
    "        score += 1\n",
    "\n",
    "    # Evidence markers\n",
    "    if any(tok.lemma_ in evidence_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Evidence or justification verb found\")\n",
    "        score += 1\n",
    "        \n",
    "    if any(tok.tag_ in {\"MD\"} for tok in doc):  # Modal verbs like would, could\n",
    "        score += 1\n",
    "\n",
    "    if len(noun_chunks) >= 2 and any(tok.lemma_ in {\"similar\", \"like\", \"as\"} for tok in doc):\n",
    "        explanations.append(\"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\")\n",
    "        score += 2\n",
    "\n",
    "\n",
    "    # Conditional marker (e.g., 'if')\n",
    "    if any(tok.text.lower() == \"if\" for tok in doc):\n",
    "        explanations.append(\"✓ Conditional structure suggesting hypothetical reasoning\")\n",
    "        score += 1\n",
    "        \n",
    "    if any(is_semantically_analogical(tok) for tok in doc if tok.pos_ in {\"ADJ\", \"NOUN\", \"VERB\"}):\n",
    "        explanations.append(\"✓ Semantic similarity to analogy-related terms detected via WordNet\")\n",
    "        score += 2\n",
    "        \n",
    "    if any(tok.dep_ in {\"prep\", \"relcl\"} and tok.lemma_ in {\"compare\", \"similar\"} for tok in doc):\n",
    "        score += 1\n",
    "        explanations.append(\"✓ Syntactic cue of analogy (e.g., 'compared with', 'similar to')\")\n",
    "\n",
    "\n",
    "\n",
    "    # Label assignment\n",
    "    if score >= 8:\n",
    "        label = \"Strong Analogy Question\"\n",
    "    elif score >= 5:\n",
    "        label = \"Weak/Partial Analogy Question\"\n",
    "    else:\n",
    "        label = \"Not Analogy Question\"\n",
    "\n",
    "    return label, score, explanations\n"
   ],
   "id": "9da728ccd68b5ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"Are cats and dogs similar in how they form social bonds?\",\n",
    "    \"Is democracy in the U.S. similar to that in ancient Greece?\",\n",
    "    \"If Finland succeeded with this policy, would it work in Germany?\",\n",
    "    \"What did Plato say about justice?\",\n",
    "    \"Are there differences between ancient Rome and the modern EU?\",\n",
    "    \"Who won the match yesterday?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    label, score, explanation = detect_analogy_question(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Label: {label} | Score: {score}/10\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "b39fdf9794c405c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "df = pd.read_csv(\"Data/Raw/SocraticQ/train_chunk_I.csv\", names=[\"category\", \"context\", \"question\"])\n",
    "df[\"context_token_len\"] = df[\"context\"].apply(lambda text: len(nlp(text)))\n",
    "df_filtered = df[df[\"context_token_len\"] >= 25].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")"
   ],
   "id": "b1150d5999f5ba69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_filtered[[\"analogy_label\", \"analogy_score\", \"analogy_explanations\"]] = df_filtered[\"question\"].apply(\n",
    "    lambda q: pd.Series(detect_analogy_question(q))\n",
    ")"
   ],
   "id": "e2e12358bd38777c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered[\"analogy_label\"].value_counts()\n",
   "id": "763f670b615c6321",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_analogy_df = df_filtered[df_filtered[\"analogy_label\"] == \"Strong Analogy Question\"].copy()\n",
    "strong_analogy_df"
   ],
   "id": "82116201782eb6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Bias \n",
   "id": "85bd378e21abec3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "\n",
    "# Setup\n",
    "nltk.download('framenet_v17')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_fear_related(token):\n",
    "    syns = wn.synsets(token.lemma_)\n",
    "    for s in syns:\n",
    "        if any(s.path_similarity(wn.synset('danger.n.01')) or\n",
    "               s.path_similarity(wn.synset('fear.n.01')) or\n",
    "               s.path_similarity(wn.synset('threat.n.01')) for s in syns):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ---- FrameNet Utility ----\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except:\n",
    "            continue\n",
    "    return terms\n",
    "\n",
    "# ---- Relevant Lexical Resources ----\n",
    "causal_frames = [\"Causation\", \"Cause_to_start\", \"Preventing\", \"Risk\", \"Threaten\", \"Danger\"]\n",
    "causal_verbs = get_lexical_units_from_frames(causal_frames)\n",
    "\n",
    "fear_keywords = {\"danger\", \"threat\", \"risky\", \"harm\", \"catastrophe\", \"crisis\", \"ruin\", \"fear\", \"worse\", \"bad\", \"fatal\"}\n",
    "preventive_keywords = {\"prevent\", \"avoid\", \"stop\", \"ban\", \"rescue\", \"save\", \"protect\"}\n",
    "modal_keywords = {\"might\", \"could\", \"would\", \"may\", \"should\"}\n",
    "\n",
    "# ---- Detection Function ----\n",
    "def detect_fear_appeal_question(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "\n",
    "    # Modal structure\n",
    "    if any(tok.lemma_ in modal_keywords for tok in doc if tok.tag_ == \"MD\"):\n",
    "        explanations.append(\"✓ Modal verb detected (e.g., 'might', 'would') suggesting hypothetical risk\")\n",
    "        score += 1\n",
    "\n",
    "    # Threat/danger terms\n",
    "    if any(tok.lemma_.lower() in fear_keywords for tok in doc):\n",
    "        explanations.append(\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\")\n",
    "        score += 2\n",
    "\n",
    "    # Prevention-related words\n",
    "    if any(tok.lemma_.lower() in preventive_keywords for tok in doc):\n",
    "        explanations.append(\"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\")\n",
    "        score += 2\n",
    "\n",
    "    # Causal verbs from FrameNet\n",
    "    if any(tok.lemma_ in causal_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Causal/preventive verb from FrameNet detected\")\n",
    "        score += 2\n",
    "\n",
    "    # Hypothetical or conditional reasoning\n",
    "    if any(tok.text.lower() in {\"if\", \"unless\"} for tok in doc):\n",
    "        explanations.append(\"✓ Conditional clause found (e.g., 'if', 'unless')\")\n",
    "        score += 1\n",
    "    \n",
    "    if any(is_fear_related(tok) for tok in doc if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"}):\n",
    "        explanations.append(\"✓ Semantic fear-related concept detected via WordNet\")\n",
    "        score += 2\n",
    "        \n",
    "    # Final label\n",
    "    if score >= 6:\n",
    "        label = \"Strong Fear Appeal\"\n",
    "        \n",
    "    elif score >= 4:\n",
    "        label = \"Weak/Partial Fear Appeal\"\n",
    "    else:\n",
    "        label = \"Not Fear Appeal\"\n",
    "\n",
    "    return label, score, explanations\n"
   ],
   "id": "8184c152fc34a316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "questions = [\n",
    "    \"If we don't regulate AI, it might take over critical infrastructure.\",\n",
    "    \"Is banning TikTok the only way to prevent surveillance?\",\n",
    "    \"Could this policy save us from a financial disaster?\",\n",
    "    \"Why is inflation bad for the middle class?\",\n",
    "    \"Should we stop immigration to prevent job loss?\",\n",
    "    \"Who benefits from this healthcare policy?\",\n",
    "    \"What happens if we ignore climate change?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    label, score, explanation = detect_fear_appeal_question(q)\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    print(f\"Label: {label} | Score: {score}/10\")\n",
    "    for e in explanation:\n",
    "        print(f\"   {e}\")\n"
   ],
   "id": "4bed2eae98d483f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_filtered[[\"fear_label\", \"fear_score\", \"fear_explanations\"]] = df_filtered[\"question\"].apply(\n",
    "    lambda q: pd.Series(detect_fear_appeal_question(q))\n",
    ")\n"
   ],
   "id": "70a23890a9e06c22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered.value_counts()",
   "id": "f1f2b94aa6bdf6c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "strong_expert_df = df_filtered[df_filtered[\"fear_label\"] == \"Strong Fear Appeal\"].copy()\n",
    "strong_expert_df"
   ],
   "id": "6549d84081082231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_filtered[\"fear_label\"].value_counts()",
   "id": "211656534fedde7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "9e03653e0d1061e8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
