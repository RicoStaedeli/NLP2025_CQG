{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_3_DPO_from_SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# Direct Preference Optimization for Critical Question Generation\n",
        "In this notebook we train the pretrained vanilla LLM with DPO Training.\n",
        "- **Model:** [ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2](https://huggingface.co/ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT_2)\n",
        "- **Dataset:** [processed_train_data_filtered_dpo.json](../Data/Processed/processed_train_data_filtered_dpo.json)\n",
        "- **Frameworks:** Unsloth, HuggingFace, transformers, bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install --no-deps xformers triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer\n",
        "import logging\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, IntervalStrategy, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "8m2YPqS1_8sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756b176b-d9ef-4ce1-8e23-886b10a6c449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df71a651-757e-45bc-81a1-906710cbdd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 1394, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 1394 (delta 184), reused 126 (delta 92), pack-reused 1136 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1394/1394), 49.82 MiB | 25.29 MiB/s, done.\n",
            "Resolving deltas: 100% (786/786), done.\n",
            "Updating files: 100% (116/116), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 3\n",
        "BASE_MODEL_REPO = \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_SFT_DPO\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/processed_train_data_filtered_dpo.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4b9d9c-ad9d-439b-acb9-2f61c69df0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--------  Start with Training  -------------\n",
            "INFO:__main__:Device selected: cuda\n",
            "INFO:__main__:Model: Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_1\n",
            "INFO:__main__:Training number: 7\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bdf526-fdce-4217-fdba-334f7077c204"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "ZSW8diIbAtk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3dc5a7-fc5c-47dc-ea78-cf4bcd435475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected', 'schema', 'id', 'context'],\n",
            "        num_rows: 3128\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Znebp07vA1Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Tokenizer"
      ],
      "metadata": {
        "id": "lLLX61krUNlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import CHAT_TEMPLATES\n",
        "print(list(CHAT_TEMPLATES.keys()))\n"
      ],
      "metadata": {
        "id": "eftsUd4XRz2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9539bbf-862a-471b-ff04-70d3ffc52a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unsloth', 'zephyr', 'chatml', 'mistral', 'llama', 'vicuna', 'vicuna_old', 'vicuna old', 'alpaca', 'gemma', 'gemma_chatml', 'gemma2', 'gemma2_chatml', 'llama-3', 'llama3', 'phi-3', 'phi-35', 'phi-3.5', 'llama-3.1', 'llama-31', 'llama-3.2', 'llama-3.3', 'llama-32', 'llama-33', 'qwen-2.5', 'qwen-25', 'qwen25', 'qwen2.5', 'phi-4', 'gemma-3', 'gemma3', 'qwen-3', 'qwen3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ],
      "metadata": {
        "id": "0wk97cwFSrMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "hZvVvX0yUSjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "SfVoyCIsbVzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a456db-92d0-4349-e6a6-e5bc1ed2729a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': [{'content': \"Generate one critical question addressing the provided context. Ensure it matches the schema: FearAppeal\\n\\nContext: implication_consequences: The argument isn't that school teachers' compensation is adequate. The argument is that everyone should be paid a living wage. Someone asked how much a living wage was, and OP responded with an estimation of 40-50k.\", 'role': 'user'}], 'chosen': [{'content': 'How much would a big Mac be if every employee made 50k a year?', 'role': 'assistant'}], 'rejected': [{'content': 'Is paying school teachers a living wage a way to ensure their well-being and job satisfaction?', 'role': 'assistant'}], 'schema': 'FearAppeal', 'id': 0, 'context': \"implication_consequences: The argument isn't that school teachers' compensation is adequate. The argument is that everyone should be paid a living wage. Someone asked how much a living wage was, and OP responded with an estimation of 40-50k.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = list(dataset[\"train\"].features)\n",
        "def apply_dpo_template(example):\n",
        "  if all(k in example.keys() for k in (\"chosen\", \"rejected\",\"prompt\")):\n",
        "\n",
        "    # For DPO, the inputs are triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue\n",
        "    prompt_messages = example[\"prompt\"]\n",
        "    chosen_messages = example[\"chosen\"]\n",
        "    rejected_messages = example[\"rejected\"]\n",
        "\n",
        "    example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "    example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "    example[\"text_prompt\"] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
        "  return example\n",
        "\n",
        "dataset = dataset.map(apply_dpo_template,remove_columns=column_names,\n",
        "          desc=\"Formatting comparisons with prompt template\",)"
      ],
      "metadata": {
        "id": "QvxvwyC7A4fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "IscEC6bXTExX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1dea62-cc6d-4402-f9e7-2558f42947f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text_chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHow much would a big Mac be if every employee made 50k a year?<|eot_id|>', 'text_rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIs paying school teachers a living wage a way to ensure their well-being and job satisfaction?<|eot_id|>', 'text_prompt': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate one critical question addressing the provided context. Ensure it matches the schema: FearAppeal\\n\\nContext: implication_consequences: The argument isn't that school teachers' compensation is adequate. The argument is that everyone should be paid a living wage. Someone asked how much a living wage was, and OP responded with an estimation of 40-50k.<|eot_id|>\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "7OQsby8nQQlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c3b836-bb48-48f8-d4fe-8cc98f8fd1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text_chosen', 'text_rejected', 'text_prompt'],\n",
            "        num_rows: 3128\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n"
      ],
      "metadata": {
        "id": "nW6J16wib7vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in [\"train\", \"test\"]:\n",
        "    dataset[split] = dataset[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ],
      "metadata": {
        "id": "meuPpzMOckLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "lgWWcVPGcn2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5b454c-aeec-46b1-8b1b-d7d576b4f66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['chosen', 'rejected', 'prompt'],\n",
            "        num_rows: 2502\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['chosen', 'rejected', 'prompt'],\n",
            "        num_rows: 626\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training variables"
      ],
      "metadata": {
        "id": "IlDjbBm7UYJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOConfig\n",
        "training_args = DPOConfig(\n",
        "        do_eval=True,\n",
        "        eval_strategy = \"steps\",\n",
        "        save_strategy = \"steps\",\n",
        "        eval_steps = 5,\n",
        "        logging_steps = 1,\n",
        "        max_steps = 40,\n",
        "        warmup_ratio = 0.1,\n",
        "        per_device_train_batch_size = 20,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        per_device_eval_batch_size = 1,\n",
        "        learning_rate = 5e-6,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        save_total_limit = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.0,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = checkpoint_dir,\n",
        "        report_to = \"tensorboard\",\n",
        "        logging_dir = log_base_path\n",
        ")\n",
        "\n",
        "\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length = 1024,\n",
        "    max_prompt_length = 512\n",
        ")\n"
      ],
      "metadata": {
        "id": "lOCskNdmQRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254909c8-daef-4043-aa09-a7eef39f816b"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:GPU  Information before Training\n",
            "INFO:__main__:GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "INFO:__main__:7.625 GB of memory reserved.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ba729d83-7785-446d-f01b-1435e5444e36"
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,502 | Num Epochs = 2 | Total steps = 40\n",
            "O^O/ \\_/ \\    Batch size per device = 20 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (20 x 4 x 1) = 80\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 33:21, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.686300</td>\n",
              "      <td>0.691160</td>\n",
              "      <td>0.012830</td>\n",
              "      <td>0.008460</td>\n",
              "      <td>0.557508</td>\n",
              "      <td>0.004370</td>\n",
              "      <td>-184.449951</td>\n",
              "      <td>-167.941071</td>\n",
              "      <td>-0.521420</td>\n",
              "      <td>-0.539907</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.687760</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.005688</td>\n",
              "      <td>0.610224</td>\n",
              "      <td>0.011244</td>\n",
              "      <td>-184.522690</td>\n",
              "      <td>-168.082550</td>\n",
              "      <td>-0.523199</td>\n",
              "      <td>-0.541632</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.637400</td>\n",
              "      <td>0.686333</td>\n",
              "      <td>0.008858</td>\n",
              "      <td>-0.005255</td>\n",
              "      <td>0.635783</td>\n",
              "      <td>0.014113</td>\n",
              "      <td>-184.489670</td>\n",
              "      <td>-168.078232</td>\n",
              "      <td>-0.519168</td>\n",
              "      <td>-0.537939</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.597600</td>\n",
              "      <td>0.683974</td>\n",
              "      <td>0.011710</td>\n",
              "      <td>-0.007240</td>\n",
              "      <td>0.666134</td>\n",
              "      <td>0.018950</td>\n",
              "      <td>-184.461151</td>\n",
              "      <td>-168.098083</td>\n",
              "      <td>-0.522574</td>\n",
              "      <td>-0.541235</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.589500</td>\n",
              "      <td>0.682351</td>\n",
              "      <td>0.013072</td>\n",
              "      <td>-0.009267</td>\n",
              "      <td>0.682109</td>\n",
              "      <td>0.022339</td>\n",
              "      <td>-184.447540</td>\n",
              "      <td>-168.118362</td>\n",
              "      <td>-0.519646</td>\n",
              "      <td>-0.538463</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.677976</td>\n",
              "      <td>0.007736</td>\n",
              "      <td>-0.023678</td>\n",
              "      <td>0.757188</td>\n",
              "      <td>0.031413</td>\n",
              "      <td>-184.500900</td>\n",
              "      <td>-168.262451</td>\n",
              "      <td>-0.521158</td>\n",
              "      <td>-0.540285</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.519000</td>\n",
              "      <td>0.680307</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>-0.023127</td>\n",
              "      <td>0.675719</td>\n",
              "      <td>0.026865</td>\n",
              "      <td>-184.540863</td>\n",
              "      <td>-168.256958</td>\n",
              "      <td>-0.522371</td>\n",
              "      <td>-0.541318</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>0.678061</td>\n",
              "      <td>0.013334</td>\n",
              "      <td>-0.018016</td>\n",
              "      <td>0.723642</td>\n",
              "      <td>0.031350</td>\n",
              "      <td>-184.444901</td>\n",
              "      <td>-168.205841</td>\n",
              "      <td>-0.521086</td>\n",
              "      <td>-0.540132</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d7e791-ebce-45f0-a7ac-39a5d3b4ad91"
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Saved LoRA adapters to /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_7/Model/Meta-Llama-3.1-8B-Instruct_SFT_2_DPO_1_lora_adapters/\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Push Lora weights to HF\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}-lora\", tokenizer, save_method = \"lora\", token = token)"
      ],
      "metadata": {
        "id": "7KDrL2FmPi11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit and save local\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "\n",
        "# Merge to 16bit and push to HF\n",
        "if True:\n",
        "  token = userdata.get('HF_TOKEN')\n",
        "  model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"$log_dir\""
      ],
      "metadata": {
        "id": "lFwHGRxBZ8q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "IEtvPF-5pFXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico St√§deli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}