{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_2_DPO_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# Direct Preference Optimization for Critical Question Generation\n",
        "In this notebook we train the pretrained vanilla LLM with DPO Training.\n",
        "- **Model:** [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
        "- **Dataset:** [processed_train_data_filtered_dpo.json](../Data/Processed/CQ%20DPO%20Dataset.json)\n",
        "- **Frameworks:** Unsloth, HuggingFace, transformers, bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install --no-deps xformers triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer\n",
        "import logging\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, IntervalStrategy, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "8m2YPqS1_8sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d773d0-27b5-4ef7-851a-51edc892d40b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9017dd-0d79-4372-b71f-42446975eec3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 1603, done.\u001b[K\n",
            "remote: Counting objects: 100% (251/251), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 1603 (delta 186), reused 143 (delta 116), pack-reused 1352 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1603/1603), 74.54 MiB | 15.76 MiB/s, done.\n",
            "Resolving deltas: 100% (916/916), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 4\n",
        "BASE_MODEL_REPO = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_DPO_1\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ DPO Dataset.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9095376f-2147-4f1b-c568-24a2bc72ed58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--------  Start with Training  -------------\n",
            "INFO:__main__:Device selected: cuda\n",
            "INFO:__main__:Model: Meta-Llama-3.1-8B-Instruct_DPO_1\n",
            "INFO:__main__:Training number: 4\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 64 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 64\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b215671-bede-4332-9d41-41f63208d6cd"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load your original dataset\n",
        "with open(train_dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "formatted_data = []\n",
        "\n",
        "# Process each entry\n",
        "for entry in raw_data:\n",
        "    # Ensure score fields exist and are numeric\n",
        "    if not (\"score_chosen\" in entry and \"score_rejected\" in entry):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        score_chosen = float(entry[\"score_chosen\"])\n",
        "        score_rejected = float(entry[\"score_rejected\"])\n",
        "    except (TypeError, ValueError):\n",
        "        continue\n",
        "\n",
        "    # Filter by score difference\n",
        "    if abs(score_chosen - score_rejected) <= 4:\n",
        "        continue\n",
        "\n",
        "    # Build prompt\n",
        "    messages = entry[\"prompt\"]\n",
        "    prompt_parts = []\n",
        "\n",
        "    for message in messages:\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"].strip()\n",
        "        if role == \"user\":\n",
        "            prompt_parts.append(f\"User: {content}\")\n",
        "        elif role == \"assistant\":\n",
        "            prompt_parts.append(f\"Assistant: {content}\")\n",
        "\n",
        "    full_prompt = \"\\n\\n\".join(prompt_parts) + \"\\n\\nAssistant:\"\n",
        "\n",
        "    # Extract chosen and rejected responses (assumed to be lists)\n",
        "    try:\n",
        "        chosen_response = entry[\"chosen\"][0][\"content\"].strip()\n",
        "        rejected_response = entry[\"rejected\"][0][\"content\"].strip()\n",
        "    except (IndexError, KeyError, TypeError):\n",
        "        continue\n",
        "\n",
        "    formatted_data.append({\n",
        "        \"prompt\": full_prompt,\n",
        "        \"chosen\": chosen_response,\n",
        "        \"rejected\": rejected_response\n",
        "    })\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "dataset.save_to_disk(\"cq_dpo_dataset_filtered\")"
      ],
      "metadata": {
        "id": "DOb5pY07iump"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "dataset = load_from_disk(\"cq_dpo_dataset_filtered\")"
      ],
      "metadata": {
        "id": "PUKsLBVSkaoV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "ZSW8diIbAtk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9f7e91-1342-4d8d-f9d3-3a47db91e64c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['prompt', 'chosen', 'rejected'],\n",
            "    num_rows: 1572\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0]['rejected'])"
      ],
      "metadata": {
        "id": "VTNi_OIpkgpT",
        "outputId": "4f2b2573-e190-48bf-e62a-bcfc689329d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is considering climate change a political issue detrimental to the billions of people who will be greatly affected by it, as well as future generations?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n"
      ],
      "metadata": {
        "id": "nW6J16wib7vb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "lgWWcVPGcn2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7b7f75-4dd3-4c7a-e50f-586ce321f4f4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected'],\n",
            "        num_rows: 1257\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected'],\n",
            "        num_rows: 315\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training variables"
      ],
      "metadata": {
        "id": "IlDjbBm7UYJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOConfig\n",
        "training_args = DPOConfig(\n",
        "        do_eval=True,\n",
        "        eval_strategy = \"steps\",\n",
        "        save_strategy = \"steps\",\n",
        "        eval_steps = 5,\n",
        "        logging_steps = 1,\n",
        "        max_steps = 40,\n",
        "        warmup_ratio = 0.1,\n",
        "        per_device_train_batch_size = 20,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        per_device_eval_batch_size = 1,\n",
        "        learning_rate = 5e-6,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        save_total_limit = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.0,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = checkpoint_dir,\n",
        "        report_to = \"tensorboard\",\n",
        "        logging_dir = log_base_path\n",
        ")\n",
        "\n",
        "\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    beta=0.5,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length = 1024,\n",
        "    max_prompt_length = 512\n",
        ")\n"
      ],
      "metadata": {
        "id": "lOCskNdmQRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9468a18c-c5ee-4aa3-cea4-83d5df6a2b6d"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:GPU  Information before Training\n",
            "INFO:__main__:GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "INFO:__main__:7.623 GB of memory reserved.\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "a995186f-db2a-4935-d6d2-29e195cc4811"
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,257 | Num Epochs = 3 | Total steps = 40\n",
            "O^O/ \\_/ \\    Batch size per device = 20 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (20 x 4 x 1) = 80\n",
            " \"-____-\"     Trainable parameters = 167,772,160/8,000,000,000 (2.10% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 23:49, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.676000</td>\n",
              "      <td>0.692742</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>-0.000897</td>\n",
              "      <td>0.520635</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>-80.654610</td>\n",
              "      <td>-58.193073</td>\n",
              "      <td>-1.237724</td>\n",
              "      <td>-1.025032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.497100</td>\n",
              "      <td>0.691041</td>\n",
              "      <td>-0.005750</td>\n",
              "      <td>-0.010552</td>\n",
              "      <td>0.526984</td>\n",
              "      <td>0.004802</td>\n",
              "      <td>-80.714127</td>\n",
              "      <td>-58.289635</td>\n",
              "      <td>-1.237507</td>\n",
              "      <td>-1.024160</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.309900</td>\n",
              "      <td>0.688899</td>\n",
              "      <td>-0.010854</td>\n",
              "      <td>-0.021021</td>\n",
              "      <td>0.498413</td>\n",
              "      <td>0.010167</td>\n",
              "      <td>-80.765167</td>\n",
              "      <td>-58.394321</td>\n",
              "      <td>-1.235197</td>\n",
              "      <td>-1.022040</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.684706</td>\n",
              "      <td>-0.014775</td>\n",
              "      <td>-0.035333</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.020558</td>\n",
              "      <td>-80.804375</td>\n",
              "      <td>-58.537437</td>\n",
              "      <td>-1.233967</td>\n",
              "      <td>-1.021422</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.121400</td>\n",
              "      <td>0.684612</td>\n",
              "      <td>-0.023641</td>\n",
              "      <td>-0.046536</td>\n",
              "      <td>0.495238</td>\n",
              "      <td>0.022895</td>\n",
              "      <td>-80.893044</td>\n",
              "      <td>-58.649471</td>\n",
              "      <td>-1.233927</td>\n",
              "      <td>-1.021017</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.115300</td>\n",
              "      <td>0.682420</td>\n",
              "      <td>-0.025046</td>\n",
              "      <td>-0.054610</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.029564</td>\n",
              "      <td>-80.907082</td>\n",
              "      <td>-58.730209</td>\n",
              "      <td>-1.233941</td>\n",
              "      <td>-1.021265</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.116700</td>\n",
              "      <td>0.680431</td>\n",
              "      <td>-0.030331</td>\n",
              "      <td>-0.065643</td>\n",
              "      <td>0.558730</td>\n",
              "      <td>0.035312</td>\n",
              "      <td>-80.959930</td>\n",
              "      <td>-58.840538</td>\n",
              "      <td>-1.234215</td>\n",
              "      <td>-1.020993</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.053700</td>\n",
              "      <td>0.678499</td>\n",
              "      <td>-0.030139</td>\n",
              "      <td>-0.069782</td>\n",
              "      <td>0.590476</td>\n",
              "      <td>0.039642</td>\n",
              "      <td>-80.958023</td>\n",
              "      <td>-58.881927</td>\n",
              "      <td>-1.235110</td>\n",
              "      <td>-1.022020</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "outputId": "ae61fa92-cbbb-4a88-c7f7-f67c39a3b606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:1470.9937 seconds used for training.\n",
            "INFO:__main__:24.52 minutes used for training.\n",
            "INFO:__main__:Peak reserved memory = 38.639 GB.\n",
            "INFO:__main__:Peak reserved memory for training = 31.016 GB.\n",
            "INFO:__main__:Peak reserved memory % of max memory = 97.679 %.\n",
            "INFO:__main__:Peak reserved memory for training % of max memory = 78.408 %.\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aaa2694-d716-457c-cb3b-a16885c6703d"
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Saved LoRA adapters to /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_4/Model/Meta-Llama-3.1-8B-Instruct_DPO_1_lora_adapters/\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Push Lora weights to HF\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}-lora\", tokenizer, save_method = \"lora\", token = token)"
      ],
      "metadata": {
        "id": "7KDrL2FmPi11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit and save local\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "\n",
        "# Merge to 16bit and push to HF\n",
        "if True:\n",
        "  token = userdata.get('HF_TOKEN')\n",
        "  model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"$log_dir\""
      ],
      "metadata": {
        "id": "lFwHGRxBZ8q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "IEtvPF-5pFXp",
        "outputId": "cef12128-79a9-454c-a347-fc13e00ed56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_a_Generate_DPO_Dataset.ipynb\t      Data\n",
            "1_Information_preprocessing.md\t      Development\n",
            "1_Preprocessing_faster.ipynb\t      Doc\n",
            "1_Preprocessing.ipynb\t\t      Evaluation\n",
            "2_Baseline_Generation.ipynb\t      INFORMATION.md\n",
            "2_Information_Baseline_Generation.md  LICENSE\n",
            "3_Evaluation.ipynb\t\t      Logs\n",
            "4_Finetuned_Generation.ipynb\t      README.md\n",
            "5_Evaluation_Analytics.ipynb\t      Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}