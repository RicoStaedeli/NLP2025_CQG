{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_2_DPO_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# Direct Preference Optimization for Critical Question Generation\n",
        "In this notebook we train the pretrained vanilla LLM with DPO Training.\n",
        "- **Model:** [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
        "- **Dataset:** [processed_train_data_filtered_dpo.json](../Data/Processed/CQ%20DPO%20Dataset.json)\n",
        "- **Frameworks:** Unsloth, HuggingFace, transformers, bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install --no-deps xformers triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer\n",
        "import logging\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, IntervalStrategy, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "8m2YPqS1_8sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b536421-3e2d-4a38-a774-41d83c06a4ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}\n"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1741cc9-0bcb-4a31-e6ea-acf5ee7c28ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 1638, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 1638 (delta 210), reused 161 (delta 123), pack-reused 1352 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1638/1638), 74.65 MiB | 22.45 MiB/s, done.\n",
            "Resolving deltas: 100% (940/940), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 4\n",
        "BASE_MODEL_REPO = \"unsloth/Meta-Llama-3.1-8B-Instruct-unsloth-bnb-4bit\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_DPO_1\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ DPO Dataset.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2703a2-c3c9-48c8-cf65-6e917fb4b433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--------  Start with Training  -------------\n",
            "INFO:__main__:Device selected: cuda\n",
            "INFO:__main__:Model: Meta-Llama-3.1-8B-Instruct_DPO_1\n",
            "INFO:__main__:Training number: 4\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f36bfc6-7cf6-407e-a081-7057d62994d8"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)\n",
        "dataset = dataset.filter(lambda x: x['score_chosen'] - x['score_rejected'] > 4)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "ZSW8diIbAtk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8ec6c5-33e7-4519-99d2-21bc082dd467"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'prompt', 'chosen', 'rejected', 'score_chosen', 'score_rejected', 'schema', 'context'],\n",
            "        num_rows: 1572\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Znebp07vA1Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Tokenizer"
      ],
      "metadata": {
        "id": "lLLX61krUNlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import CHAT_TEMPLATES\n",
        "print(list(CHAT_TEMPLATES.keys()))\n"
      ],
      "metadata": {
        "id": "eftsUd4XRz2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be08764-cd4b-43ba-d88d-11909ad3fe44"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unsloth', 'zephyr', 'chatml', 'mistral', 'llama', 'vicuna', 'vicuna_old', 'vicuna old', 'alpaca', 'gemma', 'gemma_chatml', 'gemma2', 'gemma2_chatml', 'llama-3', 'llama3', 'phi-3', 'phi-35', 'phi-3.5', 'llama-3.1', 'llama-31', 'llama-3.2', 'llama-3.3', 'llama-32', 'llama-33', 'qwen-2.5', 'qwen-25', 'qwen25', 'qwen2.5', 'phi-4', 'gemma-3', 'gemma3', 'qwen-3', 'qwen3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ],
      "metadata": {
        "id": "0wk97cwFSrMf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "hZvVvX0yUSjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "SfVoyCIsbVzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d50d71-e295-4267-f3df-1c9576ff039c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 0, 'prompt': [{'content': \"Generate one critical question addressing the provided context. Ensure it matches the schema: FearAppeal\\n\\nContext: alternate_viewpoints_perspectives: This post isnt meant to downplay the atrocities of the Holocaust, so dont misinterpret Climate change is not a political position, if you think it is you're dillusional. The billions of people who will be greatly affected by climate change is unreal and lets also consider the generations after.\", 'role': 'user'}], 'chosen': [{'content': 'What about the climate people makes them worse?', 'role': 'assistant'}], 'rejected': [{'content': 'Is considering climate change a political issue detrimental to the billions of people who will be greatly affected by it, as well as future generations?', 'role': 'assistant'}], 'score_chosen': 7.0, 'score_rejected': 2.0, 'schema': 'FearAppeal', 'context': \"alternate_viewpoints_perspectives: This post isnt meant to downplay the atrocities of the Holocaust, so dont misinterpret Climate change is not a political position, if you think it is you're dillusional. The billions of people who will be greatly affected by climate change is unreal and lets also consider the generations after.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = list(dataset[\"train\"].features)\n",
        "def apply_dpo_template(example):\n",
        "  if all(k in example.keys() for k in (\"chosen\", \"rejected\",\"prompt\")):\n",
        "\n",
        "    # For DPO, the inputs are triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue\n",
        "    prompt_messages = example[\"prompt\"]\n",
        "    chosen_messages = example[\"chosen\"]\n",
        "    rejected_messages = example[\"rejected\"]\n",
        "\n",
        "    example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "    example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "    example[\"text_prompt\"] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
        "  return example\n",
        "\n",
        "dataset = dataset.map(apply_dpo_template,remove_columns=column_names,\n",
        "          desc=\"Formatting comparisons with prompt template\",)"
      ],
      "metadata": {
        "id": "QvxvwyC7A4fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "IscEC6bXTExX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef4d8e3-d72b-4ca3-cd78-6bcb4edb5df6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text_chosen': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nWhat about the climate people makes them worse?<|eot_id|>', 'text_rejected': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIs considering climate change a political issue detrimental to the billions of people who will be greatly affected by it, as well as future generations?<|eot_id|>', 'text_prompt': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate one critical question addressing the provided context. Ensure it matches the schema: FearAppeal\\n\\nContext: alternate_viewpoints_perspectives: This post isnt meant to downplay the atrocities of the Holocaust, so dont misinterpret Climate change is not a political position, if you think it is you're dillusional. The billions of people who will be greatly affected by climate change is unreal and lets also consider the generations after.<|eot_id|>\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "7OQsby8nQQlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7159bd3c-eeb8-4fc4-d064-0f10874ec74a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text_chosen', 'text_rejected', 'text_prompt'],\n",
            "        num_rows: 1572\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n"
      ],
      "metadata": {
        "id": "nW6J16wib7vb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in [\"train\", \"test\"]:\n",
        "    dataset[split] = dataset[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ],
      "metadata": {
        "id": "meuPpzMOckLL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "lgWWcVPGcn2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa29cd2-a5a7-45e8-84f2-c4eb7bffc3c2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['chosen', 'rejected', 'prompt'],\n",
            "        num_rows: 1257\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['chosen', 'rejected', 'prompt'],\n",
            "        num_rows: 315\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training variables"
      ],
      "metadata": {
        "id": "IlDjbBm7UYJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DPOConfig\n",
        "training_args = DPOConfig(\n",
        "        do_eval=True,\n",
        "        eval_strategy = \"steps\",\n",
        "        save_strategy = \"steps\",\n",
        "        eval_steps = 5,\n",
        "        logging_steps = 1,\n",
        "        max_steps = 40,\n",
        "        warmup_ratio = 0.1,\n",
        "        per_device_train_batch_size = 20,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        per_device_eval_batch_size = 1,\n",
        "        learning_rate = 5e-6,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        save_total_limit = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.0,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = checkpoint_dir,\n",
        "        report_to = \"tensorboard\",\n",
        "        logging_dir = log_base_path\n",
        ")\n",
        "\n",
        "\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length = 1024,\n",
        "    max_prompt_length = 512\n",
        ")\n"
      ],
      "metadata": {
        "id": "lOCskNdmQRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d7ea09-e681-4ab3-905a-12235493edfd"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:GPU  Information before Training\n",
            "INFO:__main__:GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "INFO:__main__:7.623 GB of memory reserved.\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "e4f6274c-daeb-4f85-c5fd-78b544118785"
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,257 | Num Epochs = 3 | Total steps = 40\n",
            "O^O/ \\_/ \\    Batch size per device = 20 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (20 x 4 x 1) = 80\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 26:48, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>0.689418</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>-0.007790</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.007931</td>\n",
              "      <td>-186.495987</td>\n",
              "      <td>-168.128128</td>\n",
              "      <td>-0.597213</td>\n",
              "      <td>-0.600820</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.674700</td>\n",
              "      <td>0.687997</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>-0.008292</td>\n",
              "      <td>0.574603</td>\n",
              "      <td>0.010794</td>\n",
              "      <td>-186.472351</td>\n",
              "      <td>-168.133133</td>\n",
              "      <td>-0.595356</td>\n",
              "      <td>-0.599039</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.642400</td>\n",
              "      <td>0.685514</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>-0.011260</td>\n",
              "      <td>0.612698</td>\n",
              "      <td>0.015864</td>\n",
              "      <td>-186.451340</td>\n",
              "      <td>-168.162827</td>\n",
              "      <td>-0.597170</td>\n",
              "      <td>-0.601013</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.600600</td>\n",
              "      <td>0.683340</td>\n",
              "      <td>-0.000121</td>\n",
              "      <td>-0.020455</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.020335</td>\n",
              "      <td>-186.498581</td>\n",
              "      <td>-168.254761</td>\n",
              "      <td>-0.594719</td>\n",
              "      <td>-0.598853</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.561200</td>\n",
              "      <td>0.680309</td>\n",
              "      <td>-0.001787</td>\n",
              "      <td>-0.028234</td>\n",
              "      <td>0.717460</td>\n",
              "      <td>0.026447</td>\n",
              "      <td>-186.515259</td>\n",
              "      <td>-168.332550</td>\n",
              "      <td>-0.596404</td>\n",
              "      <td>-0.600756</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.533000</td>\n",
              "      <td>0.676037</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>-0.032132</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.035532</td>\n",
              "      <td>-186.463394</td>\n",
              "      <td>-168.371536</td>\n",
              "      <td>-0.598256</td>\n",
              "      <td>-0.602454</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.674797</td>\n",
              "      <td>-0.003095</td>\n",
              "      <td>-0.041103</td>\n",
              "      <td>0.774603</td>\n",
              "      <td>0.038008</td>\n",
              "      <td>-186.528336</td>\n",
              "      <td>-168.461258</td>\n",
              "      <td>-0.595334</td>\n",
              "      <td>-0.600088</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.479500</td>\n",
              "      <td>0.672818</td>\n",
              "      <td>0.011654</td>\n",
              "      <td>-0.030502</td>\n",
              "      <td>0.806349</td>\n",
              "      <td>0.042156</td>\n",
              "      <td>-186.380859</td>\n",
              "      <td>-168.355240</td>\n",
              "      <td>-0.597381</td>\n",
              "      <td>-0.601762</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "outputId": "6c12917b-8671-497f-e674-18844767f80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:1657.4516 seconds used for training.\n",
            "INFO:__main__:27.62 minutes used for training.\n",
            "INFO:__main__:Peak reserved memory = 37.773 GB.\n",
            "INFO:__main__:Peak reserved memory for training = 30.15 GB.\n",
            "INFO:__main__:Peak reserved memory % of max memory = 95.49 %.\n",
            "INFO:__main__:Peak reserved memory for training % of max memory = 76.219 %.\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c309a8-0193-451d-bb9a-9b80aed83bed"
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Saved LoRA adapters to /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_4/Model/Meta-Llama-3.1-8B-Instruct_DPO_1_lora_adapters/\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Push Lora weights to HF\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}-lora\", tokenizer, save_method = \"lora\", token = token)"
      ],
      "metadata": {
        "id": "7KDrL2FmPi11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit and save local\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "\n",
        "# Merge to 16bit and push to HF\n",
        "if True:\n",
        "  token = userdata.get('HF_TOKEN')\n",
        "  model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"$log_dir\""
      ],
      "metadata": {
        "id": "lFwHGRxBZ8q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "IEtvPF-5pFXp",
        "outputId": "a76a025e-2ae5-44ca-b186-7a02110a1659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_a_Generate_DPO_Dataset.ipynb\t      Development\n",
            "1_Information_preprocessing.md\t      Doc\n",
            "1_Preprocessing.ipynb\t\t      Evaluation\n",
            "2_Baseline_Generation.ipynb\t      INFORMATION.md\n",
            "2_Information_Baseline_Generation.md  LICENSE\n",
            "3_Evaluation.ipynb\t\t      Logs\n",
            "4_Finetuned_Generation.ipynb\t      README.md\n",
            "5_Evaluation_Analytics.ipynb\t      Training\n",
            "Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico St√§deli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}