{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_2_DPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# Training for critical question"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install --no-deps xformers triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import DPOTrainer\n",
        "import logging\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, IntervalStrategy, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "8m2YPqS1_8sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 6\n",
        "BASE_MODEL_REPO = \"ricostaedeli/Meta-Llama-3.1-1B-Instruct_SFT_2\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_SFT_2_DPO\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/processed_train_data_filtered_dpo.json\" #\"/content/NLP2025_CQG/Data/Processed/example_train.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   SFT Trainer Parameters   #############################\n",
        "################################################################################\n",
        "\n",
        "dataset_text_field = \"input_ids\"\n",
        "dataset_num_proc = 2\n",
        "packing = False\n",
        "per_device_train_batch_size = 2\n",
        "gradient_accumulation_steps = 4\n",
        "warmup_steps = 5\n",
        "max_steps = 10\n",
        "learning_rate = 2e-4\n",
        "fp16 = not is_bfloat16_supported()\n",
        "bf16 = is_bfloat16_supported()\n",
        "logging_steps = 1\n",
        "save_strategy = IntervalStrategy.STEPS\n",
        "save_steps = 1\n",
        "save_total_limit = 1\n",
        "optim = \"adamw_8bit\"\n",
        "weight_decay = 0.01\n",
        "lr_scheduler_type = \"linear\"\n",
        "seed = 3407\n",
        "output_dir = checkpoint_dir\n",
        "report_to = \"tensorboard\"\n",
        "logging_dir = log_base_path\n",
        "evaluation_strategy=\"steps\"\n",
        "eval_steps=1\n",
        "\n",
        "################################################################################\n",
        "#######################   Log Parameters            ############################\n",
        "################################################################################\n",
        "\n",
        "logger.info(\"------ Unlsoth Parameters ---------------\")\n",
        "logger.info(f\"max_seq_length: {max_seq_length}\")\n",
        "logger.info(f\"dtype: {dtype}\")\n",
        "logger.info(f\"load_in_4bit: {load_in_4bit}\")\n",
        "\n",
        "logger.info(\"------ PEFT Parameters ------------------\")\n",
        "logger.info(f\"r: {r}\")\n",
        "logger.info(f\"target_modules: {target_modules}\")\n",
        "logger.info(f\"lora_alpha: {lora_alpha}\")\n",
        "logger.info(f\"lora_dropout: {lora_dropout}\")\n",
        "logger.info(f\"bias: {bias}\")\n",
        "logger.info(f\"use_gradient_checkpointing: {use_gradient_checkpointing}\")\n",
        "logger.info(f\"random_state: {random_state}\")\n",
        "logger.info(f\"use_rslora: {use_rslora}\")\n",
        "\n",
        "logger.info(\"------  SFT Trainer Parameters ----------\")\n",
        "logger.info(f\"dataset_text_field: {dataset_text_field}\")\n",
        "logger.info(f\"dataset_num_proc: {dataset_num_proc}\")\n",
        "logger.info(f\"packing: {packing}\")\n",
        "logger.info(f\"per_device_train_batch_size: {per_device_train_batch_size}\")\n",
        "logger.info(f\"gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
        "logger.info(f\"warmup_steps: {warmup_steps}\")\n",
        "logger.info(f\"max_steps: {max_steps}\")\n",
        "logger.info(f\"learning_rate: {learning_rate}\")\n",
        "logger.info(f\"fp16: {fp16}\")\n",
        "logger.info(f\"bf16: {bf16}\")\n",
        "logger.info(f\"logging_steps: {logging_steps}\")\n",
        "logger.info(f\"save_strategy: {save_strategy}\")\n",
        "logger.info(f\"save_steps: {save_steps}\")\n",
        "logger.info(f\"save_total_limit: {save_total_limit}\")\n",
        "logger.info(f\"optim: {optim}\")\n",
        "logger.info(f\"weight_decay: {weight_decay}\")\n",
        "logger.info(f\"lr_scheduler_type: {lr_scheduler_type}\")\n",
        "logger.info(f\"seed: {seed}\")\n",
        "logger.info(f\"output_dir: {output_dir}\")\n",
        "logger.info(f\"report_to: {report_to}\")\n",
        "logger.info(f\"logging_dir: {logging_dir}\")\n",
        "logger.info(f\"evaluation_strategy: {evaluation_strategy}\")\n",
        "logger.info(f\"eval_steps: {eval_steps}\")"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "ZSW8diIbAtk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Znebp07vA1Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Tokenizer"
      ],
      "metadata": {
        "id": "lLLX61krUNlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import CHAT_TEMPLATES\n",
        "print(list(CHAT_TEMPLATES.keys()))\n"
      ],
      "metadata": {
        "id": "eftsUd4XRz2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ],
      "metadata": {
        "id": "0wk97cwFSrMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "hZvVvX0yUSjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "SfVoyCIsbVzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = list(dataset[\"train\"].features)\n",
        "def apply_dpo_template(example):\n",
        "  if all(k in example.keys() for k in (\"chosen\", \"rejected\",\"prompt\")):\n",
        "\n",
        "    # For DPO, the inputs are triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue\n",
        "    prompt_messages = example[\"prompt\"]\n",
        "    chosen_messages = example[\"chosen\"]\n",
        "    rejected_messages = example[\"rejected\"]\n",
        "\n",
        "    example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "    example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "    example[\"text_prompt\"] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
        "  return example\n",
        "\n",
        "dataset = dataset.map(apply_dpo_template,remove_columns=column_names,\n",
        "          desc=\"Formatting comparisons with prompt template\",)"
      ],
      "metadata": {
        "id": "QvxvwyC7A4fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "IscEC6bXTExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "7OQsby8nQQlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n"
      ],
      "metadata": {
        "id": "nW6J16wib7vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in [\"train\", \"test\"]:\n",
        "    dataset[split] = dataset[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )"
      ],
      "metadata": {
        "id": "meuPpzMOckLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "lgWWcVPGcn2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Training variables"
      ],
      "metadata": {
        "id": "IlDjbBm7UYJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "        do_eval=True,\n",
        "        eval_strategy = \"steps\",\n",
        "        save_strategy = \"steps\",\n",
        "        eval_steps = 5,\n",
        "        logging_steps = 1,\n",
        "        max_steps = 40,\n",
        "        warmup_ratio = 0.1,\n",
        "        per_device_train_batch_size = 20,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        per_device_eval_batch_size = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        save_total_limit = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.0,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = checkpoint_dir,\n",
        "        report_to = \"tensorboard\",\n",
        "        logging_dir = log_base_path\n",
        ")\n",
        "\n",
        "\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    max_length = 1024,\n",
        "    max_prompt_length = 512\n",
        ")\n"
      ],
      "metadata": {
        "id": "lOCskNdmQRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Push Lora weights to HF\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}-lora\", tokenizer, save_method = \"lora\", token = token)"
      ],
      "metadata": {
        "id": "7KDrL2FmPi11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit and save local\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "\n",
        "# Merge to 16bit and push to HF\n",
        "if True:\n",
        "  token = userdata.get('HF_TOKEN')\n",
        "  model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"$log_dir\""
      ],
      "metadata": {
        "id": "lFwHGRxBZ8q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "IEtvPF-5pFXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico St√§deli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}