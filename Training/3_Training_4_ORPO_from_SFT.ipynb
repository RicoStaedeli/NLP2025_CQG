{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_4_ORPO_from_SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Llama 3.1 with ORPO\n"
      ],
      "metadata": {
        "id": "xjNdEQfFZsie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4tKGKGGQWBH"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes wandb --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
        "import logging"
      ],
      "metadata": {
        "id": "1qsE1AsYbqPB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "cdfFOn11bt9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive"
      ],
      "metadata": {
        "id": "SnzIK-CKbsFJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQOcqp-Uby4t",
        "outputId": "962f10a0-65f5-4af4-b29f-975691199b46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tagW5Tp9b0eD",
        "outputId": "e1b21e03-44f2-4ded-91a2-90522693da6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 1680, done.\u001b[K\n",
            "remote: Counting objects: 100% (328/328), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 1680 (delta 239), reused 171 (delta 128), pack-reused 1352 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1680/1680), 75.04 MiB | 15.93 MiB/s, done.\n",
            "Resolving deltas: 100% (969/969), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 6\n",
        "BASE_MODEL_REPO = \"ricostaedeli/Meta-Llama-3.1-8B-Instruct_SFT\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_ORPO_SFT\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ DPO Dataset.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "TrIfQmfAb2dW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with ORPO Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRE-mOIicakV",
        "outputId": "31e8ece8-a425-4bdf-ee6e-8105d2bd9c69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--------  Start with ORPO Training  -------------\n",
            "INFO:__main__:Device selected: cuda\n",
            "INFO:__main__:Model: Meta-Llama-3.1-8B-Instruct_ORPO_SFT\n",
            "INFO:__main__:Training number: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwNxh0pCQhfI"
      },
      "outputs": [],
      "source": [
        "# Defined in the secrets tab in Google Colab\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)\n",
        "torch_dtype = torch.bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2kGBNStQoUd"
      },
      "outputs": [],
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO)\n",
        "\n",
        "# Reset chat_template if already set\n",
        "if tokenizer.chat_template is not None:\n",
        "    tokenizer.chat_template = None\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_REPO,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)\n",
        "dataset = dataset.filter(lambda x: x['score_chosen'] - x['score_rejected'] > 4)"
      ],
      "metadata": {
        "id": "tsD_R0CoctKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv6yYQsDdYE3",
        "outputId": "cb409610-14e7-4d91-a4bb-94f68db17832"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'prompt', 'chosen', 'rejected', 'score_chosen', 'score_rejected', 'schema', 'context'],\n",
            "        num_rows: 1572\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaTou6EQQnAK"
      },
      "outputs": [],
      "source": [
        "def format_chat_template(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc= os.cpu_count(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT78c0i3fq5c",
        "outputId": "53d3e75e-0b46-4aaa-b630-3f53a201de8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'prompt', 'chosen', 'rejected', 'score_chosen', 'score_rejected', 'schema', 'context'],\n",
            "        num_rows: 1572\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = ['id', 'score_chosen', 'score_rejected', 'schema', 'context']\n",
        "\n",
        "dataset = dataset['train'].remove_columns(columns_to_remove)"
      ],
      "metadata": {
        "id": "o4nJmbmuhJU4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.05)"
      ],
      "metadata": {
        "id": "NxkJl604fqGw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxep3ltjfxTz",
        "outputId": "bb44807c-015b-43f4-a64b-9175d046320b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected'],\n",
            "        num_rows: 1493\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['prompt', 'chosen', 'rejected'],\n",
            "        num_rows: 79\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWDwJe7_Qqgb"
      },
      "outputs": [],
      "source": [
        "orpo_args = ORPOConfig(\n",
        "    learning_rate=8e-6,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    beta=0.1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    remove_unused_columns=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    num_train_epochs=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    warmup_steps=10,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"./results_orpo_sft/\",\n",
        ")\n",
        "\n",
        "trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    args=orpo_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flush memory\n",
        "#del trainer, model\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO)\n",
        "# Reset chat_template if already set\n",
        "if tokenizer.chat_template is not None:\n",
        "    tokenizer.chat_template = None\n",
        "\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_REPO,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "fp16_model, tokenizer = setup_chat_format(fp16_model, tokenizer)\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(fp16_model, MODEL_NAME)\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "n6b3dCS6fL45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(MODEL_NAME, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(MODEL_NAME, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "pvWUrbyNgEnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "uvrUte4_eUii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0677460-8574-47b4-d1be-c186f2d19b85"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_a_Generate_DPO_Dataset.ipynb\t      Development\n",
            "1_Information_preprocessing.md\t      Doc\n",
            "1_Preprocessing.ipynb\t\t      Evaluation\n",
            "2_Baseline_Generation.ipynb\t      INFORMATION.md\n",
            "2_Information_Baseline_Generation.md  LICENSE\n",
            "3_Evaluation.ipynb\t\t      Logs\n",
            "4_Finetuned_Generation.ipynb\t      README.md\n",
            "5_Evaluation_Analytics.ipynb\t      Training\n",
            "Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "JpVu3t9yeWJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2134cb5-9631-46b6-cf9d-1d90925de1e4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 3a80758] Training Number: 6, Training logs in Google Drive\n",
            " 1 file changed, 4 insertions(+)\n",
            " create mode 100644 Logs/training_6.log\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 12 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 523 bytes | 523.00 KiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/RicoStaedeli/NLP2025_CQG.git\n",
            "   da0e871..3a80758  main -> main\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}