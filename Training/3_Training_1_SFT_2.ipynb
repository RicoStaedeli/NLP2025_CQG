{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_1_SFT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# SFT Training to generate critical questions\n",
        "In this training we try to use the generated scores for the evaluated SocratiQ dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install --no-deps xformers triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, IntervalStrategy, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "IHt2o6XrVqPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 2\n",
        "BASE_MODEL_REPO = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-1B-Instruct_SFT_2\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/drive/MyDrive/HSG/NLP/Project NLP/Data/Final/Categorised/categoriesed_filtered_train_data.json\" #\"/content/NLP2025_CQG/Data/Processed/example_train.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   SFT Trainer Parameters   #############################\n",
        "################################################################################\n",
        "\n",
        "dataset_text_field = \"text\"\n",
        "dataset_num_proc = 2\n",
        "packing = False\n",
        "per_device_train_batch_size = 38\n",
        "gradient_accumulation_steps = 4\n",
        "warmup_steps = 5\n",
        "num_train_epochs = 1\n",
        "max_steps = 40\n",
        "learning_rate = 2e-4\n",
        "fp16 = not is_bfloat16_supported()\n",
        "bf16 = is_bfloat16_supported()\n",
        "logging_steps = 1\n",
        "save_strategy = IntervalStrategy.STEPS\n",
        "save_steps = 10\n",
        "save_total_limit = 1\n",
        "optim = \"adamw_8bit\"\n",
        "weight_decay = 0.01\n",
        "lr_scheduler_type = \"linear\"\n",
        "seed = 3407\n",
        "output_dir = checkpoint_dir\n",
        "report_to = \"tensorboard\"\n",
        "logging_dir = log_base_path\n",
        "\n",
        "################################################################################\n",
        "#######################   Log Parameters            ############################\n",
        "################################################################################\n",
        "\n",
        "logger.info(\"------ Unlsoth Parameters ---------------\")\n",
        "logger.info(f\"max_seq_length: {max_seq_length}\")\n",
        "logger.info(f\"dtype: {dtype}\")\n",
        "logger.info(f\"load_in_4bit: {load_in_4bit}\")\n",
        "\n",
        "logger.info(\"------ PEFT Parameters ------------------\")\n",
        "logger.info(f\"r: {r}\")\n",
        "logger.info(f\"target_modules: {target_modules}\")\n",
        "logger.info(f\"lora_alpha: {lora_alpha}\")\n",
        "logger.info(f\"lora_dropout: {lora_dropout}\")\n",
        "logger.info(f\"bias: {bias}\")\n",
        "logger.info(f\"use_gradient_checkpointing: {use_gradient_checkpointing}\")\n",
        "logger.info(f\"random_state: {random_state}\")\n",
        "logger.info(f\"use_rslora: {use_rslora}\")\n",
        "\n",
        "logger.info(\"------  SFT Trainer Parameters ----------\")\n",
        "logger.info(f\"dataset_text_field: {dataset_text_field}\")\n",
        "logger.info(f\"dataset_num_proc: {dataset_num_proc}\")\n",
        "logger.info(f\"packing: {packing}\")\n",
        "logger.info(f\"per_device_train_batch_size: {per_device_train_batch_size}\")\n",
        "logger.info(f\"gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
        "logger.info(f\"warmup_steps: {warmup_steps}\")\n",
        "logger.info(f\"max_steps: {max_steps}\")\n",
        "logger.info(f\"learning_rate: {learning_rate}\")\n",
        "logger.info(f\"fp16: {fp16}\")\n",
        "logger.info(f\"bf16: {bf16}\")\n",
        "logger.info(f\"logging_steps: {logging_steps}\")\n",
        "logger.info(f\"save_strategy: {save_strategy}\")\n",
        "logger.info(f\"save_steps: {save_steps}\")\n",
        "logger.info(f\"save_total_limit: {save_total_limit}\")\n",
        "logger.info(f\"optim: {optim}\")\n",
        "logger.info(f\"weight_decay: {weight_decay}\")\n",
        "logger.info(f\"lr_scheduler_type: {lr_scheduler_type}\")\n",
        "logger.info(f\"seed: {seed}\")\n",
        "logger.info(f\"output_dir: {output_dir}\")\n",
        "logger.info(f\"report_to: {report_to}\")\n",
        "logger.info(f\"logging_dir: {logging_dir}\")"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "SIkNGcTZVEpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schemas_template = {\n",
        "\"CauseToEffect\": \"\"\"'Cause to Effect' with the examples:\n",
        "How strong is the generalisation that if <eventA> then <eventB>?\n",
        "Are there other factors in this particular case that could have interfered with the event of‘<eventB>’?\"\"\",\n",
        "\n",
        "\"ExpertOpinion\": \"\"\"'Expert Opinion' with the examples:\n",
        "Is <expertE> a genuine expert in <domainD>?\n",
        "Is <eventA> consistent with what other experts in <domainD> say? \"\"\",\n",
        "\n",
        "\"Analogy\": \"\"\"'Analogy' with the examples:\n",
        "Are <C1> and <C2> similar in the respect cited?\n",
        "Is <eventA> true in <C1>?\"\"\",\n",
        "\n",
        "\"FearAppeal\": \"\"\"'Fear Appeal' with the examples:\n",
        "Is <eventB> bad? Why and to whom is it bad?\n",
        "Is <eventA> away to prevent <eventB>?\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "RZWyPv1Zlv1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_chat_llama = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "### Instruction:\n",
        "Generate one critical question addressing the provided context. Ensure it matches the schema:\n",
        "{}\n",
        "\n",
        "### Context:\n",
        "{}\n",
        "\n",
        "Respond with only the generated question.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vUFAKRKNJFs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVBEly_dR9r"
      },
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    contexts        = examples[\"context\"]\n",
        "    questions       = examples[\"question\"]\n",
        "    schemas         = examples[\"schema\"]\n",
        "\n",
        "    texts = []\n",
        "    for context, question, CauseToEffect, Analogy, ExpertOpinion, FearAppeal in zip(contexts, questions, CauseToEffect, Analogy, ExpertOpinion, FearAppeal):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        schema_text = schemas_template.get(schema_key, \"Schema Unknown\")\n",
        "        text = template_chat_llama.format(schema_text, context, question) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzpgH-4hgOSE"
      },
      "source": [
        "# Check the structure of the dataset\n",
        "print(dataset['train'][0]['text'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "QtoUXIkSEOqM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']"
      ],
      "metadata": {
        "id": "VUtmfc1bX4gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "vH9fqG6wZaap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    warmup_steps=warmup_steps,\n",
        "    max_steps=max_steps,\n",
        "    # num_train_epochs=num_train_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    logging_steps=logging_steps,\n",
        "    save_strategy=save_strategy,\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=save_total_limit,\n",
        "    optim=optim,\n",
        "    weight_decay=weight_decay,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    seed=seed,\n",
        "    output_dir=output_dir,\n",
        "    report_to=report_to,\n",
        "    logging_dir=logging_dir,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = dataset_text_field,\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = dataset_num_proc,\n",
        "    packing = packing,\n",
        "    args = args,\n",
        ")"
      ],
      "metadata": {
        "id": "HybmWblAU9ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('HF_TOKEN')\n",
        "# Push only the adapters to Hugging Face\n",
        "model.push_to_hub(f\"ricostaedeli/{MODEL_NAME}-lora\", use_auth_token=token, private=False)\n",
        "\n",
        "# Also save tokenizer separately\n",
        "tokenizer.push_to_hub(f\"ricostaedeli/{MODEL_NAME}-lora\", use_auth_token=token, private=False)"
      ],
      "metadata": {
        "id": "mL6eYe-nRn4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is only needed if the upload failed somehow and the checkpoint has to be reloaded"
      ],
      "metadata": {
        "id": "gk2Sgy-pxXC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_lora_adapter_save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "token = userdata.get('HF_TOKEN2')\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tgP5oVGovd7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "if False:\n",
        "  token = userdata.get('HF_TOKEN')\n",
        "  model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_4bit\",)\n",
        "  logger.info(f\"Saved merged model in 4bit to {model_save_path}\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"lora\",)\n",
        "  logger.info(f\"Saved LoRA adapters to {model_save_path}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "Save model in GGUF format\n",
        "\n",
        "Some supported quant methods (full list on [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "  model.save_pretrained_gguf(model_save_path, tokenizer,)\n",
        "  logger.info(f\"Saved merged model in 8bit Q8_0 to {model_save_path}\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "  model.save_pretrained_gguf(model_save_path, tokenizer, quantization_method = \"f16\")\n",
        "  logger.info(f\"Saved merged model in 16bit GGUF to {model_save_path}\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "  model.save_pretrained_gguf(model_save_path, tokenizer, quantization_method = \"q4_k_m\")\n",
        "  logger.info(f\"Saved merged model in q4_k_m GGUF to {model_save_path}\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "bRGhKnr_OBeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive.\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}