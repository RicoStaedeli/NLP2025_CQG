{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_1_SFT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# SFT 1 Training to generate critical questions\n",
        "In this training we try to use the generated scores for the evaluated SocratiQ dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install -qqq -U transformers datasets accelerate peft trl bitsandbytes wandb --progress-bar off"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
        "import logging"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata, drive"
      ],
      "metadata": {
        "id": "cx8mSTMWdA6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 7\n",
        "BASE_MODEL_REPO = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_SFT_1\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ SFT Dataset.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined in the secrets tab in Google Colab\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)\n",
        "torch_dtype = torch.bfloat16"
      ],
      "metadata": {
        "id": "Ubwy5GiMW-nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_REPO,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "esShDnQJW-k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "SIkNGcTZVEpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schemas_template = {\n",
        "\"CauseToEffect\": \"\"\"'Cause to Effect' with the examples:\n",
        "How strong is the generalisation that if <eventA> then <eventB>?\n",
        "Are there other factors in this particular case that could have interfered with the event of‘<eventB>’?\"\"\",\n",
        "\n",
        "\"ExpertOpinion\": \"\"\"'Expert Opinion' with the examples:\n",
        "Is <expertE> a genuine expert in <domainD>?\n",
        "Is <eventA> consistent with what other experts in <domainD> say? \"\"\",\n",
        "\n",
        "\"Analogy\": \"\"\"'Analogy' with the examples:\n",
        "Are <C1> and <C2> similar in the respect cited?\n",
        "Is <eventA> true in <C1>?\"\"\",\n",
        "\n",
        "\"FearAppeal\": \"\"\"'Fear Appeal' with the examples:\n",
        "Is <eventB> bad? Why and to whom is it bad?\n",
        "Is <eventA> away to prevent <eventB>?\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "RZWyPv1Zlv1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_chat_llama = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "### Instruction:\n",
        "Generate one critical question addressing the provided context. Ensure it matches the schema:\n",
        "{}\n",
        "\n",
        "### Context:\n",
        "{}\n",
        "\n",
        "Respond with only the generated question.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vUFAKRKNJFs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVBEly_dR9r"
      },
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    contexts        = examples[\"context\"]\n",
        "    questions       = examples[\"question\"]\n",
        "    schemas         = examples[\"schema\"]\n",
        "\n",
        "    texts = []\n",
        "    for schema_key, context, question in zip(schemas, contexts, questions):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        schema_text = schemas_template.get(schema_key, \"Schema Unknown\")\n",
        "        text = template_chat_llama.format(schema_text, context, question) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzpgH-4hgOSE"
      },
      "source": [
        "# Check the structure of the dataset\n",
        "print(dataset['train'][0]['text'])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "QtoUXIkSEOqM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']"
      ],
      "metadata": {
        "id": "VUtmfc1bX4gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "vH9fqG6wZaap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = SFTConfig(\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    max_length = 2048,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps = 40,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=5,\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=\"./results_sft/\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    dataset_text_field=\"text\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,\n",
        ")"
      ],
      "metadata": {
        "id": "HybmWblAU9ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "source": [
        "trainer_stats = trainer.train()\n",
        "trainer.train()\n",
        "trainer.save_model(MODEL_NAME)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flush memory\n",
        "#del trainer, model\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_REPO)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_REPO,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(fp16_model, MODEL_NAME)\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "O5iytN6pX3AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(MODEL_NAME, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(MODEL_NAME, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "0WmdM21x6JJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "bRGhKnr_OBeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive.\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}