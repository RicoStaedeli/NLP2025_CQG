{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/Training/3_Training_1_SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXd2BtiJZudR"
      },
      "source": [
        "# SFT Training to generate critical questions\n",
        "In this training we try to use the generated scores for the evaluated SocratiQ dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "First we define some constant values and also install all needed libraries"
      ],
      "metadata": {
        "id": "2EgLCwHZjbJq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ_G82h9IJZd"
      },
      "source": [
        "\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEZVYHjIJZd"
      },
      "source": [
        "!pip install --no-deps xformers triton unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import shutil\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, IntervalStrategy, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "IHt2o6XrVqPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab\n",
        "This part is only relevant when using the notebook in google colab"
      ],
      "metadata": {
        "id": "jCclvucWLsKk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unzlb06QZtEA"
      },
      "source": [
        "from google.colab import userdata, drive"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "token = userdata.get('GITHUB')"
      ],
      "metadata": {
        "id": "y6Tz4rCcMG8v",
        "outputId": "fd3d0ae9-6aa5-47d1-e912-9524edf4600d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone GitHub Repository to directly push generated files"
      ],
      "metadata": {
        "id": "rwhQ3vxTMNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "4GRzz9_aMMcN",
        "outputId": "e9f17d5f-f87c-4b25-ee10-cbd24e2a3d3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP2025_CQG'...\n",
            "remote: Enumerating objects: 1889, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 1889 (delta 279), reused 204 (delta 172), pack-reused 1530 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1889/1889), 51.74 MiB | 27.03 MiB/s, done.\n",
            "Resolving deltas: 100% (1133/1133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Path Variables and Logger"
      ],
      "metadata": {
        "id": "H_ifLVF4jodo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "TRAINING_NUMBER = 1\n",
        "BASE_MODEL_REPO = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct_SFT\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "train_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ SFT Dataset.json\"\n",
        "\n",
        "log_base_path = f\"/content/NLP2025_CQG/Training/Logs/Traing_{TRAINING_NUMBER}/Tensorboard/\"\n",
        "os.makedirs(log_base_path, exist_ok=True)\n",
        "\n",
        "log_file_path = f\"/content/NLP2025_CQG/Logs/training_{TRAINING_NUMBER}.log\"\n",
        "\n",
        "model_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_finetuned/\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model_lora_adapter_save_path = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Model/{MODEL_NAME}_lora_adapters/\"\n",
        "os.makedirs(model_lora_adapter_save_path, exist_ok=True)\n",
        "\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_{TRAINING_NUMBER}/Checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   LOGGER                ################################\n",
        "################################################################################\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_file_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ],
      "metadata": {
        "id": "zV2l7p5Zjn3J"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"--------  Start with Training  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Model: {MODEL_NAME}')\n",
        "logger.info(f'Training number: {TRAINING_NUMBER}')"
      ],
      "metadata": {
        "id": "S0hztCbTzdFl",
        "outputId": "044ed9e0-25fa-46c1-f5d2-6dfa82029331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--------  Start with Training  -------------\n",
            "INFO:__main__:Device selected: cuda\n",
            "INFO:__main__:Model: Meta-Llama-3.1-8B-Instruct_SFT\n",
            "INFO:__main__:Training number: 1\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Parameters"
      ],
      "metadata": {
        "id": "RuEtJFzlt8Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   Unlsoth Parameters    ################################\n",
        "################################################################################\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   PEFT Parameters       ################################\n",
        "################################################################################\n",
        "\n",
        "r = 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0 # Supports any, but = 0 is optimized\n",
        "bias = \"none\"    # Supports any, but = \"none\" is optimized\n",
        "use_gradient_checkpointing = \"unsloth\" # True or \"unsloth\" for very long context\n",
        "random_state = 3407\n",
        "use_rslora = False  # Unsloth supports rank stabilized LoRA\n",
        "loftq_config = None # And LoftQ\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#######################   SFT Trainer Parameters   #############################\n",
        "################################################################################\n",
        "\n",
        "dataset_text_field = \"text\"\n",
        "dataset_num_proc = 2\n",
        "packing = False\n",
        "per_device_train_batch_size = 38\n",
        "gradient_accumulation_steps = 4\n",
        "warmup_steps = 5\n",
        "num_train_epochs = 1\n",
        "max_steps = 40\n",
        "learning_rate = 2e-4\n",
        "fp16 = not is_bfloat16_supported()\n",
        "bf16 = is_bfloat16_supported()\n",
        "logging_steps = 1\n",
        "save_strategy = IntervalStrategy.STEPS\n",
        "save_steps = 10\n",
        "save_total_limit = 1\n",
        "optim = \"adamw_8bit\"\n",
        "weight_decay = 0.01\n",
        "lr_scheduler_type = \"linear\"\n",
        "seed = 3407\n",
        "output_dir = checkpoint_dir\n",
        "report_to = \"tensorboard\"\n",
        "logging_dir = log_base_path\n",
        "\n",
        "################################################################################\n",
        "#######################   Log Parameters            ############################\n",
        "################################################################################\n",
        "\n",
        "logger.info(\"------ Unlsoth Parameters ---------------\")\n",
        "logger.info(f\"max_seq_length: {max_seq_length}\")\n",
        "logger.info(f\"dtype: {dtype}\")\n",
        "logger.info(f\"load_in_4bit: {load_in_4bit}\")\n",
        "\n",
        "logger.info(\"------ PEFT Parameters ------------------\")\n",
        "logger.info(f\"r: {r}\")\n",
        "logger.info(f\"target_modules: {target_modules}\")\n",
        "logger.info(f\"lora_alpha: {lora_alpha}\")\n",
        "logger.info(f\"lora_dropout: {lora_dropout}\")\n",
        "logger.info(f\"bias: {bias}\")\n",
        "logger.info(f\"use_gradient_checkpointing: {use_gradient_checkpointing}\")\n",
        "logger.info(f\"random_state: {random_state}\")\n",
        "logger.info(f\"use_rslora: {use_rslora}\")\n",
        "\n",
        "logger.info(\"------  SFT Trainer Parameters ----------\")\n",
        "logger.info(f\"dataset_text_field: {dataset_text_field}\")\n",
        "logger.info(f\"dataset_num_proc: {dataset_num_proc}\")\n",
        "logger.info(f\"packing: {packing}\")\n",
        "logger.info(f\"per_device_train_batch_size: {per_device_train_batch_size}\")\n",
        "logger.info(f\"gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
        "logger.info(f\"warmup_steps: {warmup_steps}\")\n",
        "logger.info(f\"max_steps: {max_steps}\")\n",
        "logger.info(f\"learning_rate: {learning_rate}\")\n",
        "logger.info(f\"fp16: {fp16}\")\n",
        "logger.info(f\"bf16: {bf16}\")\n",
        "logger.info(f\"logging_steps: {logging_steps}\")\n",
        "logger.info(f\"save_strategy: {save_strategy}\")\n",
        "logger.info(f\"save_steps: {save_steps}\")\n",
        "logger.info(f\"save_total_limit: {save_total_limit}\")\n",
        "logger.info(f\"optim: {optim}\")\n",
        "logger.info(f\"weight_decay: {weight_decay}\")\n",
        "logger.info(f\"lr_scheduler_type: {lr_scheduler_type}\")\n",
        "logger.info(f\"seed: {seed}\")\n",
        "logger.info(f\"output_dir: {output_dir}\")\n",
        "logger.info(f\"report_to: {report_to}\")\n",
        "logger.info(f\"logging_dir: {logging_dir}\")"
      ],
      "metadata": {
        "id": "nqB-T9L1t63x",
        "outputId": "3765a882-b846-485a-dc22-5b6ab425655e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:------ Unlsoth Parameters ---------------\n",
            "INFO:__main__:max_seq_length: 2048\n",
            "INFO:__main__:dtype: None\n",
            "INFO:__main__:load_in_4bit: True\n",
            "INFO:__main__:------ PEFT Parameters ------------------\n",
            "INFO:__main__:r: 16\n",
            "INFO:__main__:target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "INFO:__main__:lora_alpha: 16\n",
            "INFO:__main__:lora_dropout: 0\n",
            "INFO:__main__:bias: none\n",
            "INFO:__main__:use_gradient_checkpointing: unsloth\n",
            "INFO:__main__:random_state: 3407\n",
            "INFO:__main__:use_rslora: False\n",
            "INFO:__main__:------  SFT Trainer Parameters ----------\n",
            "INFO:__main__:dataset_text_field: text\n",
            "INFO:__main__:dataset_num_proc: 2\n",
            "INFO:__main__:packing: False\n",
            "INFO:__main__:per_device_train_batch_size: 38\n",
            "INFO:__main__:gradient_accumulation_steps: 4\n",
            "INFO:__main__:warmup_steps: 5\n",
            "INFO:__main__:max_steps: 40\n",
            "INFO:__main__:learning_rate: 0.0002\n",
            "INFO:__main__:fp16: False\n",
            "INFO:__main__:bf16: True\n",
            "INFO:__main__:logging_steps: 1\n",
            "INFO:__main__:save_strategy: IntervalStrategy.STEPS\n",
            "INFO:__main__:save_steps: 10\n",
            "INFO:__main__:save_total_limit: 1\n",
            "INFO:__main__:optim: adamw_8bit\n",
            "INFO:__main__:weight_decay: 0.01\n",
            "INFO:__main__:lr_scheduler_type: linear\n",
            "INFO:__main__:seed: 3407\n",
            "INFO:__main__:output_dir: /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_1/Checkpoints/\n",
            "INFO:__main__:report_to: tensorboard\n",
            "INFO:__main__:logging_dir: /content/NLP2025_CQG/Training/Logs/Traing_1/Tensorboard/\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiDyJlJIJZe"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL_REPO,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "outputId": "1d4f6650-654b-46cc-e278-2555f2f4d4f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = r,\n",
        "    target_modules = target_modules,\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    bias = bias,\n",
        "    use_gradient_checkpointing = use_gradient_checkpointing,\n",
        "    random_state = random_state,\n",
        "    use_rslora = use_rslora,\n",
        "    loftq_config = loftq_config,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr4IjbeMgFx-"
      },
      "source": [
        "## Load and preprocess dataset\n",
        "The raw dataset [SocratiQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main) has a label at the begining of the context. We have to remove that and also tokenize the input for the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynl_2GhwaO1K"
      },
      "source": [
        "dataset = load_dataset('json', data_files=train_dataset_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "SIkNGcTZVEpi",
        "outputId": "7465a663-5b31-4d25-8372-0ad726746804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'context', 'question', 'schema'],\n",
            "        num_rows: 3426\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schemas_template = {\n",
        "\"CauseToEffect\": \"\"\"'Cause to Effect' with the examples:\n",
        "How strong is the generalisation that if <eventA> then <eventB>?\n",
        "Are there other factors in this particular case that could have interfered with the event of‘<eventB>’?\"\"\",\n",
        "\n",
        "\"ExpertOpinion\": \"\"\"'Expert Opinion' with the examples:\n",
        "Is <expertE> a genuine expert in <domainD>?\n",
        "Is <eventA> consistent with what other experts in <domainD> say? \"\"\",\n",
        "\n",
        "\"Analogy\": \"\"\"'Analogy' with the examples:\n",
        "Are <C1> and <C2> similar in the respect cited?\n",
        "Is <eventA> true in <C1>?\"\"\",\n",
        "\n",
        "\"FearAppeal\": \"\"\"'Fear Appeal' with the examples:\n",
        "Is <eventB> bad? Why and to whom is it bad?\n",
        "Is <eventA> away to prevent <eventB>?\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "RZWyPv1Zlv1F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_chat_llama = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "### Instruction:\n",
        "Generate one critical question addressing the provided context. Ensure it matches the schema:\n",
        "{}\n",
        "\n",
        "### Context:\n",
        "{}\n",
        "\n",
        "Respond with only the generated question.<|eot_id|>\n",
        "\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vUFAKRKNJFs8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVBEly_dR9r"
      },
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    contexts        = examples[\"context\"]\n",
        "    questions       = examples[\"question\"]\n",
        "    schemas         = examples[\"schema\"]\n",
        "\n",
        "    texts = []\n",
        "    for schema_key, context, question in zip(schemas, contexts, questions):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        schema_text = schemas_template.get(schema_key, \"Schema Unknown\")\n",
        "        text = template_chat_llama.format(schema_text, context, question) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzpgH-4hgOSE",
        "outputId": "e12a8877-ba47-45db-c346-5d2c70d71e4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the structure of the dataset\n",
        "print(dataset['train'][0]['text'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You generate concise, critical, single-sentence questions for argumentative contexts, matching specified question schemas.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "### Instruction:\n",
            "Generate one critical question addressing the provided context. Ensure it matches the schema:\n",
            "'Cause to Effect' with the examples:\n",
            "How strong is the generalisation that if <eventA> then <eventB>?\n",
            "Are there other factors in this particular case that could have interfered with the event of‘<eventB>’?\n",
            "\n",
            "### Context:\n",
            "alternate_viewpoints_perspectives: I'd certainly feel good if the shooter 300 feet above me had to specially, consciously train and practice repetitive trigger techniques on multiple trigger assemblies instead of buying a piece of plastic that does the work for you. I'd feel better if limited magazine sizes were enforced as to make rapid fire less useful for slaughtering a crowd of people at random and give first responders a better chance of confronting an armed suspect who has to keep changing magazines to continue his assault. I'd feel great if people who amass over 40 weapons had to undergo additional wait times, background screening, and register as a \"collector\" or selling FFL instead of John Q.\n",
            "\n",
            "Respond with only the generated question.<|eot_id|>\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "### Response:\n",
            "What about him owning 40 guns made him more dangerous than if he had only owned one or two?\n",
            "<|eot_id|>\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "QtoUXIkSEOqM",
        "outputId": "0505d085-6f5c-440e-f262-d4e9a4366a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'context', 'question', 'schema', 'text'],\n",
            "        num_rows: 3426\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']"
      ],
      "metadata": {
        "id": "VUtmfc1bX4gj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "id": "vH9fqG6wZaap",
        "outputId": "66b06d64-6e58-4a07-c6dc-15f952f51338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'context', 'question', 'schema', 'text'],\n",
            "    num_rows: 3426\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    warmup_steps=warmup_steps,\n",
        "    max_steps=max_steps,\n",
        "    # num_train_epochs=num_train_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    logging_steps=logging_steps,\n",
        "    save_strategy=save_strategy,\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=save_total_limit,\n",
        "    optim=optim,\n",
        "    weight_decay=weight_decay,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    seed=seed,\n",
        "    output_dir=output_dir,\n",
        "    report_to=report_to,\n",
        "    logging_dir=logging_dir,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = dataset_text_field,\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = dataset_num_proc,\n",
        "    packing = packing,\n",
        "    args = args,\n",
        ")"
      ],
      "metadata": {
        "id": "HybmWblAU9ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "outputId": "4a5e4d15-fce2-4a9a-ffeb-e98b5bd31ce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "logger.info(f\"GPU  Information before Training\")\n",
        "logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "logger.info(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:GPU  Information before Training\n",
            "INFO:__main__:GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "INFO:__main__:7.623 GB of memory reserved.\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "outputId": "ac9b8994-832d-48f0-851a-151efbc75777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,426 | Num Epochs = 2 | Total steps = 40\n",
            "O^O/ \\_/ \\    Batch size per device = 38 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (38 x 4 x 1) = 152\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 12:43, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.641100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.608900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.344600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.668800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.342900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.177100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.132300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.124700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.106500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.088300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.078400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.060900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.051100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.047600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.032400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.017700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.013400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.010900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.009300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.008300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.007100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.007200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.007200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.006900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "outputId": "a5c204f7-a638-4e58-f570-6c4c6c4fa6cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "logger.info(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "logger.info(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:797.1529 seconds used for training.\n",
            "INFO:__main__:13.29 minutes used for training.\n",
            "INFO:__main__:Peak reserved memory = 22.479 GB.\n",
            "INFO:__main__:Peak reserved memory for training = 14.856 GB.\n",
            "INFO:__main__:Peak reserved memory % of max memory = 56.827 %.\n",
            "INFO:__main__:Peak reserved memory for training % of max memory = 37.556 %.\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "## Save\n",
        "Save the finetuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Lora delta weights"
      ],
      "metadata": {
        "id": "rwmygDJBB4dJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upcOlWe7A1vc",
        "outputId": "9303958d-cc9f-42d3-a88b-4479b264fc5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.save_pretrained(model_lora_adapter_save_path)  # Local saving\n",
        "tokenizer.save_pretrained(model_lora_adapter_save_path)\n",
        "logger.info(f\"Saved LoRA adapters to {model_lora_adapter_save_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Saved LoRA adapters to /content/drive/MyDrive/HSG/NLP/Project NLP/Training/Training_1/Model/Meta-Llama-3.1-8B-Instruct_SFT_lora_adapters/\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Push Lora weights to HF\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}-lora\", tokenizer, save_method = \"lora\", token = token)"
      ],
      "metadata": {
        "id": "mL6eYe-nRn4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving merged model\n",
        "\n",
        "Save merged model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is only needed if the upload failed somehow and the checkpoint has to be reloaded"
      ],
      "metadata": {
        "id": "gk2Sgy-pxXC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_lora_adapter_save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "token = userdata.get('HF_TOKEN2')\n",
        "model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tgP5oVGovd7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "source": [
        "# Merge to 16bit and save local\n",
        "if False:\n",
        "  model.save_pretrained_merged(model_save_path, tokenizer, save_method = \"merged_16bit\",)\n",
        "  logger.info(f\"Saved merged model in 16bit to {model_save_path}\")\n",
        "\n",
        "# Merge to 16bit and push to HF\n",
        "if True:\n",
        "  token = userdata.get('HF_TOKEN')\n",
        "  model.push_to_hub_merged(f\"ricostaedeli/{MODEL_NAME}\", tokenizer, save_method=\"merged_16bit\", token=token, private=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push Logs"
      ],
      "metadata": {
        "id": "pG6hGjrYoLFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "bRGhKnr_OBeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Rico Städeli\"\n",
        "!git config --global user.email \"rico@yabriga.ch\"\n",
        "\n",
        "\n",
        "commit_message = f\"Training Number: {TRAINING_NUMBER}, Training logs in Google Drive.\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "ezeAgls_cl5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}