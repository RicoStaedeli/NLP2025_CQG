{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/2a_Baseline_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "id": "IJ1bQaNGMh_9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import spacy\n",
        "import textstat\n",
        "\n",
        "# Load models and tools\n",
        "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text and generated question\n",
        "source_text = \"The French Revolution was a period of far-reaching social and political upheaval in France and its colonies.\"\n",
        "generated_question = \"How did the French Revolution influence modern political ideologies?\"\n",
        "\n",
        "# 1. Relevance: Semantic similarity\n",
        "source_emb = bert_model.encode(source_text, convert_to_tensor=True)\n",
        "question_emb = bert_model.encode(generated_question, convert_to_tensor=True)\n",
        "relevance_score = util.cos_sim(source_emb, question_emb).item()\n",
        "\n",
        "# 2. Clarity\n",
        "def calculate_perplexity(text):\n",
        "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    return torch.exp(outputs.loss).item()\n",
        "\n",
        "def grammar_errors(text):\n",
        "    doc = nlp(text)\n",
        "    # Simple grammar heuristic: count tokens with dependency 'dep'\n",
        "    return sum(1 for token in doc if token.dep_ == \"dep\")\n",
        "\n",
        "perplexity = calculate_perplexity(generated_question)\n",
        "grammar_issue_count = grammar_errors(generated_question)\n",
        "readability_score = textstat.flesch_reading_ease(generated_question)\n",
        "\n",
        "# 3. Depth: Semantic distance (proxy for abstraction)\n",
        "depth_score = 1 - relevance_score\n",
        "\n",
        "# 4. Insightfulness: Heuristic\n",
        "abstract_keywords = [\"impact\", \"influence\", \"significance\", \"consequences\", \"assumption\", \"implication\"]\n",
        "insightfulness_score = depth_score\n",
        "if any(word in generated_question.lower() for word in abstract_keywords):\n",
        "    insightfulness_score += 1\n",
        "\n",
        "# Results\n",
        "print(\"\\n--- Critical Question Evaluation ---\")\n",
        "print(f\"Question: {generated_question}\\n\")\n",
        "print(f\"Relevance Score (0–1): {relevance_score:.3f}\")\n",
        "print(f\"Perplexity (lower = better): {perplexity:.2f}\")\n",
        "print(f\"Grammar Issue Count (lower = better): {grammar_issue_count}\")\n",
        "print(f\"Readability Score (Flesch, higher = easier): {readability_score:.2f}\")\n",
        "print(f\"Depth Score (0–1): {depth_score:.3f}\")\n",
        "print(f\"Insightfulness Score (0–2): {insightfulness_score:.3f}\")"
      ],
      "metadata": {
        "id": "54-LM-Hm8re7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Baseline predictions\n",
        "In this notebook we evaluate the results of our baseline predictions. For the baseline the following pretrained models are used:\n",
        "- Llama 3.1-8B-Instruct\n",
        "- Qwen2.5 7B Instruct"
      ],
      "metadata": {
        "id": "PObhsFPu8vX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and Installation"
      ],
      "metadata": {
        "id": "D__-Jqvu9OTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers evaluate scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3tGZA-MT3UHc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google-research/bleurt.git"
      ],
      "metadata": {
        "id": "5XCIN_HbI8Cy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "1D56ttFi9r1S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from evaluate import load\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "6vq84kLn26Lj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get('GITHUB')\n",
        "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
        "\n",
        "!git clone {repo_url}"
      ],
      "metadata": {
        "id": "pHJEqQKiWmf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"NLP2025_CQG\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "bRVvx5BgWoTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Variables and Paths"
      ],
      "metadata": {
        "id": "BherrjmE9FYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#######################   STATIC VARIABLES      ################################\n",
        "################################################################################\n",
        "\n",
        "EVALUATION_NAME = \"results_zeroshot_llama_3.1-8B-instruct\"\n",
        "\n",
        "sim_threshold = 0.6 # recommened 0.6 - 0.75 [0,1]\n",
        "bleurt_threshold = 0.2 # recommened 0.2 - 0.4 [-1,1]\n",
        "\n",
        "sentence_transformer_model = \"stsb-mpnet-base-v2\"\n",
        "\n",
        "################################################################################\n",
        "#######################   PATH VARIABLES        ################################\n",
        "################################################################################\n",
        "\n",
        "base_file_path = os.path.join(os.getcwd(), \"Data/Processed/test.json\")\n",
        "generated_cqs_path = os.path.join(os.getcwd(), f\"Evaluation/Results/{EVALUATION_NAME}.json\")\n",
        "evaluation_result_path = os.path.join(os.getcwd(), f\"Evaluation/Scored/{EVALUATION_NAME}_eval.json\")\n",
        "log_path = \"Logs/2a_baseline_evaluation.log\"\n",
        "\n",
        "# Setup logger manually\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create file handler (only if not already added)\n",
        "if not logger.handlers:\n",
        "    fh = logging.FileHandler(log_path)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)"
      ],
      "metadata": {
        "id": "9z9B4oqE9N6A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect device\n",
        "device = torch.device(\n",
        "    \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "logger.info(f\"--------  Start with evaluation for {EVALUATION_NAME}  -------------\")\n",
        "logger.info(f'Device selected: {device}')\n",
        "logger.info(f'Similarity threshold: {sim_threshold}')\n",
        "logger.info(f'Bleurt threshold: {bleurt_threshold}')\n",
        "logger.info(f'Sentence transformer model: {sentence_transformer_model}')"
      ],
      "metadata": {
        "id": "yP_VPE2v3ABd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "reeGN8Dm9YZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load model ===\n",
        "sim_model = SentenceTransformer(sentence_transformer_model)\n",
        "bleurt_model = load(\"bleurt\", module_type=\"metric\")"
      ],
      "metadata": {
        "id": "EShdGaTn3D_6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load data ===\n",
        "with open(base_file_path) as f:\n",
        "    reference = json.load(f)\n",
        "\n",
        "with open(generated_cqs_path) as f:\n",
        "    new = json.load(f)"
      ],
      "metadata": {
        "id": "GwNz7_GD3F17"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Evaluate ===\n",
        "predicted_labels = []\n",
        "punctuations = []\n",
        "\n",
        "\n",
        "for instance in new.keys():\n",
        "    sim_punctuation = 0\n",
        "    bleurt_punctuation = 0\n",
        "    reference_set = [ref['cq'] for ref in reference[instance]['cqs']]\n",
        "\n",
        "    if new[instance]['cqs'] != 'Missing CQs':\n",
        "        cqs_check = [cq['cq'] for cq in new[instance]['cqs']]\n",
        "        if len(cqs_check) != len(set(cqs_check)):\n",
        "            logger.warning('There are repeated CQs in ' + instance)\n",
        "\n",
        "        for i, line in enumerate(new[instance]['cqs']):\n",
        "\n",
        "            sim_winner = None\n",
        "            sim_sentence_embedding = sim_model.encode(line['cq'])\n",
        "            sim_reference_embedding = sim_model.encode(reference_set)\n",
        "            sims = sim_model.similarity(sim_sentence_embedding, sim_reference_embedding).tolist()[0]\n",
        "\n",
        "            sim_winner = np.argmax(sims)\n",
        "            sims_max_score = sims[sim_winner]\n",
        "\n",
        "            if sims_max_score > sim_threshold:\n",
        "                simlabel = reference[instance]['cqs'][sim_winner]['label']\n",
        "                if simlabel == 'Useful':\n",
        "                    sim_punctuation += 1/3\n",
        "            else:\n",
        "                label = 'not_able_to_evaluate'\n",
        "            new[instance]['cqs'][i]['sim_label'] = simlabel\n",
        "            new[instance]['cqs'][i]['sim_score'] = sims_max_score\n",
        "\n",
        "\n",
        "            bleuert_winner = None\n",
        "            bleurt_results = bleurt_model.compute(predictions=[line['cq']] * len(reference_set), references=reference_set)\n",
        "            bleurt = bleurt_results['scores']\n",
        "\n",
        "            bleuert_winner = np.argmax(bleurt)\n",
        "            bleurt_max_score = bleurt[bleuert_winner]\n",
        "\n",
        "            if bleurt_max_score > bleurt_threshold:\n",
        "                bleurt_label = reference[instance]['cqs'][bleuert_winner]['label']\n",
        "                if bleurt_label == 'Useful':\n",
        "                    bleurt_punctuation += 1/3\n",
        "            else:\n",
        "                bleurt_label = 'not_able_to_evaluate'\n",
        "            new[instance]['cqs'][i]['bleurt_label'] = bleurt_label\n",
        "            new[instance]['cqs'][i]['bleurt_score'] = bleurt_max_score\n",
        "\n",
        "            predicted_labels.append((simlabel, bleurt_label))\n",
        "\n",
        "    else:\n",
        "        predicted_labels.extend([('not_able_to_evaluate', 'not_able_to_evaluate'), ('not_able_to_evaluate', 'not_able_to_evaluate'), ('not_able_to_evaluate', 'not_able_to_evaluate')])\n",
        "\n",
        "    new[instance]['sim_score'] = sim_punctuation\n",
        "    new[instance]['bleurt_score'] = bleurt_punctuation\n",
        "    punctuations.append((sim_punctuation, bleurt_punctuation))\n",
        "    logger.info(f'{instance} score (based on similarity): {sim_punctuation:.2f}/1.00 score (based on bleurt): {bleurt_punctuation:.2f}/1.00')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kJOvVHwgfIcs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# === Summary ===\n",
        "logger.info('------ Summary Metrics ------')\n",
        "logger.info(f'Distribution of labels: {Counter(predicted_labels)}')\n",
        "logger.info(f'Distribution of punctuation: {Counter(punctuations)}')\n",
        "total_sim, total_bleurt = 0.0, 0.0\n",
        "for sim, bleurt in punctuations:\n",
        "    total_sim += sim\n",
        "    total_bleurt += bleurt\n",
        "\n",
        "avg_sim = total_sim / len(punctuations) if punctuations else 0.0\n",
        "avg_bleurt = total_bleurt / len(punctuations) if punctuations else 0.0\n",
        "\n",
        "logger.info(f'Overall sim punctuation: {avg_sim:.4f}')\n",
        "logger.info(f'Overall bleurt punctuation: {avg_bleurt:.4f}')\n",
        "\n",
        "# === Save updated results with labels ===\n",
        "with open(evaluation_result_path, 'w') as o:\n",
        "    json.dump(new, o, indent=4)\n",
        "logger.info(f\"Saved labeled output to {evaluation_result_path}\")\n"
      ],
      "metadata": {
        "id": "-9gmGu_x3IIS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Showcas\"\n",
        "!git config --global user.email \"cedric.bohni@gmx.de\"\n",
        "\n",
        "\n",
        "commit_message = f\"evaluate CQs for Baseline model {EVALUATION_NAME}\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "sec-S12NsbDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}