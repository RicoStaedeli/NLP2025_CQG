{
 "cells": [
  {
   "metadata": {
    "id": "ea5047c0b73859ef"
   },
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/RicoStaedeli/NLP2025_CQG/blob/main/1_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "ea5047c0b73859ef"
  },
  {
   "metadata": {
    "id": "2ecc99f084d1f1f5"
   },
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "In this notebook we preprocess the dataset SocratiQ to get all needed datasets.\n",
    "- **Base Dataset**:  [SocraticQ](https://github.com/NUS-IDS/eacl23_soqg/tree/main)\n",
    "- **Evaluation**: Rule based according Walton's argumentation schemes\n",
    "\n",
    "**Prerquisits**\n",
    "- Access to GitHub Repository and added Secret in Colab for access token to GitHub\n",
    "- GPU for preprocessing at least T4 from Google Colab\n",
    "\n",
    "**Output:** <br>\n",
    "This notebook generates:\n",
    "- **[CQ FULL Dataset](Data/Processed/CQ%20FULL%20Dataset.json)**: A dataset containing all original context-question pairs from the SocratiQ Dataset with all evaluation scores.\n",
    "- **[CQ SFT Dataset](Data/Processed/CQ%20SFT%20Dataset.json)**: A dataset containing only context-question pairs which are marked as critical."
   ],
   "id": "2ecc99f084d1f1f5"
  },
  {
   "metadata": {
    "id": "427efc26daba8f64"
   },
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "First we define some constant values and also install all needed libraries"
   ],
   "id": "427efc26daba8f64"
  },
  {
   "metadata": {
    "id": "8c9569b5241bfa88"
   },
   "cell_type": "markdown",
   "source": [
    "### Installation"
   ],
   "id": "8c9569b5241bfa88"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "527b4b9300213a9e",
    "outputId": "aa254dc8-e021-4881-de72-e7667d87f6f9"
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "from nltk.corpus import wordnet as wn\n",
    "import logging\n",
    "import os\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('framenet_v17')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "id": "527b4b9300213a9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Colab\n",
    "This part is only relevant when using the notebook in google colab"
   ],
   "metadata": {
    "id": "tsQe0KZ2KVIe"
   },
   "id": "tsQe0KZ2KVIe"
  },
  {
   "cell_type": "code",
   "source": "from google.colab import userdata",
   "metadata": {
    "id": "GZvlcNbJKYhR"
   },
   "id": "GZvlcNbJKYhR",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a2ab1dead67d524a"
   },
   "cell_type": "markdown",
   "source": [
    "### GitHub\n",
    "Clone GitHub Repository to directly push generated files"
   ],
   "id": "a2ab1dead67d524a"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a05e2a7ab9f959f5",
    "outputId": "b7215b2e-198a-456f-e727-d5cf5dd456d3"
   },
   "cell_type": "code",
   "source": [
    "token = userdata.get('GITHUB')\n",
    "repo_url = f\"https://{token}@github.com/RicoStaedeli/NLP2025_CQG.git\"\n",
    "\n",
    "!git clone {repo_url}"
   ],
   "id": "a05e2a7ab9f959f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d125fb8e5c848adc"
   },
   "cell_type": "markdown",
   "source": "### Static Variables",
   "id": "d125fb8e5c848adc"
  },
  {
   "metadata": {
    "id": "19748da5750143b1"
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "#######################   STATIC VARIABLES      ################################\n",
    "################################################################################\n",
    "\n",
    "MIN_CONTEXT_LENGTH = 25\n",
    "TRESHHOLD_STRONG = 7\n",
    "TRESHHOLD_WEAK = 4\n",
    "\n",
    "PUSH_TO_GITHUB = False\n",
    "\n",
    "################################################################################\n",
    "#######################   PATH VARIABLES        ################################\n",
    "################################################################################\n",
    "\n",
    "raw_socratiq_chunk_1_path = \"/content/NLP2025_CQG/Data/Raw/SocraticQ/train_chunk_I.csv\"\n",
    "raw_socratiq_chunk_2_path = \"/content/NLP2025_CQG/Data/Raw/SocraticQ/train_chunk_II.csv\"\n",
    "raw_socratiq_chunk_3_path = \"/content/NLP2025_CQG/Data/Raw/SocraticQ/train_chunk_III.csv\"\n",
    "\n",
    "processed_sft_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ SFT Dataset.json\"\n",
    "processed_dpo_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ DPO Dataset.json\"\n",
    "processed_full_dataset_path = \"/content/NLP2025_CQG/Data/Processed/CQ FULL Dataset.json\"\n",
    "\n",
    "log_file_path = f\"/content/NLP2025_CQG/Logs/data_preprocessing.log\"\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#######################   LOGGER                ################################\n",
    "################################################################################\n",
    "\n",
    "# Setup logger manually\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create file handler (only if not already added)\n",
    "if not logger.handlers:\n",
    "    fh = logging.FileHandler(log_file_path)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)"
   ],
   "id": "19748da5750143b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b93046409c27c0f1"
   },
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "id": "b93046409c27c0f1"
  },
  {
   "metadata": {
    "id": "c6c79537bcba2f9a"
   },
   "cell_type": "markdown",
   "source": [
    "### lentgh filtering\n",
    "In this step we filter all very small questions from the dataset."
   ],
   "id": "c6c79537bcba2f9a"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e105af9fdae043c8",
    "outputId": "36dcfd10-505b-4100-8fad-ed560827eada"
   },
   "cell_type": "code",
   "source": [
    "logger.info(\"Start Preprocessing...\")\n",
    "logger.info(f\"Start length filtering with min context size: {MIN_CONTEXT_LENGTH}\")\n",
    "\n",
    "# Define paths to the input files\n",
    "input_files = [\n",
    "    raw_socratiq_chunk_1_path,\n",
    "    raw_socratiq_chunk_2_path,\n",
    "    raw_socratiq_chunk_3_path\n",
    "]\n",
    "\n",
    "# Load and combine all CSVs\n",
    "dataframes = []\n",
    "for file_path in input_files:\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        names=[\"category\", \"context\", \"question\"]\n",
    "    )\n",
    "    dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "### Only for debuging purpose\n",
    "# combined_df = combined_df.head(200).copy()\n",
    "\n",
    "# Compute context token lengths\n",
    "combined_df[\"context_token_len\"] = combined_df[\"context\"].apply(lambda text: len(nlp(str(text))))\n",
    "\n",
    "# Filter rows with context token length >= 25\n",
    "filtered_df = combined_df[combined_df[\"context_token_len\"] >= MIN_CONTEXT_LENGTH].copy()\n",
    "\n",
    "# Print stats\n",
    "print(f\"Total rows: {len(combined_df)}\")\n",
    "print(f\"Rows after filtering: {len(filtered_df)}\")\n",
    "logger.info(f\"Total rows: {len(combined_df)}\")\n",
    "logger.info(f\"Rows after filtering: {len(filtered_df)}\")\n",
    "\n",
    "# Save to new CSV\n",
    "#filtered_df.to_csv(\"filtered_socraticq.csv\", index=False)"
   ],
   "id": "e105af9fdae043c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3160b0fc6cf9fa5b"
   },
   "cell_type": "markdown",
   "source": [
    "### Argumentative schmeme scoring\n",
    "In this section each entry in the dataset is scored and evaluated according four different argumentative schemes"
   ],
   "id": "3160b0fc6cf9fa5b"
  },
  {
   "metadata": {
    "id": "59f3dbbc12863ec"
   },
   "cell_type": "markdown",
   "source": [
    "#### 1. cause to effect"
   ],
   "id": "59f3dbbc12863ec"
  },
  {
   "metadata": {
    "id": "93410b8f08242699"
   },
   "cell_type": "code",
   "source": [
    "def get_causal_verbs_from_framenet():\n",
    "    causal_frame_names = [\n",
    "        \"Causation\", \"Cause_change\", \"Cause_change_of_position_on_a_scale\",\n",
    "        \"Cause_motion\", \"Cause_to_amalgamate\", \"Cause_to_start\", \"Cause_to_make_progress\",\n",
    "        \"Causation_scenario\", \"Cause_to_end\", \"Cause_to_resume\",\n",
    "        \"Cause_to_continue\", \"Cause_change_of_consistency\",\"Cause_expansion\",\"Cause_impact\"\n",
    "    ]\n",
    "\n",
    "    causal_verbs = set()\n",
    "    for frame_name in causal_frame_names:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:  # Only verbs\n",
    "                    causal_verbs.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame '{frame_name}': {e}\")\n",
    "\n",
    "    return causal_verbs\n",
    "\n",
    "\n",
    "causal_meta_terms = {\"generalisation\", \"implies\", \"entail\", \"necessitate\", \"follow from\", \"inference\"}\n",
    "alternative_factor_terms = {\"factor\", \"interfere\", \"influence\", \"affect\", \"contribute\", \"complicate\"}\n",
    "\n",
    "\n",
    "def detect_cause_to_effect(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    explanations = []\n",
    "    score = 0\n",
    "\n",
    "    causal_verbs = get_causal_verbs_from_framenet()\n",
    "\n",
    "    has_condition = any(tok.dep_ == \"mark\" and tok.text.lower() in {\"if\", \"when\"} for tok in doc)\n",
    "    if has_condition:\n",
    "        explanations.append(\"✓ Conditional clause detected (e.g., 'if', 'when')\")\n",
    "        score += 3\n",
    "\n",
    "    has_advcl = any(tok.dep_ == \"advcl\" for tok in doc)\n",
    "    if has_advcl:\n",
    "        explanations.append(\"✓ Adverbial clause (likely effect clause) detected\")\n",
    "        score += 2\n",
    "\n",
    "    has_causal_verb_structure = False\n",
    "    for tok in doc:\n",
    "        if tok.lemma_ in causal_verbs and tok.pos_ == \"VERB\":\n",
    "            subj = any(child.dep_ == \"nsubj\" for child in tok.children)\n",
    "            obj = any(child.dep_ == \"dobj\" for child in tok.children)\n",
    "            prep = any(child.dep_ == \"prep\" for child in tok.children)\n",
    "            if subj or obj or prep:\n",
    "                has_causal_verb_structure = True\n",
    "                explanations.append(\n",
    "                    f\"✓ Verb '{tok.lemma_}' is listed in FrameNet under causal frames with subject/object/prep\"\n",
    "                )\n",
    "                score += 3\n",
    "                if subj: score += 0.5\n",
    "                if obj: score += 0.5\n",
    "                if prep: score += 0.5\n",
    "                break\n",
    "\n",
    "    if any(tok.lemma_ in causal_meta_terms for tok in doc):\n",
    "      explanations.append(\"✓ Causal generalisation or implication term detected (e.g., 'implies', 'generalisation')\")\n",
    "      score += 1\n",
    "\n",
    "    if any(tok.lemma_ in alternative_factor_terms for tok in doc):\n",
    "      explanations.append(\"✓ Terms indicating alternative causes or interfering factors detected\")\n",
    "      score += 1\n",
    "\n",
    "    is_causal = has_condition and has_advcl or has_causal_verb_structure\n",
    "    if not is_causal:\n",
    "        causal_phrases = [\"result in\", \"lead to\", \"may cause\", \"because of\", \"due to\",\"given rise to\",\"resulting from\", \"stemming from\", \"driven by\", \"caused by\", \"attributed to\", \"stems from\", \"reason\", \"result of\", \"consequence of\", \"owning to\", \"thus\", \"so\", \"therefore\", \"hence\"  \"thereby\"]\n",
    "        if any(phrase in sentence.lower() for phrase in causal_phrases):\n",
    "            explanations.append(\"✓ Phrase pattern matches known cause-to-effect trigger\")\n",
    "            score += 2\n",
    "\n",
    "    score = min(score, 10)\n",
    "    label = \"Strong CauseToEffect\" if score >= TRESHHOLD_STRONG else \"Weak/Partial CauseToEffect\" if score >= TRESHHOLD_WEAK else \"Not CauseToEffect\"\n",
    "    logger.info(f\"CauseToEffect --> Score: {score}, Label: {label}, Explanations: {explanations}\")\n",
    "    return label, score, explanations"
   ],
   "id": "93410b8f08242699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9a0b65f173521c87"
   },
   "cell_type": "markdown",
   "source": [
    "#### 2. expert opinion"
   ],
   "id": "9a0b65f173521c87"
  },
  {
   "metadata": {
    "id": "bf1757d5f91c9875"
   },
   "cell_type": "code",
   "source": [
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load frame '{frame_name}': {e}\")\n",
    "    return terms\n",
    "\n",
    "\n",
    "expert_frames = [\n",
    "    \"Expertise\", \"Judgment_communication\", \"Opinion\",\n",
    "    \"Authority\", \"Statement\", \"Certainty\"\n",
    "]\n",
    "quote_frames = [\"Statement\", \"Judgment_communication\"]\n",
    "clarity_frames = [\"Reasoning\"]\n",
    "evidence_frames = [\"Evidence\", \"Certainty\", \"Causation\"]\n",
    "\n",
    "\n",
    "expert_verbs = get_lexical_units_from_frames(expert_frames)\n",
    "quote_verbs = get_lexical_units_from_frames(quote_frames)\n",
    "clarity_terms = get_lexical_units_from_frames(clarity_frames)\n",
    "evidence_terms = get_lexical_units_from_frames(evidence_frames)\n",
    "\n",
    "def detect_expert_opinion(question):\n",
    "\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "\n",
    "    expert_titles = {\"expert\", \"researcher\", \"scientist\", \"doctor\", \"analyst\", \"professor\", \"Dr.\"}\n",
    "\n",
    "    implicit_expert_terms = {\"study\", \"research\", \"evidence\", \"report\", \"findings\", \"scientific\", \"government\", \"official\", \"paper\", \"survey\", \"data\"}\n",
    "    comparison_cues = {\"consistent\", \"align\", \"similar\", \"agree\", \"disagree\", \"corroborate\", \"conflict\"}\n",
    "    technical_request_verbs = {\"define\", \"explain\", \"describe\", \"elaborate\", \"clarify\"}\n",
    "    assertion_verbs = {\"assert\", \"affirm\", \"pronounce\", \"declare\", \"maintain\", \"claim\", \"state\"}\n",
    "    reference_terms = {\"quote\", \"reference\", \"cite\", \"check\", \"verify\", \"source\"}\n",
    "    domain_terms = {\"science\", \"scientific\", \"domain\", \"field\", \"discipline\", \"area\", \"sector\"}\n",
    "\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
    "            if any(title in ent.text.lower() for title in expert_titles):\n",
    "                explanations.append(f\"✓ Expert entity detected: '{ent.text}'\")\n",
    "                score += 3\n",
    "                break\n",
    "\n",
    "    if any(tok.lemma_ in expert_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Detected expert-related verb from FrameNet\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.lemma_ in quote_verbs for tok in doc):\n",
    "        explanations.append(\"✓ Quotation or claim verb found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in clarity_terms for tok in doc):\n",
    "        explanations.append(\"✓ Clarity/definition markers found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in evidence_terms for tok in doc):\n",
    "        explanations.append(\"✓ Evidence or support-related terms found\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.lemma_.lower() in implicit_expert_terms for tok in doc):\n",
    "      explanations.append(\"✓ Implicit expert-related term detected (e.g., 'study', 'government')\")\n",
    "      score += 2\n",
    "\n",
    "    if any(tok.lemma_.lower() in comparison_cues for tok in doc):\n",
    "      explanations.append(\"✓ Cross-study comparison term detected (e.g., 'consistent', 'similar')\")\n",
    "      score += 0.5\n",
    "\n",
    "    if any(tok.lemma_.lower() in technical_request_verbs for tok in doc):\n",
    "      explanations.append(\"✓ Technical explanation request detected (e.g., 'define', 'explain')\")\n",
    "      score += 1\n",
    "\n",
    "    if any(tok.dep_ == \"attr\" and tok.lemma_ == \"expert\" for tok in doc):\n",
    "      explanations.append(\"✓ Predicate nominative indicating expertise detected (e.g., 'X is an expert')\")\n",
    "      score += 2\n",
    "\n",
    "    if any(tok.lemma_.lower() in assertion_verbs for tok in doc):\n",
    "      explanations.append(\"✓ Assertion or claim verb detected (e.g., 'assert', 'affirm')\")\n",
    "      score += 1\n",
    "\n",
    "    if any(tok.lemma_.lower() in reference_terms for tok in doc):\n",
    "      explanations.append(\"✓ Source/reference validation term detected (e.g., 'quote', 'reference')\")\n",
    "      score += 1\n",
    "\n",
    "    if any(tok.lemma_.lower() in domain_terms for tok in doc):\n",
    "      explanations.append(\"✓ Domain relevance indicator detected (e.g., 'science', 'domainD')\")\n",
    "      score += 1\n",
    "\n",
    "    score = min(score, 10)\n",
    "    label = \"Strong Expert Opinion\" if score >= TRESHHOLD_STRONG else \"Weak/Partial Expert Opinion\" if score >= TRESHHOLD_WEAK else \"Not Expert Opinion\"\n",
    "    logger.info(f\"ExpertOpinion --> Score: {score}, Label: {label}, Explanations: {explanations}\")\n",
    "    return label, score, explanations"
   ],
   "id": "bf1757d5f91c9875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "fd40c3ea318ba47e"
   },
   "cell_type": "markdown",
   "source": [
    "#### 3. Analogy detection"
   ],
   "id": "fd40c3ea318ba47e"
  },
  {
   "metadata": {
    "id": "f03e4efb2d914dac"
   },
   "cell_type": "code",
   "source": [
    "analogy_synsets = [wn.synset('similar.a.01'), wn.synset('analogy.n.01'), wn.synset('compare.v.01')]\n",
    "\n",
    "comparison_frames = [\"Similarity\"]\n",
    "contrast_frames = [\"Categorization\"]\n",
    "evidence_frames = [\"Evidence\", \"Judgment_communication\"]\n",
    "\n",
    "comparison_verbs = get_lexical_units_from_frames(comparison_frames)\n",
    "contrast_verbs = get_lexical_units_from_frames(contrast_frames)\n",
    "evidence_verbs = get_lexical_units_from_frames(evidence_frames)\n",
    "\n",
    "def is_semantically_analogical(word_token):\n",
    "    token_synsets = wn.synsets(word_token.lemma_)\n",
    "    for s in token_synsets:\n",
    "        for analogy_syn in analogy_synsets:\n",
    "            if s.path_similarity(analogy_syn) and s.path_similarity(analogy_syn) > 0.3:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "analogy_context_cues = {\"respect\", \"in which\", \"such that\", \"with regard to\", \"in terms of\"}\n",
    "\n",
    "analogy_force_cues = {\"undermine\", \"weaken\", \"strengthen\", \"force of similarity\", \"degree of analogy\"}\n",
    "\n",
    "analogy_nouns = {\"analogy\", \"comparison\", \"parallel\", \"similarity\", \"analogue\"}\n",
    "\n",
    "def detect_analogy_question(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "    noun_chunks = list(doc.noun_chunks)\n",
    "\n",
    "    if any(tok.lemma_ in comparison_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Comparison verb detected from FrameNet\")\n",
    "        score += 2.5\n",
    "\n",
    "    entity_tokens = [tok for tok in doc if tok.pos_ in {\"PROPN\", \"NOUN\"}]\n",
    "    if len(set(tok.lemma_ for tok in entity_tokens)) >= 2:\n",
    "        explanations.append(\"✓ Contains at least two distinct concepts/entities\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in contrast_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Contrast or difference verb detected from FrameNet\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in evidence_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Evidence or justification verb found\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.tag_ == \"MD\" for tok in doc):\n",
    "        score += 0.5\n",
    "\n",
    "    if len(noun_chunks) >= 2 and any(tok.lemma_ in {\"similar\", \"like\", \"as\"} for tok in doc):\n",
    "        explanations.append(\"✓ Two concepts compared with similarity cue (e.g., 'similar', 'like')\")\n",
    "        score += 3\n",
    "\n",
    "    if any(tok.text.lower() == \"if\" for tok in doc):\n",
    "        explanations.append(\"✓ Conditional structure suggesting hypothetical reasoning\")\n",
    "        score += 1\n",
    "\n",
    "    if any(is_semantically_analogical(tok) for tok in doc if tok.pos_ in {\"ADJ\", \"NOUN\", \"VERB\"}):\n",
    "        explanations.append(\"✓ Semantic similarity to analogy-related terms detected via WordNet\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.dep_ in {\"prep\", \"relcl\"} and tok.lemma_ in {\"compare\", \"similar\"} for tok in doc):\n",
    "        explanations.append(\"✓ Syntactic cue of analogy (e.g., 'compared with', 'similar to')\")\n",
    "        score += 1\n",
    "\n",
    "    if any(phrase in question.lower() for phrase in analogy_context_cues):\n",
    "      explanations.append(\"✓ Contextual analogy marker detected (e.g., 'in which', 'such that')\")\n",
    "      score += 0.5\n",
    "\n",
    "    if any(tok.lemma_ in analogy_force_cues for tok in doc):\n",
    "      explanations.append(\"✓ Analogy evaluation term detected (e.g., 'undermine', 'strengthen')\")\n",
    "      score += 0.5\n",
    "\n",
    "    if any(tok.lemma_ in analogy_nouns for tok in doc if tok.pos_ == \"NOUN\"):\n",
    "      explanations.append(\"✓ Explicit analogy noun detected (e.g., 'analogy', 'comparison')\")\n",
    "      score += 2\n",
    "\n",
    "    if any(tok.dep_ == \"neg\" for tok in doc):\n",
    "      if any(tok.lemma_ in {\"similar\", \"compare\", \"alike\", \"match\"} for tok in doc):\n",
    "          explanations.append(\"✓ Negated comparison detected (suggesting analogy breakdown)\")\n",
    "          score += 1\n",
    "\n",
    "    score = min(score, 10)\n",
    "    label = \"Strong Analogy Question\" if score >= 7 else \"Weak/Partial Analogy Question\" if score >= TRESHHOLD_WEAK else \"Not Analogy Question\"\n",
    "    logger.info(f\"Analogy --> Score: {score}, Label: {label}, Explanations: {explanations}\")\n",
    "    return label, score, explanations"
   ],
   "id": "f03e4efb2d914dac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "da06c0d1ea24b7ef"
   },
   "cell_type": "markdown",
   "source": [
    "#### 4. Fear appeal"
   ],
   "id": "da06c0d1ea24b7ef"
  },
  {
   "metadata": {
    "id": "f28b26e9e3c89b51"
   },
   "cell_type": "code",
   "source": [
    "def is_fear_related(word_token):\n",
    "    syns = wn.synsets(word_token.lemma_)\n",
    "    for s in syns:\n",
    "        if any(s.path_similarity(wn.synset('danger.n.01')) or s.path_similarity(wn.synset('problem.n.01')) or\n",
    "               s.path_similarity(wn.synset('fear.n.01')) or s.path_similarity(wn.synset('harm.n.01')) or\n",
    "               s.path_similarity(wn.synset('threat.n.01')) for s in syns):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ---- FrameNet Utility ----\n",
    "def get_lexical_units_from_frames(frames):\n",
    "    terms = set()\n",
    "    for frame_name in frames:\n",
    "        try:\n",
    "            frame = fn.frame_by_name(frame_name)\n",
    "            for lu in frame.lexUnit.values():\n",
    "                if '.v' in lu['name']:\n",
    "                    terms.add(lu['name'].split('.')[0])\n",
    "        except:\n",
    "            continue\n",
    "    return terms\n",
    "\n",
    "# ---- Relevant Lexical Resources ----\n",
    "causal_frames = [\"Causation\", \"Cause_to_start\", \"Preventing\", \"Risk\", \"Threaten\", \"Danger\"]\n",
    "causal_verbs = get_lexical_units_from_frames(causal_frames)\n",
    "\n",
    "fear_keywords = {\"danger\", \"threat\", \"risky\", \"harm\", \"catastrophe\", \"crisis\", \"ruin\", \"fear\", \"worse\", \"bad\", \"fatal\", \"negative\", \"die\", \"death\"}\n",
    "preventive_keywords = {\"prevent\", \"avoid\", \"stop\", \"ban\", \"rescue\", \"save\", \"protect\"}\n",
    "\n",
    "urgency_keywords = {\"immediately\", \"soon\", \"before it's too late\", \"critical\", \"urgent\", \"suddenly\", \"unexpectedly\"}\n",
    "\n",
    "possibility_terms = {\"possible\", \"possibility\", \"likely\", \"likelihood\", \"chance\", \"probability\", \"conceivable\", \"potential\", \"can\", \"could\", \"might\", \"may\", \"able\"}\n",
    "\n",
    "\n",
    "def detect_fear_appeal_question(question):\n",
    "    doc = nlp(question)\n",
    "    score = 0\n",
    "    explanations = []\n",
    "\n",
    "    if any(tok.lemma_.lower() in fear_keywords for tok in doc):\n",
    "        explanations.append(\"✓ Fear-related keyword detected (e.g., 'threat', 'danger')\")\n",
    "        score += 3\n",
    "\n",
    "    if any(tok.lemma_.lower() in preventive_keywords for tok in doc):\n",
    "        explanations.append(\"✓ Preventive action verb detected (e.g., 'prevent', 'stop')\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.lemma_ in causal_verbs for tok in doc if tok.pos_ == \"VERB\"):\n",
    "        explanations.append(\"✓ Causal/preventive verb from FrameNet detected\")\n",
    "        score += 2\n",
    "\n",
    "    if any(tok.text.lower() in {\"if\", \"unless\"} for tok in doc):\n",
    "        explanations.append(\"✓ Conditional clause found (e.g., 'if', 'unless')\")\n",
    "        score += 1\n",
    "\n",
    "    if any(is_fear_related(tok) for tok in doc if tok.pos_ in {\"NOUN\", \"VERB\", \"ADJ\"}):\n",
    "        explanations.append(\"✓ Semantic fear-related concept detected via WordNet\")\n",
    "        score += 2\n",
    "\n",
    "    if any(phrase in question.lower() for phrase in urgency_keywords):\n",
    "        explanations.append(\"✓ Urgency marker detected (e.g., 'immediately', 'before it's too late')\")\n",
    "        score += 1\n",
    "\n",
    "    if any(tok.lemma_ in possibility_terms for tok in doc):\n",
    "        explanations.append(\"✓ Possibility-related term detected (e.g., 'possible', 'feasible', 'chance')\")\n",
    "        score += 1\n",
    "\n",
    "    score = min(score, 10)\n",
    "    label = \"Strong Fear Appeal\" if score >= TRESHHOLD_STRONG else \"Weak/Partial Fear Appeal\" if score >= TRESHHOLD_WEAK else \"Not Fear Appeal\"\n",
    "    logger.info(f\"FearApeal --> Score: {score}, Label: {label}, Explanations: {explanations}\")\n",
    "    return label, score, explanations"
   ],
   "id": "f28b26e9e3c89b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "40f3c540ff34f2ef"
   },
   "cell_type": "markdown",
   "source": [
    "## Final method for preprocessing and filtering for all types"
   ],
   "id": "40f3c540ff34f2ef"
  },
  {
   "metadata": {
    "id": "c06d5fb2d56a3d77"
   },
   "cell_type": "code",
   "source": [
    "def classify_schema(row):\n",
    "    question = row[\"question\"]\n",
    "\n",
    "    is_critical = False\n",
    "\n",
    "    _, cte_score, _ = detect_cause_to_effect(question)\n",
    "    _, expert_score, _ = detect_expert_opinion(question)\n",
    "    _, analogy_score, _ = detect_analogy_question(question)\n",
    "    _, fear_score, _ = detect_fear_appeal_question(question)\n",
    "\n",
    "    if cte_score >= TRESHHOLD_STRONG:\n",
    "      is_critical = True\n",
    "\n",
    "    if expert_score >= TRESHHOLD_STRONG:\n",
    "      is_critical = True\n",
    "\n",
    "    if analogy_score >= TRESHHOLD_STRONG:\n",
    "      is_critical = True\n",
    "\n",
    "    if fear_score >= TRESHHOLD_STRONG:\n",
    "       is_critical = True\n",
    "\n",
    "    return pd.Series({\n",
    "        \"is_Critical\": is_critical,\n",
    "        \"CauseToEffect\": cte_score,\n",
    "        \"ExpertOpinion\": expert_score,\n",
    "        \"Analogy\": analogy_score,\n",
    "        \"FearAppeal\": fear_score,\n",
    "    })"
   ],
   "id": "c06d5fb2d56a3d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "logger.info(\"--- Start evaluation with Argumentative schemes -----\")\n",
    "filtered_df[[\"is_Critical\",\"CauseToEffect\", \"ExpertOpinion\", \"Analogy\", \"FearAppeal\"]] = filtered_df.apply(classify_schema, axis=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF7MO_h4Rj7V",
    "outputId": "251362e0-52ab-4a45-d9c1-5d5f13043ab4"
   },
   "id": "QF7MO_h4Rj7V",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "12df58c8ca44b3e4"
   },
   "cell_type": "markdown",
   "source": [
    "### final check: how many questions per category ?"
   ],
   "id": "12df58c8ca44b3e4"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceb270de5989cf74",
    "outputId": "c0e943c2-6886-4e04-a9d1-cbc13d0775dd"
   },
   "cell_type": "code",
   "source": [
    "value_counts = filtered_df['is_Critical'].value_counts(dropna=False)\n",
    "print(value_counts)\n",
    "\n",
    "columns = [\"CauseToEffect\", \"ExpertOpinion\", \"Analogy\", \"FearAppeal\"]\n",
    "\n",
    "value_counts_schema = filtered_df[columns].apply(pd.Series.value_counts, dropna=False)\n",
    "print(value_counts_schema)\n",
    "logger.info(f\"Value counts per Schema: {value_counts_schema}\")"
   ],
   "id": "ceb270de5989cf74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4307b97d08bb4a9a",
    "outputId": "a12f7bf9-3e32-411b-db06-dd41b770a682"
   },
   "cell_type": "code",
   "source": [
    "value_counts_schema = filtered_df[columns].apply(pd.Series.value_counts, dropna=False)\n",
    "\n",
    "per_column_gte_STRONG = value_counts_schema[value_counts_schema.index >= TRESHHOLD_STRONG].sum()\n",
    "\n",
    "print(f\"Entries with score >= {TRESHHOLD_STRONG} per column:\")\n",
    "print(per_column_gte_STRONG)\n",
    "logger.info(f\"Entries with score >= {TRESHHOLD_STRONG} per column: {per_column_gte_STRONG}\")"
   ],
   "id": "4307b97d08bb4a9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6e58e788197b9b83"
   },
   "cell_type": "markdown",
   "source": [
    "## Save and filter datasets"
   ],
   "id": "6e58e788197b9b83"
  },
  {
   "metadata": {
    "id": "275c73fa5b6f14b9"
   },
   "cell_type": "code",
   "source": [
    "df_to_save = filtered_df.copy()\n",
    "\n",
    "df_to_save['id'] = range(1, len(df_to_save) + 1)\n",
    "\n",
    "columns_to_save = [\n",
    "    'id',\n",
    "    'context',\n",
    "    'question',\n",
    "    'context_token_len',\n",
    "    'is_Critical',\n",
    "    'CauseToEffect',\n",
    "    'Analogy',\n",
    "    'ExpertOpinion',\n",
    "    'FearAppeal'\n",
    "]\n",
    "\n",
    "# Save the full dataset with scores per entry per scheme\n",
    "df_to_save[columns_to_save].to_json(processed_full_dataset_path, orient='records', indent=2)\n",
    "logger.infor(f\"Full dataset saved to: {processed_full_dataset_path}\")"
   ],
   "id": "275c73fa5b6f14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SFT Dataset\n",
    "Create a daset usable during SFT Training. This dataset needs for every relevant scheme one entry."
   ],
   "metadata": {
    "id": "gsNsYBHFSmsN"
   },
   "id": "gsNsYBHFSmsN"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25a414071cf249c8",
    "outputId": "bb6e55b1-bc3c-4cec-f42a-5c0b71690941"
   },
   "cell_type": "code",
   "source": [
    "full_scored_df = pd.read_json(processed_full_dataset_path, orient='records')\n",
    "\n",
    "schema_cols = ['CauseToEffect', 'ExpertOpinion', 'Analogy', 'FearAppeal']\n",
    "\n",
    "# Transform the filtered DataFrame from wide format to long format,\n",
    "# so each schema type and its score become separate rows\n",
    "long_df = filtered_df.melt(\n",
    "    id_vars=['context', 'question'],\n",
    "    value_vars=schema_cols,\n",
    "    var_name='schema',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Filter out rows where the schema score meets or exceeds the threshold for being \"strong\"\n",
    "qualified = long_df[long_df['score'] >= TRESHHOLD_STRONG].copy()\n",
    "\n",
    "qualified = qualified.drop(columns=['score'])\n",
    "\n",
    "# Identify rows in the original filtered DataFrame where none of the schema scores meet the strong threshold\n",
    "mask_no_schema = filtered_df[schema_cols].max(axis=1) < TRESHHOLD_STRONG\n",
    "\n",
    "# Create a new DataFrame for these rows, indicating that no strong schema was found (empty string for schema)\n",
    "no_schema_df = filtered_df.loc[mask_no_schema, ['context', 'question']].copy()\n",
    "no_schema_df['schema'] = ''\n",
    "\n",
    "final_df = pd.concat([qualified, no_schema_df], ignore_index=True)\n",
    "\n",
    "final_df.shape"
   ],
   "id": "25a414071cf249c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df['id'] = final_df.index + 1\n",
    "\n",
    "final_df = final_df[['id', 'context', 'question', 'schema']]\n",
    "\n",
    "# Filter out empty schema rows\n",
    "final_df = final_df[final_df['schema'] != '']\n",
    "\n",
    "# Save the filtered dataset\n",
    "final_df.to_json(processed_sft_dataset_path, orient='records', indent=2)\n",
    "logger.info(f\"SFT dataset saved to: {processed_sft_dataset_path}\")"
   ],
   "metadata": {
    "id": "5Cpy0u9dRBbt"
   },
   "id": "5Cpy0u9dRBbt",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not used but with this code it is possible to create a subset of questiony scored above a certain treshhold."
   ],
   "metadata": {
    "id": "lxGmJJs4TXtU"
   },
   "id": "lxGmJJs4TXtU"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "86f8f3c96036337f",
    "outputId": "b002d6a8-04db-4b91-bc4c-d90835627aa8"
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "full_scored_df = pd.read_json(processed_full_dataset_path, orient='records')\n",
    "\n",
    "score_columns = ['CauseToEffect', 'Analogy', 'ExpertOpinion', 'FearAppeal']\n",
    "\n",
    "# Filter rows where any score is greater than 7\n",
    "filtered_df = full_scored_df[full_scored_df[score_columns].gt(7).any(axis=1)].copy()\n",
    "\n",
    "# Reset the index and update the 'id' field to start from 1\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df['id'] = filtered_df.index + 1\n",
    "\n",
    "filtered_df.to_json(\"path goese here\", orient='records', indent=2)\n",
    "'''"
   ],
   "id": "86f8f3c96036337f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "eb90f14ffbe99af8"
   },
   "cell_type": "markdown",
   "source": [
    "## analyse dataset"
   ],
   "id": "eb90f14ffbe99af8"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a38e5235ea1252af",
    "outputId": "bc96ee1c-6480-4165-da97-52728a1dc597"
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_json(processed_full_dataset_path, orient='records')\n",
    "df.shape"
   ],
   "id": "a38e5235ea1252af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1a83fb7c287d724",
    "outputId": "dedacd6c-d8d1-43ac-d998-fa0df3435cc7"
   },
   "cell_type": "code",
   "source": [
    "score_columns = ['CauseToEffect', 'Analogy', 'ExpertOpinion', 'FearAppeal']\n",
    "\n",
    "score_counts = pd.DataFrame()\n",
    "\n",
    "for col in score_columns:\n",
    "    counts = df[col].value_counts().sort_index()\n",
    "    score_counts[col] = counts\n",
    "\n",
    "score_counts = score_counts.fillna(0).astype(int)\n",
    "\n",
    "print(score_counts)"
   ],
   "id": "f1a83fb7c287d724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e19136c35c735fec"
   },
   "cell_type": "markdown",
   "source": [
    "## Git"
   ],
   "id": "e19136c35c735fec"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b244293861007956",
    "outputId": "a560e38f-5d2e-480b-e6fa-d9398e0926d4"
   },
   "cell_type": "code",
   "source": [
    "if PUSH_TO_GITHUB:\n",
    "    os.chdir(\"NLP2025_CQG\")\n",
    "    !ls\n",
    "    !git config --global user.name \"Rico Städeli\"\n",
    "    !git config --global user.email \"rico@yabriga.ch \"\n",
    "\n",
    "    commit_message = f\"Preprocess and evaluate data\"\n",
    "    !git add .\n",
    "    !git commit -m \"{commit_message}\"\n",
    "    !git push"
   ],
   "id": "b244293861007956",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
